[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Modeling and Reproducibility for Educational Scientists",
    "section": "",
    "text": "Front Matter\nThe content in this “book”, as the title suggests, is related to advanced statistical modeling and reproducibility. More specifically, the content focuses on expanding the General Linear Model (GLM) to provide statistical evidence that can help answer substantive questions in the educational and social sciences. It also includes computing content for leveling-up educational scientists’ reproducibility of analyses.\nWhile it is a book intended for applied practitioners in the educational or social sciences, and the statistical content is hopefully presented in a manner that these domain scientists will find useful, it does require a good foundation in regression analysis using the GLM. Moreover, since the content is somewhat mathematical in nature, the reader will need a solid understanding of algebra for maximum benefit. The burden of calculation that typically accompanied statistical work in previous generations is now primarily carried out in a scientific computing environment. As Thisted & Velleman (1992) point out, “computational advances have changed the face of statistical practice by transforming what we do and by challenging how we think about scientific problems.” To support and help facilitate the use of scientific computing, examples using the R computer language will be used throughout this work.\nThe organization of content is consistent with the sequence this content is taught in EPsy 8252, the second of two applied statistics courses that form the foundational sequence for many graduate students in the educational and social sciences at the University of Minnesota. This course require that students have taken a previous statistics course at the graduate level focusing on the GLM.",
    "crumbs": [
      "Front Matter"
    ]
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Advanced Modeling and Reproducibility for Educational Scientists",
    "section": "Resources",
    "text": "Resources\nThis book refers to and uses several data sets throughout the text. Each of these data sets and their codebooks are available online at the book’s github repository, https://github.com/zief0002/adv-modeling/.",
    "crumbs": [
      "Front Matter"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Advanced Modeling and Reproducibility for Educational Scientists",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nMany thanks to all the students in my courses who have been through previous iterations of this material. Your feedback has been invaluable, and you are the world’s greatest copyeditors. In particular, I would like to thank the following students who have gone above and beyond in the feedback they have provided: Jonathan Brown, Pablo Vivas Corrales, Amaniel Mrutu, Corissa Rohloff, and Mireya Smith.",
    "crumbs": [
      "Front Matter"
    ]
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "Advanced Modeling and Reproducibility for Educational Scientists",
    "section": "Colophon",
    "text": "Colophon\nThe book is typeset using Atkinson Hyperlegible. The color palette was generated using coolors.co.\nIcon and note ideas and prototypes by Desirée De Leon.\nArtwork\n\nHedgehog banner by Alison Hill and Allison Horst, for RStudio.\nData science and statistics artwork by @allison_horst",
    "crumbs": [
      "Front Matter"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Advanced Modeling and Reproducibility for Educational Scientists",
    "section": "License",
    "text": "License\n\nAdvanced Modeling and Reproducibility for Educational Scientists by Andrew Zieffler is licensed under CC BY-NC-SA 4.0",
    "crumbs": [
      "Front Matter"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Advanced Modeling and Reproducibility for Educational Scientists",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nThisted, R. A., & Velleman, P. F. (1992). Computers and modern statistics. In D. C. Hoaglin & D. S. Moore (Eds.), Perspectives on contemporary statistics, MAA notes no. 21 (pp. 41–53). Mathematical Association of America.",
    "crumbs": [
      "Front Matter"
    ]
  },
  {
    "objectID": "01-00-reproducibility.html",
    "href": "01-00-reproducibility.html",
    "title": "Introduction to Statistical Reproducibility",
    "section": "",
    "text": "The first set of tools we will discuss will be related to reproducibility. Reproducibility, the idea that same results from a study should be obtained if the study is repeated, is a key part of the scientific process. Here, we are particularly concerned with statistical reproducibility. That is, given the same data and same code, two analysts should get the exact same results.\nIn practice, reproducibility means making your data and syntax available to other researchers. There are many ways to do this, but the Open Science Framework website is a fantastic option for making resources from research projects openly accessible to other researchers. Aside from the obvious advantages of minimizing fraud and bias, making research artifacts openly available also improves trust in the scientific process.\n\nPROTIP\nMaking your data and syntax available is something to consider duing the planning phases of a study so that you can make this transparent in the IRB proposal.\n\nThe initial chapters of this book will address:\n\nInstalling Quarto and other computational tools;\nProject organization; and\nUsing Quarto to integrate text and computation.",
    "crumbs": [
      "Introduction to Statistical Reproducibility"
    ]
  },
  {
    "objectID": "01-02-project-organization.html",
    "href": "01-02-project-organization.html",
    "title": "1  Project Organization",
    "section": "",
    "text": "1.1 Projects\nIn this chapter, you will learn some basic tips and tricks for organizing directories and files for project management. At the end of it, you will have an organized project directory to begin work on Assignment 1.\nIn any project, whether it is work for an RA-ship, an assignment in EPsy 8252, a milestone for your degree (e.g., thesis) or a paper you are working on, it is important to have an organized set of files and good documentation. Both of these help foster rigor and reproducibility of research, and are even more important in collaborative work.\nElectronically, a project consists of a set of directories (i.e., folders) and files. Organizationally, you will want to think about, among other things:\nWhile there are any of a number of alternative organizational structures that may work in different situations, here I will recommend a general strategy of creating projects for EPsy 8252. (This will also work for other types of projects.) For each project, we will have, at a minimum, the following directories:\nThe screenshot below shows a project with these directories.\nIt is typical to show directory and file organization via a “directory tree”. The directory tree for this project is shown below.\nHere the directory assignment-01 is the primary folder associated with our project. We refer to this as the root directory of our project.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-02-project-organization.html#projects",
    "href": "01-02-project-organization.html#projects",
    "title": "1  Project Organization",
    "section": "",
    "text": "What directories you will have;\nWhich files will be located in which directories;\nThe naming rules for files and directories;\n\n\n\nassets\ndata\nfigs\nscripts\n\n\n\n\n├── assets\n├── data\n├── figs\n└── scripts\n\nYour Turn\nSet up a directory called assignment-01. Within this directory, create the four directories: assets, data, figs, and scripts. The directory tree will look like this:\nassignment-01\n    ├── assets\n    ├── data\n    ├── figs\n    └── scripts\n\n\n\n\n1.1.1 Naming Conventions\nThe naming conventions for the directories and files in our project are as follows:\n\nFile names should be short but descriptive (less than 25 characters)\nAll lowercase letters\nAvoid special characters and spaces in a file name\n\nUse hyphens instead of spaces to separate words (e.g., assignment-01)\n\nAny names that include the date will use the ISO 8601 date format (YYYYMMDD)\nAny names that include a number will include at least two digits (e.g., assignment-01 rather than assignment-1)\n\nAgain, while there is no one best naming convention, it is important that you have one, and that you are consistent throughout the project. That being said, as you develop naming conventions for your projects, all the conventions should be documented! This documentation helps onboard collaborators to your project.\n\nFYI\nThere are several guides available to help you establish naming conventions including here and here.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-02-project-organization.html#r-projects",
    "href": "01-02-project-organization.html#r-projects",
    "title": "1  Project Organization",
    "section": "1.2 R Projects",
    "text": "1.2 R Projects\nCreating an R project is a way to inform RStudio about which folder is your project’s root directory. This also sets R’s working directory to your project’s root directory making it easier to access data and other files in your QMD documents. To create an R Project associated with the assignmnet-01 root directory,\n\nClick the project icon in RStudio (it might say Project: (None) or something like that) and select New Project... (see screenshot below)\n\n\n\n\n\n\n\n\n\n\nIn the project wizard, select:\n\nExisting Directory,\nClick Browse and select the assignment-01 root directory you created earlier\nClick Open\nClick Create Project\n\nThe R Project will then be created and you should now see this project (called assignment-01.Rproj) in the assignment-01 directory.1 The project icon in RStudio should also have changed to assignment-01.\n└── assignment-01\n    ├── assets\n    ├── assignment-01.Rproj\n    ├── data\n    ├── figs\n    └── scripts\nOnce an R project has been created, it can be opened in RStudio by double-clicking on the .Rproj file.\n\nYour Turn\nIf you haven’t already, create an R project associated with the assignment-01 root directory.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-02-project-organization.html#working-in-your-project",
    "href": "01-02-project-organization.html#working-in-your-project",
    "title": "1  Project Organization",
    "section": "1.3 Working in Your Project",
    "text": "1.3 Working in Your Project\nIn RStudio, the Files pane of your assignmnet-01 R Project (see below) should show the contents of your root directory. You can use this pane to navigate to the different directories, or open files in the project.\n\n\n\n\n\n\n\n\n\nWe can add files and directories by creating them using an application on our computer or downloading them and saving them to the appropriate project directory.\n\nYour Turn\nDownload the pew.csv file to your computer. Make sure that the file suffix is .csv. (Sometimes Safari users will find that the suffix .txt gets appended to the end of the filename—pew.csv.txt. If that is the case delete the .txt part that was appended.) Place pew.csv into the data directory of your project. The new tree is:\nassignment-01\n    ├── assets\n    ├── assignment-01.Rproj\n    ├── data\n    │   └── pew.csv\n    ├── figs\n    └── scripts\n\nIn RStudio, in the Files pane, if you click on the data directory, you should see pew.csv.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-02-project-organization.html#creating-a-readme-file",
    "href": "01-02-project-organization.html#creating-a-readme-file",
    "title": "1  Project Organization",
    "section": "1.4 Creating a README File",
    "text": "1.4 Creating a README File\nWe can also create files from inside RStudio . We are going to create a README file. A README is a plain text file that introduces and explains a project. It contains information that is commonly required to understand what the project is about. Every project should have a README file.\n\nYour Turn\nFrom the RStudio menu, select: File &gt; New File... &gt; Text File. This should open a blank text file in the RStudio editor. Copy the following text into your blank file:\n# assignment-01\n\nThis directory contains all of the files necessary to complete Assignment 1.\nClick the Save icon (or select File &gt; Save) and save this file as README; all uppercase. (README files are the one exception to our use of uppercase letters in our naming convention.) README files typically have no file extension—README is correct as opposed to README.txt.\n\nOur directory tree now look like this:\nassignment-01\n    ├── README\n    ├── assets\n    ├── assignment-01.Rproj\n    ├── data\n    │   └── pew.csv\n    ├── figs\n    └── scripts\nNotice that despite the README file starting with the letter “r”, it is placed first in the root directory prior to our other directories and files. This is by design. In Unix, any file in all uppercase letters is shown first in the directory tree. (It does not show first in the RStudio Files pane, nor in the file view when you open the root directory on your computer.)\n\nProtip The {usethis} package includes functionality to create many useful files for projects, including README files.\n\n\n\n1.4.1 Adding Content to README\nSince README files are plain text files, they cannot include formatting like bold or italic. However, they do typically include Markdown syntax (which is itself plain text). The plain text nature of these files keeps them small in size and accessible to anyone with any type of computer.\n\nFYI\nThere are several online guides for what to include in a README file, including here and here. There is also a pretty good template for a README for data science oriented projects here.\n\nSince the README file is informational, you can include any type of information in this file that is useful to the project. For example, you could add your naming conventions to this file.\n# assignment-01\n\nThis directory contains all of the files necessary to complete Assignment 1.\n\n\n# Naming Conventions\n\n- File names should be short but descriptive (less than 25 characters)\n- All lowercase letters\n- Avoid special characters and spaces in a file name\n  + Use hyphens instead of spaces to separate words (e.g., `assignment-01`)\n- Any names that include the date will use the ISO 8601 date format (YYYYMMDD)\n- Any names that include a number will include at least two digits (e.g., `assignment-01` rather than `assignment-1`)\n\nWhile you should only have one README file per directory, you can have different README files in other directories. For example, you could create a README file in the data directory that includes the codebook information for your data files.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-02-project-organization.html#references",
    "href": "01-02-project-organization.html#references",
    "title": "1  Project Organization",
    "section": "1.5 References",
    "text": "1.5 References",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-02-project-organization.html#footnotes",
    "href": "01-02-project-organization.html#footnotes",
    "title": "1  Project Organization",
    "section": "",
    "text": "R projects have the same name as the root directory you associated it with and have the file suffix .Rproj. Our directory tree is now:↩︎",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html",
    "href": "01-03-introduction-to-quarto.html",
    "title": "2  Introduction to Quarto",
    "section": "",
    "text": "2.1 Creating a New Quarto Document\nIn this set of notes, you will begin your Quarto journey.\nTo create a new Quarto document, go to the directory where you want to put the file and click New File under the “Files” tab, and select Quarto Document. In the popup box, give your new document a name. Quarto documents have the extension .qmd.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#yaml-the-stuff-between-the-sets-of-three-dashes",
    "href": "01-03-introduction-to-quarto.html#yaml-the-stuff-between-the-sets-of-three-dashes",
    "title": "2  Introduction to Quarto",
    "section": "2.2 YAML: The Stuff Between the Sets of Three Dashes",
    "text": "2.2 YAML: The Stuff Between the Sets of Three Dashes\nYAML (pronounced Yam-el) is an acronym for “Yet Another Markup Language”. It constitutes the metadata for your Quarto document. Not only is it informational, but it also helps format your document. The metadata in your newly created Quarto document looks something like this:\n\n---\ntitle: \"hello-world\"\nformat: html\n---\n\nThese are called key-value pairs. The “keys” are “title” and “format”. The value associated with title is “hello-world” (by default this will be whatever you named the document), and the value associated with format is “html”.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#rendering-the-document",
    "href": "01-03-introduction-to-quarto.html#rendering-the-document",
    "title": "2  Introduction to Quarto",
    "section": "2.3 Rendering the Document",
    "text": "2.3 Rendering the Document\nClick the Render button above your Quarto document. This will render the quarto document (i.e., the QMD file) into a human-consumable format—in this case an HTML file that can be opened in a web browser. The rendered document should also open in your “Viewer” tab in RStudio. If you go back and look at yuor project directory, you should also see a new HTML file has been created. (In my case, this was “hello-world.html”.) This can be opened in a web-browser; try it by double-clicking the HTML file.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#update-the-yaml",
    "href": "01-03-introduction-to-quarto.html#update-the-yaml",
    "title": "2  Introduction to Quarto",
    "section": "2.4 Update the YAML",
    "text": "2.4 Update the YAML\nOur document is pretty boring—it only has a title that is the same as our file name. Let’s edit the YAML to see how we can add to our document’s metadata. First you can update the title value to whatever you want the title of your document to be:\n\n---\ntitle: \"My First Quarto Document\"\nformat: html\n---\n\nWe can also add additional metadata. Here we add a subtitle, author(s), and date.\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\nOnce you have edited and added in the new YAML keys and values, re-click the Render button.\n\nNotice\nAll the YAML keys and special values (e.g. html) are all lower case! Special values do not need quotes. All other strings need quotes.\n\nThe keys “subtitle”, “author”, and “date” are all keys that Quarto recognizes. Some of these keys are more global (e.g., title, author) and work across different formats. Others are unique to the type of document format. For example, some keys work with HTML formatted documents and others work with PDF formatted documents. You can learn about some of the YAML that works with HTML formatted documents here: https://quarto.org/docs/output-formats/html-basics.html\n\n\n2.4.1 Autopopulating the Date\nWe can auto-populate the date: key in our document’s YAML by including “today” rather than a specifc date. The date will update to the current date when you render the document.\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#adding-content",
    "href": "01-03-introduction-to-quarto.html#adding-content",
    "title": "2  Introduction to Quarto",
    "section": "2.5 Adding Content",
    "text": "2.5 Adding Content\nThe primary part of a Quarto document is not the YAML, but the actual body of the document. Things like headings, text, analysis, and other content are part of the body of the Quarto document. This is all added BELOW the sets of three dashes.\nLet’s add some text to our document.\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\nHere is some text for my document. Notice that it is below the YAML.\n\nHere is a new paragraph. A line space indicates that we start a new paragraph.\n\n\nWORKFLOW\nAs you add content to your QMD file you should save it often. To see how the new content looks, you need to render the document. Rendering the document also typically saves the added content to the QMD file. So your workflow is: Add content –&gt; Render.\n\n\n\n2.5.1 Headings\nIn an HTML document there are six levels of headings. You can think about headings as denoting sections and subsections (and sub-sub-sections, etc.) in a document.\nHeadings need to be on their own line and the star of the line is one or more hashtags. A level-1 heading uses one hashtag, a level-2 heading uses two hashtags. A level-3 heading uses three hastags. Etc. You also need a blank line above and below the heading line.\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Hello\n\nThis is a level-1 heading. Note the blank line above and below the heading.\n\n## World\n\nThis is a level-2 heading. Note the heading starts with two hashtags.\n\n\n\n\n2.5.2 Bold and Italics\nText can be bolded or italicized by adding asterisks around the text. A single set of asterisks is placed around text that you want to italicize and a double set placed around text you want to bold. (Note: You can also use underscores to italicize or bold things in a similar manner.)\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Bold and Italics Text\n\nHere is *italicized text*. And here is **bold text**.\n\nWe can also use undercores to create _italicized text_ or __bold text__.\n\nCombinations of asterisks and underscore create __*bold, italicized text*__.\n\n\n\n\n2.5.3 Other Fun Content\nYou can also add content such as unordered (i.e., bulleted) lists, ordered (i.e., numbered) lists, hyperlinks, images, footnotes and blockquotes. To see how to do check out the Markdown Basics page.\n\n\n\n2.5.4 Source versus Visual Editor\nAbove the QMD document is a set of buttons named “Source” and “Visual”. These buttons change the editong mode of the document. By default the “Source” mode is used. Clicking the “Visual” button will switch to the visual editing mode. The visual editing mode is more like Word or Google Docs—there is a toolbar to select things like bold/italics, etc.\nYou can use whatever editor you want, but In my experience, novices like to use the visual editor because it is closer to other editing programs they have used before. Experts tend to use the source editor. Not only is it faster for them, but as your documents become more complex, the visual editor has many shortcomings.\nFor that reason, I encourage you to become proficient with the source editor. To further encourage this all of the instruction I give will be in the source editor.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#back-to-the-yaml-indentation-for-options",
    "href": "01-03-introduction-to-quarto.html#back-to-the-yaml-indentation-for-options",
    "title": "2  Introduction to Quarto",
    "section": "2.6 Back to the YAML: Indentation for Options",
    "text": "2.6 Back to the YAML: Indentation for Options\nNow that you have added a few headings and some text to youe QMD file, let’s include some additional YAML. The new YAML will add a table-of-contents to your rendered document. Note that the format: html key-value pair is now separated—html is on a new line and indented.\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-location: left\n---\n\nAs you try out different YAML it is important to pay attention to the indentation. Keys that are indented have to be 2 or 4 spaces (generally 1 tab). The indentation is a way of denoting options for different keys. For example, consider the following YAML:\nNow the value “html” is indented (2 spaces) from the “format” key. This is because we have now added additional keys that set options in our HTML document. Namely adding a table of contents (“toc: true”) and a location for that table of contents (“toc-location: left”). These keys are indented an additional two spaces to indicate they are options for the “html” key.\n\nIMPORTANT\nThe spacing in the YAML really matters a lot. If you don’t have the correct spacing, the document will not render.\n\nYou can think of the indentation as indicating the hierarchical structure of elements withing the YAML. In our example the key “html” is a subcomponent of the “format” key. And, the “toc” and “toc-location” keys are subcomponents of the “html” key.\nHere is a more complex example. Don’t worry what the new keys do, but instead think about the heirarchy. Which keys are subcomponents of which other keys?\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-location: left\n    html-math-method:\n      method: mathjax\n  pdf:\n    mainfont: \"Open Sans\"\n---",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#code-chunks-integrating-r-syntax-into-your-qmd-document",
    "href": "01-03-introduction-to-quarto.html#code-chunks-integrating-r-syntax-into-your-qmd-document",
    "title": "2  Introduction to Quarto",
    "section": "2.7 Code Chunks: Integrating R Syntax into Your QMD Document",
    "text": "2.7 Code Chunks: Integrating R Syntax into Your QMD Document\nThe real bang-for-your-buck of wrting in a Quarto document is that you can integrate your R syntax directly into your text document. To do this we need to add a code chunk. To add a code chunk click the green “+C” above your quarto document and select “R”. This will add a blank R code chunk to your QMD document.\nAny R code you add inside the code chunk will be executed when you render the document.\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Code Chunk\n\n```{r}\n2 + 3 * 3\n```\n\nWhen you render the document, both the syntax and output from the syntax are printed in the rendered HTML document.\n&lt; br /&gt;\n\n2.7.1 Code Chunk Options\nWe can include options for code chunks that customize what will be printed into the HTML document. For example, in a paper, you often want to include the figure, but may not want to print all of the syntax that was used to create that plot in the final paper. Code chunk options are placed at the beginning of the code chunk and begin with #|. Similar to YAML, they are key-value pairs. For example, the code chunk below has two options: #| label: and #| echo.\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Code Chunk\n\n```{r}\n#| label: order-of-operations\n#| echo: true\n2 + 3 * 3\n```\n\nThe #| label: option should be included with every code chunk. It is a way of naming the code chunk. Here, the name of this code chunk is order-of-operations. Other code chunks can have any label you want. They should be descriptive of what code is being run in the chunk. For example, label: correlations or label: fit-model-1. Note that labels can’t include space or special characters. They also have to be unique, so a label name cannot be used again.\nThe second option, #| echo: indicates whether the syntax should be printed in the HTML document. If the value of this option is true, then the syntax is printed along with the output of the syntax. If the value of this option is false, then only the output of the syntax is printed to the HTML document.\nYou can see more code chunk options in the Quarto documentation page, Executable Options.\n\nPROTIP\nAt the beginning of a Quarto document (immediately after the YAML) include a code chunk that loads all the libraries you need and also imports any datasets for an analysis. In this code chunk I often use the following code options:\n\n#| label: setup\n#| echo: false\n#| message: false\n#| warning: false\n\nThe last two options supress any messages or warnings that are elicited when you load your libraries. The setup label is special value. The code chunk labelled setup will be run when you try to run other chunks. This is important since other code often relies on you having loaded libraries and imported the data!\n\nHere is an example of where I would include the first setup code chunk.\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Code Chunk\n\n```{r}\n#| label: setup\n#| echo: false\n#| message: false\n#| warning: false\n\n# Load libraries\nlibrary(tidyverse)\n\n# Import data\npew = read_csv(\"data/pew.csv\")\n```\n\n\nSUPER IMPORTANT\nIn EPsy 8251, when we create objects, they are stored in the R environment and we can operate on those objects. For example, in this document we will read in the pew.csv data and assign it to the object pew. We can then use functions to compute values and operate on the data.\nTrying to operate on an object that we created in the R environment, but do not create in our Quarto document will lead to an error. This is because the R environment and your Quarto document are completely independent from one another.\nIf you want to operate on an object you have to create the object in a code chunk in the Quarto document.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#equations",
    "href": "01-03-introduction-to-quarto.html#equations",
    "title": "2  Introduction to Quarto",
    "section": "2.8 Equations",
    "text": "2.8 Equations\n\nIMPORTANT\nEquation syntax is not R code. Therefore equation syntax should not be placed inside of a code chunk!\n\nThere are two different manners in which equations/mathematics is included in a document.\n\nDisplay equations are typeset on a separate line from the body text and are centered on the page.\nInline equations are typeset directly within the body text.\n\nFor example, here is a display equation:\n\\[\ny_i = \\beta_0 + \\beta_1(x_i) + \\epsilon_i\n\\]\nThe syntax I used to create this display equation is:\n$$\ny_i = \\beta_0 + \\beta_1(x_i) + \\epsilon_i\n$$\nHere is the same equation as an inline equation: \\(y_i = \\beta_0 + \\beta_1(x_i) + \\epsilon_i\\). Notice that in an inline equation, the equation is embedded directly in the text. To create the inline equation we embed the mathematical expression in single dollar signs ($y_i = \\beta_0 + \\beta_1(x_i) + \\epsilon_i$) rather than double dollar signs.\n\nLEARN MORE\nIf you need a reminder about how to create these, check out the “Equations” section of the Markdown Basics tutorial.\n\nThe syntax we use to create the mathematical expressions is from LaTeX. Here are a couple reference you can use:\n\nList of LaTeX mathematical symbols\nLaTeX Mathematical Symbols\n\nTo write fitted equations, you need a hat over the outcome. To do that we can use \\hat{} or \\widehat{}. I use widehat over words (e.g., \\(\\widehat{\\text{Knowledge}_i}\\)) and hat over single letters (e.g., \\(\\hat{y}_i\\)). In both, whatever you want to add the hat over is placed in between the curly braces. For example, here is how you might write the equation for the sample fitted line:\n$$\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1(x_i)\n$$\n\n\n2.8.1 Better Typesetting of Equations\nWhen we typeset equations, there are a couple things we should do:\n\nUse variable names that make sense to our reader, rather than their name in our dataset/R. For example “News Exposure” is a better name than “news”.\nVariable names should also be typeset using normal text rather than italic (the default in mathematical expressions).\n\nWe can fix both issues by including the variable names in a text environment (\\text{}). So to get the name “News Knowledge” we need to use the following in our equation:\n$$\n\\text{News Knowledge}\n$$\nOther useful environments include: \\mathit{} (italics), \\mathbf{} (bold), \\mathtt{} (typewriter text), and \\mathcal{} (caligraphy; this is useful for the “N” we use to indicate a normal distribution). If you want a space when using these, you include a tilde (~) to denote a space.\n\nHyphens need special syntax since a hyphen would be interpreted as a minus sign. In typesetting, the minus sign is longer than the hyphen symbol.\n\nIf you want to include a hyphen, we need to include it in \\mbox{}. For example, to add a hyphen in our variable name to get “Education-Level”, we use:\n$$\n\\text{Education\\mbox{-}Level}\n$$",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html",
    "href": "01-04-more-quarto.html",
    "title": "3  More Quarto",
    "section": "",
    "text": "3.1 Code Chunks for Figures\nThis document contains some additional instruction for EPsy 8252. Note that I might add to this as students hit me with questions over the course of the semester.\nCode chunks can be used to do anything you can do in R. For example we can fit regression models, create plots, compute summary measures, or mutate dummy variables into our data. For figures, there are a bunch of chunk options that are useful.\nTo label code chunks that include figures, we will append fig- to the label name. For example, we can use #| label: fig-density-knowledge to label a code chunk where we are creating a density plot of the news knowledge variable.\nWe can add a caption using the #| fig-cap: chunk option. It is also a good idea to also include the #| fig-alt: chunk option with figures. This provides alternative text for screenreaders (which is required under the new federal accessability laws). Below is an example of this:\nNotice that by labeling this chunk with fig-, the figure is numbered in the document. This allows the figure to also be cross-referenced. You can do this by using the “at” symbol (@) along with the figure’s label name. For example, to cross-reference to the density plot we created, we could use something like:",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html#code-chunks-for-figures",
    "href": "01-04-more-quarto.html#code-chunks-for-figures",
    "title": "3  More Quarto",
    "section": "",
    "text": "---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Code Chunk\n\n```{r}\n#| label: fig-density-knowledge\n#| fig-cap: \"Here is your figure caption.\"\n#| fig-alt: \"Here is alternative text for screenreaders.\"\nggplot(data = pew, aes(x = knowledge)) +\n   geom_density()\n```\n\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Code Chunk\n\n```{r}\n#| label: fig-density-knowledge\n#| fig-cap: \"Here is your figure caption.\"\n#| fig-alt: \"Here is alternative text for screenreaders.\"\nggplot(data = pew, aes(x = knowledge)) +\n   geom_density()\n```\n\n\nThe distribution of news knowledge is unimodal (see @fig-density-knowledge).\n\n\n3.1.1 Figure Size\nThere are a couple other chunk options for figures that help set the aspect ratio and size of the figure in your document. The aspect ratio is the ratio of a figure’s width to its height. We set this using the #| fig-width: and #| fig-height: chunk options. (The default value for both is 6; thus the figure appears square.) Below we change these so that the figure appears wider than it is tall.\n\n```{r}\n#| label: fig-density-knowledge-2\n#| fig-cap: \"Density plot showing the distribution of news knowledge.\"\n#| fig-alt: \"Density plot showing a a unimodal distribution of news knowledge.\"\n#| fig-width: 9\n#| fig-height: 5\nggplot(data = pew, aes(x = knowledge)) +\n   geom_density()\n```\n\n\n\n\n\n\n\nFigure 3.1: Density plot showing the distribution of news knowledge.\n\n\n\n\n\n\nFYI\nIt is important to change the aspect ratio when you have legends or when you are using {patchwork} to stack or put multiple figures next to each other. Many times you need to find a good aspect ratio though trial-and-error of trying different values.\n\nThe actual size of the figure in the document is independent of the aspect ratio and can be set using the #| out-width: or #| out-height: chunk option. These options take a character string of the direct size (in HTML documents this is typically in pixels) or of the percentage of the output width/height. Here we keep the same aspect ratio, but make the figure smaller by setting the figure width to 40% of the document’s width.\n\n```{r}\n#| label: fig-density-knowledge-3\n#| fig-cap: \"Density plot showing the distribution of news knowledge.\"\n#| fig-alt: \"Density plot showing a a unimodal distribution of news knowledge.\"\n#| out-width: \"40%\"\nggplot(data = pew, aes(x = news, y = knowledge)) +\n   geom_point()\n```\n\n\n\n\n\n\n\nFigure 3.2: Density plot showing the distribution of news knowledge.\n\n\n\n\n\n\nPROTIP\nIf you set the aspect ratio of the figure, you need only set #| out-width: or #| out-height:. You don’t need to set both as the other will be determined by the aspect ratio.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html#inline-code-chunks-for-better-reproducibility",
    "href": "01-04-more-quarto.html#inline-code-chunks-for-better-reproducibility",
    "title": "3  More Quarto",
    "section": "3.2 Inline Code Chunks for Better Reproducibility",
    "text": "3.2 Inline Code Chunks for Better Reproducibility\nIn writing papers where there are results from data analyses being reported in the text, inline code chunks are boss! For example, consider writing the following sentence in an analysis of the pew.csv data.\n\nThe effect of news exposure on news knowledge is 0.327 (SE = 0.09).\n\nRather than computing these values and then transposing the values into the sentence, we can use inline code chunks to directly compute and write the values in the sentence. Recall, earlier we assigned the tidy() output into an object called lm1coef. The results from tidy() are a tibble (or data frame).\n\n# Print output\nlm1coef\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   39.7      5.20        7.63 1.56e-11\n2 news           0.327    0.0899      3.64 4.42e- 4\n\n\nA tibble/data frame is a combination of rows and columns. We can access different parts of the tibble by indexing the row and column that we want. For example, to access the slope standard error, which is in the 2nd row and 3rd column, we could use:\n\n# Access slope SE\nlm1coef[2, 3]\n\n# A tibble: 1 × 1\n  std.error\n      &lt;dbl&gt;\n1    0.0899\n\n\nUnfortunately, this also comes out as a labelled tibble. If we just want the SE value we double index this. Thus to get the value of the slope SE, we use:\n\n# Access slope SE number only\nlm1coef[[2, 3]]\n\n[1] 0.08994213\n\n\nHere is how we could extract the slope value and the SE of the slope value to write our earlier sentence:\n\nThe effect of news exposure on news knowledge is `r lm1coef[[2, 2]]` (SE = `r lm1coef[[2, 3]]`).\n\nThis produces the following sentence in the rendered document:\n\nThe effect of news exposure on news knowledge is 0.3271158 (SE = 0.0899421).\n\nNote: You don’t need to assign the output to an object to index it. For example, the SE could also be extracted using: `r tidy(lm.1)[[2, 2]]`\n\n\n3.2.1 Rounding\nThere are several ways to set the rounding to two decimal places. One is to embed the computation in the round() function. For example, in the first inline chunk we could use: round(lm1coef[[2, 3]], 2).\nYou can also do this in a separate code chunk and then call the values in the inline computation (as suggested in the Quarto Computations Tutorial).",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html#multiline-equations",
    "href": "01-04-more-quarto.html#multiline-equations",
    "title": "3  More Quarto",
    "section": "3.3 Multiline Equations",
    "text": "3.3 Multiline Equations\nThere are a couple of ways to create multiline equations in a Quarto document. I tend to use the split environment from LaTeX to do this. To use this:\n\nInclude \\begin{split} and \\end{split} in the display equation.\nAt the end of each line (where you want a linebreak) include a double backslash, \\\\.\nIn each line of the multiline equation, also include the ampersand sign (&) at the point we want the multiple lines to align vertically.\n\nHere is an example:\n$$\n\\begin{split}\n\\hat{y}_i &= 3 + 4(1) \\\\\n&= 7\n\\end{split}\n$$\nAnd the resulting display equation is:\n\\[\n\\begin{split}\n\\hat{y}_i &= 3 + 4(1) \\\\\n&= 7\n\\end{split}\n\\]\nNote that the ampersand appeared immediately before the equal sign in each line of the equation, so that is where the two lines are aligned vertically (the equal signs are on top of each other). You can add as many lines to this as you want.\n\nFYI\nMultiline equations are quite useful to show your work and when you have really long single equations that need to be broken up so they fit on the page (e.g., a regression equation with many predictors).\n\n\n\n3.3.1 Spacing Out the Lines\nYou can add space between the lines of your multiline equation by including a unit of measurement between square brackets after the double backslashes.\n$$\n\\begin{split}\n\\hat{y}_i &= 3 + 4(1) \\\\[1em]\n&= 7\n\\end{split}\n$$\nHere the resulting display equation is:\n\\[\n\\begin{split}\n\\hat{y}_i &= 3 + 4(1) \\\\[1em]\n&= 7\n\\end{split}\n\\]\nHere we have added line space of 1 em. An em is a unit for measuring the width of printed work, equal to the height of the type size being used (typically the width of the letter “m”). Other common printing units include the en (the width of the letter “n”), and the ex (the width of the letter “x”).\n\n\n\n\n\n\n\n3.3.2 Typesetting 101: Hyphens, Minus Signs, and Dashes\nIn printed work the width of a hyphen, a minus sign, and dashes are all different. Quarto will correctly typeset these, but you have to indicate what you want. Below are how we indicate these in Quarto.\n\nHyphen: A hyphen is used to join two or more words together. To typeset a hyphen in Quarto, use a single hyphen; -.\nEn Dash: An en dash is used to mark ranges and to indicate the meaning “to” in phrases like “Minneapolis–St. Paul”. An en dash is slightly longer than a hyphen and is typeset in Quarto using two hyphens; --.\nEm Dash: An em dash is used to separate extra information or mark a break in a sentence. An em dash is slightly longer than an en dash and is typeset in Quarto using three hyphens; ---.\nMinus Sign: A minus sign is used in mathematical expressions (e.g., in subtraction or to indicate a negative number). To typeset a minus sign, use the inline equation with a single hyphen; $-$. When a minus sign is typeset, not only is the length of it different than a hyphen, but it also includes the correct spacing around it for mathematical typesetting.\n\nHere is a sentence incorporating each of these so you can see the differences:\nDuring the time from January–February, my three dogs enjoy playing in the snow—apparently snow is fun for canines—with their sweaters on since it is \\(-30\\)-degrees Fahrenheit outside.\nThe input for this sentence was:\nDuring the time from January--February, my three dogs enjoy playing in the snow---apparently snow is fun for canines---with their sweaters on since it is $-30$-degrees Fahrenheit outside.\nNote that you can also typeset these in Word:\n\nHyphen: On both a Mac and PC, press “hyphen”.\nEn Dash: On a Mac, press “option+hyphen key”. On a PC, press “ctrl+hyphen”.\nEm Dash: On a Mac, press “option+shift+hyphen key”. On a PC, press “alt+ctrl+hyphen”.\nMinus Sign: For a minus sign, you should use Equation Editor.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html#citations",
    "href": "01-04-more-quarto.html#citations",
    "title": "3  More Quarto",
    "section": "3.4 Citations",
    "text": "3.4 Citations\nTo add citations, we need to:\n\nCreate a BibTeX (.bib) file that holds the metadata for our references.\nSave the BibTeX file to our project directory (typically in the assets directory)\nInclude bibliography: \"location/bibliography-name.bib\" in the YAML of our QMD document\n\n\n\n3.4.1 Creating .bib Files Using Zotero\nMost reference managers (e.g., Papers, Zotero, Mendelay) can produce BibTex files. Here I will illustrate the process using Zotero.\n\nCreate a New Collection.\nDrag the references you want in your BibTeX database into this collection. For today, drag the Carmichael (1954) and Ross et al. (2020) references into this collection.\nRight-click the collection and select Export Collection.\nIn the pop-up window, change the format of the exported collection to BibTex.\nClick OK.\n\nName the BibTex file (here I named it my-bibliography.bib) and save it in the assets folder.\n\n\n\n\n\n\n\n\n\nWhen you call the BibTex file in the bibliography: key of the YAML in your QMD document, you will need to give the location of the BibTex (relative to the QMD document) and the name you just gave it. For example if you are calling a bibliography in the quarto document assignment-01.qmd that has the following directory/folder structure:\nassignment-01\n    ├── README\n    ├── assets\n       └── my-bibliography.bib\n    ├── assignment-01.qmd\n    ├── assignment-01.Rproj\n    ├── data\n       └── fertility.csv\n    ├── figs\n    └── scripts\n\n\n\n3.4.2 Including the BIB File in your YAML\nYou would then include the following in your Quarto document’s YAML:\n---\ntitle: \"Assignment 1\"\nsubtitle: \"Introduction to Quarto\"\nauthor: \"Your Group's Names\"\ndate: \"January XX, 2023\"\nformat: html\neditor: visual\nbibliography: \"assets/my-bibliography.bib\"\n---\n\n\n\n3.4.3 BibTex Files\nBibTeX files are essentially databases that store bibliographic information in a plain-text (style-independent) file. The database includes a set of references and their metadata. Here is the raw text inside an example BibTex file that includes two articles written by Carmichael (1954) and Ross et al. (2020).\n@article{carmichael_1954,\n    title = {Laziness and the Scholarly Life},\n    volume = {78},\n    number = {4},\n    journal = {The Scientific Monthly},\n    author = {Carmichael, Leonard},\n    year = {1954},\n    pages = {208--213}\n}\n\n@article{ross_2020,\n    title = {Racial disparities in police use of deadly force against unarmed individuals persist after appropriately benchmarking shooting data on violent crime rates},\n    issn = {1948-5506, 1948-5514},\n    url = {http://journals.sagepub.com/doi/10.1177/1948550620916071},\n    doi = {10.1177/1948550620916071},\n    language = {en},\n    urldate = {2020-06-24},\n    journal = {Social Psychological and Personality Science},\n    author = {Ross, Cody T. and Winterhalder, Bruce and McElreath, Richard},\n    month = jun,\n    year = {2020},\n    pages = {194855062091607}\n}\nThe citation identifiers in this example are carmichael_1954 and ross_2020. These identifiers are the first bit of text after the initial curly brace in the reference. This is how we will refer to the citations in our QMD document. (Typically these are auto-generated from our reference manager.) To determine the citiation identifiers, you can open your BibTeX file in RStudio. (Do this by using Open File... within RStudio; double-clicking on the BibTeX file will likely open it in a different application.)\n\n\n\n3.4.4 Including Citations in Your Quarto Document\nCitations go inside square brackets and are separated by semicolons. Each citation must have a key, composed of @citation_identifier from the database (no spaces between them). For example to cite the Carmichael article:\nHere is some text and a citation [@carmichael_1954].\nThis will create a citation where you included it in the text and also add the reference at the end of the document. If you want a section header for your references, include a level-1 heading called “References” at the end of your document.\nThe rendered document now includes a citation where you added this is the QMD document. The associated reference is also included at the end of the document.\n\n\n\n\n\n\n\n\n\nCitations may also include. additional text before and after the citation. In this example we have prefixed the citation with the word “see” and added a page number after the citation. The citation identifier and the text following the citation identifier are separated by a comma.\nHere is some text and a citation [see @carmichael_1954, p. 208].\nThe output of this is:\n\nHere is some text and a citation (see Carmichael, 1954, p. 208).\n\nYou can also include multiple citations. To do this we include the different citation identifiers separated by a semicolon.\nHere are multiple citations [@ross_2020; @carmichael_1954].\nThe output of this is:\n\nHere are multiple citations (Carmichael, 1954; Ross et al., 2020).\n\nWe can also change the format of the citation. For example, here we use a format common to starting a sentence with a citation. To do that we omit the square brackets.\n@carmichael_1954 suggests something is true.\nThe output of this is:\n\nCarmichael (1954) suggests something is true.\n\n\nYou can learn more on the Citations and Footnotes Help Page in the Quarto documentation.\n\n\n\n\n3.4.5 Use APA Formatted Citations and References\nBy default, citations and references are formatted using the Chicago style. To use another style, you will need to:\n\nDownload the appropriate citation style language (CSL) file. (Find many at https://zotero.org/styles)\nPlace the CSL file in the assets folder of your project’s directory.\nSpecify the name of the CSL style file in the csl: key in the QMD file’s YAML.\n\nFor example, say you have downloaded and saved the apa.csl file and placed it in the assets directory, giving the following directory/folder structure:\nassignment-01\n    ├── README\n    ├── assets\n       ├── apa.csl\n       └── my-bibliography.bib\n    ├── assignment-01.qmd\n    ├── assignment-01.Rproj\n    ├── data\n       └── fertility.csv\n    ├── figs\n    └── scripts\nYou would include the following in your YAML.\nIf, for example, you had named the BIB file my-bibliography.bib and had put this file in the assets directory, your YAML would be:\n---\ntitle: \"Assignment 1\"\nsubtitle: \"Introduction to Quarto\"\nauthor: \"Your Group's Names\"\ndate: \"January XX, 2023\"\nformat: html\neditor: visual\nbibliography: \"assets/my-bibliography.bib\"\ncsl: \"assets/apa-single-spaced.csl\"\n---\nUse of the APA CSL file not only formats the references according to APA format, but it also fixed the order of the citations in the text itself! (Even though we included the Ross citation identifier prior to the Carmichael citation identifier in the multiple references example, the APA CSL file put them in the text alphabetically!)\n\n\n\n3.4.6 Citation Extras\nOne R package that is extremely useful, especially if you are using the visual editor in RStudio for writing QMD documents, is {citr}. This package provides functionality and an RStudio Add-In to search a BibTeX-file to create and insert formatted Markdown citations into a QMD document.\n\n\n\n\n\n\n\n\n\nThe {citr} package is only available via GitHub, so you will need to install it using {remotes} or {devtools}. (See the Computational Toolkit book for a reminder on how to do this.) The directions for installing {citr} are also on the citr webpage along with instructions for using the package.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html#learn-more",
    "href": "01-04-more-quarto.html#learn-more",
    "title": "3  More Quarto",
    "section": "4.1 Learn More",
    "text": "4.1 Learn More\nYihui Xie, the creater of the RMarkdown package (which does a lot of the work in converting your QMD file to an HTML/PDF file, gave a nice trechnical talk to the American Statistical Association’s Section on Statistical Computing that explain what actually happens behind the scenes when you click the Knit or Render button in RStudio to render a RMD or QMD document.\n\nAn anatomy of R Markdown with minimal applications",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html#references",
    "href": "01-04-more-quarto.html#references",
    "title": "3  More Quarto",
    "section": "4.2 References",
    "text": "4.2 References\n\n\n\n\n\n\nCarmichael, L. (1954). Laziness and the scholarly life. The Scientific Monthly, 78(4), 208–213.\n\n\nRoss, C. T., Winterhalder, B., & McElreath, R. (2020). Racial disparities in police use of deadly force against unarmed individuals persist after appropriately benchmarking shooting data on violent crime rates. Social Psychological and Personality Science, 194855062091607. https://doi.org/10.1177/1948550620916071",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html",
    "href": "01-05-creating-tables.html",
    "title": "4  Creating Tables with gt",
    "section": "",
    "text": "5 Anatomy of a Table\nAny table is essentially a rectangular layout (rows and columns) of information. Below I show two common tables of statistical output taken from the Regression Review notes. I have also added guide-lines to show the rectangular display of information within each of these tables.\nTo illustrate how to create a table in the QMD document, we will attempt to recreate Table 1 from the Regression Review notes. We can use the fact that data frames are also rectangular displays of information to create a table in the QMD document. The data.frame() function is used to enter the cell information from each column. This is, of course, done in a code chunk.\n# Input cell information\ntab_01 = data.frame(\n  variable = c(\"Environmental policy strength\", \"Corruption\", \"Wealth\", \"Toxic waste severity\", \n               \"Democratic control\", \"Interparty competition\", \"Public environmentalism\"),\n  m = c(17.6, .32, 28.15, 3.53, .63, 39.03, 2.49),\n  sd = c(8.23, .22, 36.38, 1.14, .26, 11.40, .10),\n  min = c(4, 0, 12.78, 0, 0, 9.26, 2.31),\n  max = c(37, .98, 278.01, 5.76, 1, 56.58, 2.7)\n)\n\n# Show output\ntab_01\n\n                       variable     m    sd   min    max\n1 Environmental policy strength 17.60  8.23  4.00  37.00\n2                    Corruption  0.32  0.22  0.00   0.98\n3                        Wealth 28.15 36.38 12.78 278.01\n4          Toxic waste severity  3.53  1.14  0.00   5.76\n5            Democratic control  0.63  0.26  0.00   1.00\n6        Interparty competition 39.03 11.40  9.26  56.58\n7       Public environmentalism  2.49  0.10  2.31   2.70",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#column-labels",
    "href": "01-05-creating-tables.html#column-labels",
    "title": "4  Creating Tables with gt",
    "section": "6.1 Column Labels",
    "text": "6.1 Column Labels\nColumn labels can be changed from the names of the columns used in the data frame. To change them we will use the cols_label() function. This function takes as many arguments as there are columns, each mapping a label to the original column name. Below, we change our column names to match those in Table 1.\n\n# Change Column labels\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = \"Variable\",\n    m = \"M\",\n    sd = \"SD\",\n    min = \"Min.\",\n    max = \"Max.\"\n  )\n\n\n\n\n\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\nWe can also use the md() function to include Markdown syntax to further format our labels. For example, to make the column labels italics we use the following.\n\n# Change Column labels to italics\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) \n\n\n\n  \n  \n\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\nThe names we just gave to the variables are only labels. As we refer to the columns in additional functions, we need to use their original names from the data frame.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#column-alignment",
    "href": "01-05-creating-tables.html#column-alignment",
    "title": "4  Creating Tables with gt",
    "section": "6.2 Column Alignment",
    "text": "6.2 Column Alignment\nTo change the column alignment, we use the cols_align() function. We provide this with two arguments. The columns= argument takes a vector of column names using the c() function, and the align= argument takes a character string of \"left\", \"right\", or \"center\". Following typical formatting rules, we left align text columns and center numerical columns.\n\n# Change Column labels to italics\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |&gt;\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  )\n\n\n\n  \n  \n\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\nLearn more in the documentation: https://gt.rstudio.com/reference/cols_align.html",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#adding-a-title-and-subtitle-using-quarto",
    "href": "01-05-creating-tables.html#adding-a-title-and-subtitle-using-quarto",
    "title": "4  Creating Tables with gt",
    "section": "6.3 Adding a Title and Subtitle (Using Quarto)",
    "text": "6.3 Adding a Title and Subtitle (Using Quarto)\nIf you are creating a table within a QMD document, we will use the code chunk options to add a table number, title, and subtitle. To create these use the following code chunks:\n\nThe label: field is used to label the table and give it a table number. The key here is that the lable name must start withtbl-. In the example below, the label name I gave the table is tbl-summary-measures.\nThe tbl-cap: field is used to provide a table caption, similar to how fig-cap: is used to give a figure caption.\nThe tbl-subcap: field is used to provide a table subcaption. Not all tables need a subcaption!\nThe tbl-cap-location: field can be use to change the table caption location from top to bottom, or even have it placed in the margin.\n\nHere we show the R code chunk and chunk options to produce the table numbering and caption. (I do not use a subcaption on this table.) The resulting table, along with its numbering (which is autopopulated by Quarto), and caption is shown below.\n\n```{r}\n#| label: tbl-summary-measures\n#| tbl-cap: \"Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\"\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |&gt;\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  ) |&gt;\n  opt_align_table_header(\"left\")\n```\n\n\n\nTable 6.1: Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\n\n\n\n\n\n\n\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\n\n\n\nThe advantage to including a label: and naming it with a tbl- prefix is that it makes your tables cross-referenceable in the document. In the document’s text you can link to that table using the @ and giving the table name. For example, see Table 6.1. To do this I wrote the following in my Quarto document:\nFor example, see @tbl-summary-measures.\nSee here for more detail about cross-referencing tables, substables, figures, equations, and more!\n\n6.3.1 Changing the Formatting of the Table Numbering\nWe can change how Quarto formats the table labeling in the YAML part of the document. The default is: “Table X: Table Caption”. Here we make “Table X.” bold with a period after it, where “X” is an arabic number.\n\n---\ntitle: \"My First Quarto Document\"\nformat: html\ncrossref:\n  tbl-title: \"**Table**\"\n  tbl-labels: arabic\n  title-delim: \".\"\n---",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#adding-a-title-and-subtitle-not-using-quarto",
    "href": "01-05-creating-tables.html#adding-a-title-and-subtitle-not-using-quarto",
    "title": "4  Creating Tables with gt",
    "section": "6.4 Adding a Title and Subtitle (Not Using Quarto)",
    "text": "6.4 Adding a Title and Subtitle (Not Using Quarto)\nThe tab_header() function can be used to add a title or subtitle to your table. Here we again use the md() function to allow us to use Markdown syntax directly in the title. I also use the opt_align_table_header() function to left align the title and subtitle per APA.\n\n# Add title and subtitle\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |&gt;\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  ) |&gt;\n    tab_header(\n    title = md(\"**Table 1.** Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\"),\n  ) |&gt;\n  opt_align_table_header(\"left\")\n\n\n\n  \n  \n\n\n\nTable 1. Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\nSince the title and subtitle appear on separate lines, you can take advantage of that to use the title to provide the table number and the subtitle provides the table caption if you are trying to format in APA style.\n\n\nLearn more in the documentation: https://gt.rstudio.com/reference/tab_header.html",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#fine-tuning-the-table",
    "href": "01-05-creating-tables.html#fine-tuning-the-table",
    "title": "4  Creating Tables with gt",
    "section": "6.5 Fine-Tuning the Table",
    "text": "6.5 Fine-Tuning the Table\nThe table is very close to matching the original Table 1. But, there are still a couple of things (if you are an Enneagram One) that we need to attend to. For example, we could remove the horizontal lines in the table. These lines are called “borders” and we can modify them in the tab_style() function. This is a general function that allows us to customize many parts of the table (akin to theme() in ggplot()).\nTo do this we use the style= argument and call the cell_borders() function within tab_style(). Here we remove all borders (top, bottom, left, and right) by using sides=\"all\" and setting style=NULL. The tab_style() function also requires the argument locations=. We give this argument the function cell_body() which we provide the column and row numbers that we want to remove the borders from. Since we want to keep the horizontal border associated with the first and last rows, we omit those row numbers from the rows= argument.\n\n# Add title and subtitle\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |&gt;\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  ) |&gt;\n    tab_header(\n    title = md(\"**Table 1.** Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\"),\n  ) |&gt;\n  opt_align_table_header(\"left\")  |&gt;\n  tab_style(\n    style = cell_borders(\n      sides = \"all\", \n      style = NULL\n      ),\n    locations = cells_body(\n      columns = 1:5,\n      rows = 2:6\n    )\n  )\n\n\n\n  \n  \n\n\n\nTable 1. Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#formatting-decimal-places",
    "href": "01-05-creating-tables.html#formatting-decimal-places",
    "title": "4  Creating Tables with gt",
    "section": "7.1 Formatting Decimal Places",
    "text": "7.1 Formatting Decimal Places\nIn these tables it is typical to format the number of decimal places to 2 for coefficients, standard errors, and t-values and to three decimal places for p-values. To do this, we will use the fmt_number() function from {gt}.\n\ntidy(lm.1) |&gt;\n  gt() |&gt;\n  cols_label(\n    term = md(\"*Variable*\"),\n    estimate = md(\"*B*\"),\n    std.error = md(\"*SE*\"),\n    statistic = md(\"*t*\"),\n    p.value = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(term),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(estimate, std.error, statistic, p.value),\n    align = \"center\"\n  ) |&gt;\n    fmt_number(\n    columns = c(estimate, std.error, statistic),\n    decimals = 2\n  ) |&gt;\n    fmt_number(\n    columns = p.value,\n    decimals = 3\n  )\n\n\n\nTable 7.2: Unstandardized regression coefficients for a model to predict variation in environmental program strength.\n\n\n\n\n\n\n\n\n\nVariable\nB\nSE\nt\np\n\n\n\n\n(Intercept)\n20.49\n3.37\n6.07\n0.000\n\n\ncorrupt\n−10.02\n5.26\n−1.91\n0.063\n\n\ntoxicwaste\n0.08\n0.82\n0.09\n0.927\n\n\n\n\n\n\n\n\n\n\n\nLearn more in the documentation: https://gt.rstudio.com/reference/fmt_number.html\n\nThe p-value for the intercept is now “0.000” after rounding to three decimal places. It is conventional in this case to report it as “&lt;0.001” rather than “0.000”. To do this we will use the sub_values() function from {gt} to search the p.value column for any value less than .001 and replace it with the text “&lt;.001”.\n\ntidy(lm.1) |&gt;\n  gt() |&gt;\n  cols_label(\n    term = md(\"*Variable*\"),\n    estimate = md(\"*B*\"),\n    std.error = md(\"*SE*\"),\n    statistic = md(\"*t*\"),\n    p.value = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(term),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(estimate, std.error, statistic, p.value),\n    align = \"center\"\n  ) |&gt;\n    fmt_number(\n    columns = c(estimate, std.error, statistic),\n    decimals = 2\n  ) |&gt;\n    fmt_number(\n    columns = p.value,\n    decimals = 3\n  ) |&gt;\n  sub_values(columns = p.value, fn = function(x) x &lt; .001, replacement = \"&lt;.001\")\n\n\n\nTable 7.3: Unstandardized regression coefficients for a model to predict variation in environmental program strength.\n\n\n\n\n\n\n\n\n\nVariable\nB\nSE\nt\np\n\n\n\n\n(Intercept)\n20.49\n3.37\n6.07\n&lt;.001\n\n\ncorrupt\n−10.02\n5.26\n−1.91\n0.063\n\n\ntoxicwaste\n0.08\n0.82\n0.09\n0.927\n\n\n\n\n\n\n\n\n\n\nWe can also use the sub_values() function to change the text in the term column to make our vasriable names more human-friendly. To do this we again specify the column name, and then indicate the values we want to replace along with the replacement text.\n\ntidy(lm.1) |&gt;\n  gt() |&gt;\n  cols_label(\n    term = md(\"*Variable*\"),\n    estimate = md(\"*B*\"),\n    std.error = md(\"*SE*\"),\n    statistic = md(\"*t*\"),\n    p.value = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(term),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(estimate, std.error, statistic, p.value),\n    align = \"center\"\n  ) |&gt;\n    fmt_number(\n    columns = c(estimate, std.error, statistic),\n    decimals = 2\n  ) |&gt;\n    fmt_number(\n    columns = p.value,\n    decimals = 3\n  ) |&gt;\n  sub_values(columns = p.value, fn = function(x) x &lt; .001, replacement = \"&lt;.001\") |&gt;\n  sub_values(columns = term, values = \"(Intercept)\", replacement = \"Constant\") |&gt;\n  sub_values(columns = term, values = \"corrupt\", replacement = \"Political Corruption\") |&gt;\n  sub_values(columns = term, values = \"toxicwaste\", replacement = \"Toxic Waste\")\n\n\n\nTable 7.4: Unstandardized regression coefficients for a model to predict variation in environmental program strength.\n\n\n\n\n\n\n\n\n\nVariable\nB\nSE\nt\np\n\n\n\n\nConstant\n20.49\n3.37\n6.07\n&lt;.001\n\n\nPolitical Corruption\n−10.02\n5.26\n−1.91\n0.063\n\n\nToxic Waste\n0.08\n0.82\n0.09\n0.927\n\n\n\n\n\n\n\n\n\n\n\nLearn more in the documentation: https://gt.rstudio.com/reference/sub_values.html",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#adding-a-footnote",
    "href": "01-05-creating-tables.html#adding-a-footnote",
    "title": "4  Creating Tables with gt",
    "section": "7.2 Adding a Footnote",
    "text": "7.2 Adding a Footnote\nWe can add a footnote to our table using the tab_footnote() function. This function takes the argument footnote= which gives the text for the footnote. It also requires the locations= argument to indicate where the actual footnote symbol should be in the table. This is specified using one of the many cells_*() helper functions (e.g., cells_body(), cells_column_labels()).\nBelow we use the cells_body() function to indicate the location of the actual footnote. This function takes the row and column where the footnote should go. Here we want to place the footnote next to the text “Political Corruption” which is in the second row of the table and the first column.\n\ntidy(lm.1) |&gt;\n  gt() |&gt;\n  cols_label(\n    term = md(\"*Variable*\"),\n    estimate = md(\"*B*\"),\n    std.error = md(\"*SE*\"),\n    statistic = md(\"*t*\"),\n    p.value = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(term),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(estimate, std.error, statistic, p.value),\n    align = \"center\"\n  ) |&gt;\n    fmt_number(\n    columns = c(estimate, std.error, statistic),\n    decimals = 2\n  ) |&gt;\n    fmt_number(\n    columns = p.value,\n    decimals = 3\n  ) |&gt;\n  sub_values(columns = p.value, fn = function(x) x &lt; .001, replacement = \"&lt;.001\") |&gt;\n  sub_values(columns = term, values = \"(Intercept)\", replacement = \"Constant\") |&gt;\n  sub_values(columns = term, values = \"corrupt\", replacement = \"Political Corruption\") |&gt;\n  sub_values(columns = term, values = \"toxicwaste\", replacement = \"Toxic Waste\") |&gt;\n  tab_footnote(\n    footnote = \"Political corruption is a dummy-coded variable (0 = no corruption; 1 = corruption).\",\n    locations = cells_body(rows = 2, columns = 1)\n  )\n\n\n\nTable 7.5: Unstandardized regression coefficients for a model to predict variation in environmental program strength.\n\n\n\n\n\n\n\n\n\nVariable\nB\nSE\nt\np\n\n\n\n\nConstant\n20.49\n3.37\n6.07\n&lt;.001\n\n\nPolitical Corruption1\n−10.02\n5.26\n−1.91\n0.063\n\n\nToxic Waste\n0.08\n0.82\n0.09\n0.927\n\n\n\n1 Political corruption is a dummy-coded variable (0 = no corruption; 1 = corruption).\n\n\n\n\n\n\n\n\n\n\n\n\nLearn more about different footnote options in the documentation: https://gt.rstudio.com/reference/tab_footnote.html",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "02-00-likelihood.html",
    "href": "02-00-likelihood.html",
    "title": "Likelihood",
    "section": "",
    "text": "Many of the advanced statistical methods use estimation and testing methods based on likelihood. In this section we introduce likelihood for testing and estimating model parameters. To understand likelihood, you need to have some knowledge of probability density and logarithms. For this reason, we begin this section by introducing these two foundational ideas.",
    "crumbs": [
      "Likelihood"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html",
    "href": "02-01-mathematical-foundations-probability-density.html",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "",
    "text": "5.1 Terminology and Basic Ideas of Probability Distributions\nUnderstanding ideas about probability distributions (especially continuous probability distributions) is essential for understanding likelihood.\nA probability distribution is simply a mapping of every outcome in the sample space to a probability. (It is conventional to use x to denote the outcomes.) This mapping can take many forms such a list, a plot, or an equation. All are reasonable to define the probability distribution, although some of these are more typical for discrete variables (e.g., list, plot), while others are more typical for continuous distributions (e.g., plot, equation). Here are a couple different probability distributions:\nFigure 5.1: An example of a discrete and continuous probability distribution.\nIn general, the probability of an outcome (p(x)) is defined as the number of times an outcome appears in the sample space divided by the total number of outcomes. So, for example, in the probability experiment of flipping a fair coin twice, the sample space is:\n\\[\nS = \\{HH,~HT,~TH,~TT\\}\n\\]\nThe probability of a head and a tail (assuming order doesn’t matter) is:\n\\[\np(HT~\\mathrm{or}~TH) = \\frac{2}{4} = 0.5\n\\] This falls apart when we move to continuous distributions. For example, consider finding the probability of a zero occurring in the standard normal distribution given in Figure 6.1. The sample space includes every outcome possible and is:\n\\[\nS = \\{-\\infty, ~ +\\infty\\}\n\\] so the probability of zero (which occurs once in that sample space) is:\n\\[\np(0) = \\frac{1}{\\infty} \\approx 0\n\\]\nLikewise the probability of 1 or 2 is also:\n\\[\n\\begin{split}\np(1) = \\frac{1}{\\infty} \\approx 0 \\\\\np(2) = \\frac{1}{\\infty} \\approx 0\n\\end{split}\n\\]\nIn fact the probability of every possible outcome is 0. The problem is that if we add all the probabilities together, we have to get 1 (Law of Probability), and in this case we don’t; the sum of \\(0 + 0 + \\ldots + 0 = 0\\). Because of this, in continuous distributions, we typically compute the probability of a range of values. For example, in our standard normal distribution:\n\\[\n\\begin{split}\np(x \\leq 0) = 0.5 \\\\\np(-1 \\leq x \\leq 1) = 0.68 \\\\\np(-2 \\leq x \\leq 2) = 0.95\n\\end{split}\n\\]",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#terminology-and-basic-ideas-of-probability-distributions",
    "href": "02-01-mathematical-foundations-probability-density.html#terminology-and-basic-ideas-of-probability-distributions",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "",
    "text": "5.1.1 Probability Density\nIn a continuous distribution we also need to account be able to talk about the fact that some outcomes are more likely than other outcomes. For example, in our standard normal distribution outcomes near zero are more probable than outcomes near 1, which are more probable than outcomes near 2, etc. Since we can’t use probability to do this (remember the probability of each outcome is the same, namely 0), we use something called probability density. This is akin to a relative probability, so outcomes with a higher probability density are more likely than outcomes with a lower probability density.\nThe mapping of all the outcomes to their probability densities is called a probability density function (PDF). Thus the equation or “bell-shaped” curve describing the standard normal distribution in Figure 6.1 is technically a PDF Here are some laws governing PDFs:\n\nProbability densities are always positive.\nThe probability of an outcome x between a and b equals the integral (area under the curve) between a and b of the probability density function. That is:\n\n\\[\np(a \\leq x \\leq b) = \\int_a^b p(x) dx\n\\]\n\nThe area under the curve from negative infinity to positive infinity is 1. That is:\n\n\\[\np(-\\infty \\leq x \\leq +\\infty) = \\int_{-\\infty}^{+\\infty} p(x) = 1\n\\]\nNext we will look at the PDF for a normal distribution.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#pdf-for-a-normal-gaussian-distribution",
    "href": "02-01-mathematical-foundations-probability-density.html#pdf-for-a-normal-gaussian-distribution",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.2 PDF for a Normal (Gaussian) Distribution",
    "text": "5.2 PDF for a Normal (Gaussian) Distribution\nThe probability density function (PDF) of a normal distribution is mathematically defined as:\n\\[\np(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]\n\\]\nfor \\(-\\infty \\leq x \\leq \\infty\\).\nTo calculate the probability density, we need three pieces of information: (1) the outcome (x-value) for which we want to determine the probability density, (2) the mean (\\(\\mu\\)) of the normal distribution x is a member of, and (3) the standard deviation (\\(\\sigma\\)) of the normal distribution x is a member of. Then, we can compute the probability density (\\(p(x)\\)) for a particular \\(x\\) value by using the equation.\nAs an example, consider a normal distribution with a mean of 50, and a standard deviation of 10. The probability density for \\(x=65\\) can be found using,\n\\[\n\\begin{split}\np(65) &= \\frac{1}{10\\sqrt{2\\pi}}\\exp\\left[-\\frac{(65-50)^2}{2\\times10^2}\\right] \\\\[1ex]\n&= 0.01295176\n\\end{split}\n\\]\nUsing R, we can carry out the computation,\n\n# Compute the probability density of x=65 in N(50,10)\n(1 / (10 * sqrt(2 * pi))) * exp(-(225) / 200)\n\n[1] 0.01295176\n\n\nThere is also a more direct way to compute this using the dnorm() function. This function computes the density of x from a normal distribution with a specified mean and sd.\n\n# Compute the probability density of x=65 in N(50,10)\ndnorm(x = 65, mean = 50, sd = 10)\n\n[1] 0.01295176\n\n\nSymbolically, we might write\n\\[\nP\\bigg(x=65 \\mid \\mathcal{N}(50,10)\\bigg) = 0.01295176\n\\]\nwhich is read:\n\nThe probability density of \\(x=65\\) GIVEN the normal distribution having a mean of 50 and standard deviation of 10 is equal to 0.013.\n\nNote that the probability density for a value is not only a function of x, but also depends on the mean and standard deviation of the normal distribution. For example, the probability density of \\(x=65\\) in the normal distribution having a mean of 30 and standard deviation of 20 is a different value than the probability density we found earlier.\n\n# Compute the probability density of x=65 in N(30,20)\ndnorm(x = 65, mean = 30, sd = 20)\n\n[1] 0.004313866\n\n\nHere,\n\\[\nP\\bigg(x=65 \\mid \\mathcal{N}(30,20)\\bigg) = 0.004313866\n\\]\nIn general, when we think about the normal distribution, we are thinking about the mapping of each x-value from \\(-\\infty\\) to \\(+\\infty\\) to its associated probability density. Rather than list each of these mappings out, we can create a plot of these mappings. This plot gives us the familiar “bell shape”. Theoretically this plot is the graphical depiction of the PDF.\n\n\nCode\n# Load library\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\n# Create dataset\nfig_02 = data.frame(\n  X = seq(from = -40, to = 90, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y1 = dnorm(x = X, mean = 50, sd = 10),\n    Y2 = dnorm(x = X, mean = 30, sd = 20)\n    )\n\n# Create plot\nggplot(data = fig_02, aes(x = X, y = Y1)) +\n  geom_line(color = \"#0085af\", linetype = \"solid\") +\n  geom_line(aes(y = Y2), linetype = \"dashed\", color = \"#c62f4b\") +\n  xlab(\"X\") +\n  ylab(\"Probability density\") +\n  theme_light() +\n  geom_point(x = 65, y = 0.01295176,  size = 3, color = \"#0085af\") +\n  geom_point(x = 65, y = 0.004313866, size = 3, color = \"#c62f4b\")\n\n\n\n\n\n\n\n\nFigure 5.2: Plot of the probability density function (PDF) for a \\(\\mathcal{N}(50,10)\\) distribution (blue, solid line) and for a \\(\\mathcal{N}(30,20)\\) distribution (red, dashed line). The probability density value for \\(x=65\\) is also displayed on both PDFs.\n\n\n\n\n\nOf course, the PDF is different for normal distributions with different means (\\(\\mu\\)) or standard deviations (\\(\\sigma\\)). This implies that there is not one normal distribution, but rather an infinite number of normal distributions, each with a different mean or standard deviation. (We refer to the normal distribution as a “family” of distributions.)\nTo completely define the PDF we need to specify the mean and standard deviation we are using to compute the probability densities. Specifying these values is referred to as parameterizing the distribution1.\n\n\n5.2.1 Other Useful R Functions for Working with Normal Probability Distributions\nWe use dnorm() when we want to compute the probability density associated with a particular x-value in a given normal distribution. There are three other functions that are quite useful for working with the normal probability distribution:\n\npnorm() : To compute the probability (area under the PDF)\nqnorm() : To compute the \\(x\\) value given a particular probability\nrnorm() : To draw a random observation from the distribution\n\nEach of these function also requires the arguments mean= and sd=. Below we will examine how to use each of these additional functions.\n\n\n\n5.2.2 pnorm(): Computing Cumulative Probability Density\nThe function pnorm() computes the area under the PDF curve from \\(-\\infty\\) to some x-value. (Sometimes this is referred to as the cumulative probability density of x.) It is important to note that the PDF is defined such that the entire area under the curve is equal to 1. Because of this, we can also think about using area under the curve as an analog to probability in a continuous distribution.\nFor example, we might ask about the probability of observing an x-value that is less than or equal to 65 given it is from a \\(\\mathcal{N}(50,10)\\) distribution. Symbolically, we want to find:\n\\[\nP\\bigg(x \\leq 65 \\mid \\mathcal{N}(50,10)\\bigg)\n\\]\nThis is akin to finding the proportion of the area under the \\(\\mathcal{N}(50,10)\\) PDF that is to the left of 65. The figure below shows a graphical depiction of the cumulative probability density for \\(x=65\\).\n\n\nCode\n# Create dataset\nfig_03 = data.frame(\n  X = seq(from = 10, to = 90, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = dnorm(x = X, mean = 50, sd = 10)\n    )\n\n# Filter out X&lt;=65\nshaded = fig_03 %&gt;%\n  filter(X &lt;= 65)\n\n# Create plot\nggplot(data = fig_03, aes(x = X, y = Y)) +\n  geom_ribbon(data = shaded, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"X\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\n\n\n\nFigure 5.3: Plot of the probability density function (PDF) for a N(50,10) distribution. The area that is shaded grey (relative to the total area under the PDF) represents the cumulative probability density for x=65.\n\n\n\n\n\nWe can compute the cumulative probability density using the pnorm() function. The “p” stand for “probability”.\n\n# Find P(x&lt;=65 | N(50,10) )\npnorm(q = 65, mean = 50, sd = 10)\n\n[1] 0.9331928\n\n\nWe can interpret this as:\n\nThe probability of observing an x-value that is less than or equal to 65 (if it is drawn from a normal distribution with a mean of 50 and standard deviation of 10) is 0.933.\n\nIn mathematics, the area under a curve is called an integral. The grey-shaded area in the previous figure can also be expressed as an integral of the probability density function:\n\\[\n\\int_{-\\infty}^{65} p(x) dx\n\\]\nwhere \\(p(x)\\) is the PDF for the normal distribution.\nThe most common application for finding the cumulative density is to compute a p-value. The p-value is just the area under the distribution (curve) that is AT LEAST as extreme as some observed value. For example, assume we computed a test statistic of \\(z=2.5\\), and were evaluating whether this was different from 0 (two-tailed test). Graphically, we want to determine the proportion of the area under the PDF that is shaded grey in the figure below.\n\n\nCode\n# Create data\nfig_04 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = dnorm(x = X, mean = 0, sd = 1)\n    )\n\n# Filter data for shading\nshade_01 = fig_04 %&gt;%\n  filter(X &gt;= 2.5)\n\nshade_02 = fig_04 %&gt;%\n  filter(X &lt;= -2.5)\n\n# Create plot\nggplot(data = fig_04, aes(x = X, y = Y)) +\n  geom_ribbon(data = shade_01, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_ribbon(data = shade_02, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"z\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\n\n\n\nFigure 5.4: Plot of the probability density function (PDF) for the standard normal distribution (\\(M=0\\), \\(SD=1\\)). The cumulative density representing the p-value for a two-tailed test evaluating whether \\(\\mu=0\\) using an observed mean of 2.5 is also displayed.\n\n\n\n\n\nIf the distribution of the test statistic is normally distributed, we can use pnorm() to compute the p-value. If we assume the test statistic, z, has been scaled to use standardized units, the standard deviation we use in pnorm() will be sd=1. The mean is based on the value being tested in the null hypothesis. In most null hypotheses, we are testing a difference from 0 (e.g., \\(H_0: \\mu=0\\), \\(H_0: \\beta=0\\)), so we would use mean=0 in the pnorm() function.\nRemember, pnorm() computes the proportion of the area under the curve TO THE LEFT of a particular value. Here we will compute the area to the left of \\(-2.5\\) and then double it to produce the actual p-value. (We can double it because the normal distribution is symmetric so the area to the left of \\(-2.5\\) is the same as the area to the right of \\(+2.5\\).)\n\n# Compute the p-value based on z=2.5\n2 * pnorm(q = -2.5, mean = 0, sd = 1)\n\n[1] 0.01241933\n\n\nWe interpret this p-value as:\n\nThe probability of observing a statistic at least as extreme as 2.5, assuming the null hypothesis is true, is 0.012. This is evidence against the null hypothesis since the data are inconsistent with the assumed hypothesis.\n\n\n\n\n5.2.3 qnorm(): Computing Quantiles\nThe qnorm() function is essentially the inverse of the pnorm() function. The pnorm() function computes the cumulative probability GIVEN a particular quantile (x-value). The qnorm() function computes the quantile GIVEN a cumulative probability. For example, in the \\(\\mathcal{N}(50, 10)\\) distribution, half of the area under the PDF is below the x-value (quantile) of 50.\nTo use the qnorm() function to give the x-value (quantile) that defines the lower 0.5 of the area under the \\(\\mathcal{N}(50, 10)\\) PDF, the syntax would be:\n\n# Find the quantile that has a cumulative density of 0.5 in the N(50, 10) distribution\nqnorm(p = 0.5, mean = 50, sd = 10)\n\n[1] 50\n\n\n\n\n\n5.2.4 rnorm(): Generating Random Observations\nThe rnorm() function can be used to generate random observations drawn from a specified normal distribution. Aside from the mean= and sd= arguments, we also need to specify the number of observations to generate by including the argument n=. For example, to generate 15 observations drawn from a \\(\\mathcal{N}(50,10)\\) distribution we would use the following syntax:\n\n# Generate 15 observations from N(50,10)\nset.seed(100)\nrnorm(n = 15, mean = 50, sd = 10)\n\n [1] 44.97808 51.31531 49.21083 58.86785 51.16971 53.18630 44.18209 57.14533\n [9] 41.74741 46.40138 50.89886 50.96274 47.98366 57.39840 51.23380\n\n\nThe set.seed() function sets the state of the random number generator used in R so that the results are reproducible. If you don’t use set.seed() you will get a different set of observations each time you run rnorm(). Here we set the starting seed to 100, but you can set this to any integer you want.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#students-t-distribution",
    "href": "02-01-mathematical-foundations-probability-density.html#students-t-distribution",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.3 Student’s t-Distribution",
    "text": "5.3 Student’s t-Distribution\nThe PDF of Student’s t-distribution looks similar to the PDF for a standard normal distribution. In the figure below, Student’s t-distribution is depicted with a solid, black line and the standard normal distribution (\\(M=0\\), \\(SD=1\\)) is depicted with a dotted, red line.\n\n\nCode\n# Create data\nfig_05 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n) %&gt;%\n  mutate(\n    Y_t = dt(x = X, df = 5),\n    Y_norm = dnorm(x = X, mean = 0, sd = 1)\n    ) \n\n# Create plot\nggplot(data = fig_05, aes(x = X, y = Y_t)) +\n  geom_line() +\n  geom_line(aes(y = Y_norm), color = \"red\", linetype = \"dotted\") +\n  xlab(\"t\") +\n  ylab(\"Probability density\") +\n  theme_light() +\n  geom_vline(xintercept = 0)\n\n\n\n\n\n\n\n\nFigure 5.5: Plot of the probability density function (PDF) for both the standard normal distribution (dotted, red line) and Student’s \\(t(5)\\) distribution (solid, black line).\n\n\n\n\n\n\nBoth the standard normal distribution and Student’s t-distribution are symmetric distributions.\nBoth the standard normal distribution and Student’s t-distribution have a mean (expected value) of 0.\nThe standard deviation for Student’s t-distribution is, however, larger than the standard deviation for the standard normal distribution (\\(SD&gt;1\\)). You can see this in the distribution because the tails in Student’s t-distribution are fatter (more error) than the standard normal distribution.\n\nIn practice, we often use Student’s t-distribution rather than the standard normal distribution in our evaluations of sample data. This is because the increased error (i.e., standard deviation) associated with Student’s t-distribution better models the additional uncertainty associated with having incomplete information (i.e., a sample rather than the entire population).\nStudent’s t-distribution also constitutes a family of distributions; there is not a single t-distribution. The specific shape (and thus probability density) is defined by a parameter called the degrees of freedom (df). The plot below shows the standard normal distribution (purple) and four t-distributions with varying df-values. The means and standard deviations for each of these distributions is also provided in a table.\n\n\nCode\n# Create data for t(3)\ndf_03 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = dt(x = X, df = 3),\n    df = \"03\"\n    ) \n\n# Create data for t(5)\ndf_05 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = dt(x = X, df = 5),\n    df = \"05\"\n    )\n\n# Create data for t(10)\ndf_10 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n) %&gt;%\n   mutate(\n    Y = dt(x = X, df = 10),\n    df = \"10\"\n    )\n\n# Create data for t(25)\ndf_25 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = dt(x = X, df = 25),\n    df = \"25\"\n    )\n\n# Create data for standard normal\nz = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = dnorm(x = X, mean = 0, sd = 1),\n    df = \"Standard normal\"\n    )\n\n# Combine all datasets into one\nfig_06 = rbind(df_03, df_05, df_10, df_25, z)\n\n# Create plot\nggplot(data = fig_06, aes(x = X, y = Y, color = df, linetype = df)) +\n  geom_line() +\n  xlab(\"t\") +\n  ylab(\"Probability density\") +\n  theme_light() +\n  geom_vline(xintercept = 0) +\n  ggsci::scale_color_d3() +\n  scale_linetype_manual(values = 5:1)\n\n\n\n\n\n\n\n\nFigure 5.6: Plot of several t-Distributions with differing degrees of freedom.\n\n\n\n\n\n\n\nCode\n# Load library\nlibrary(gt)\n\n# Create data from table\ntab_01 = data.frame(\n  df = c(\"3\", \"5\", \"10\", \"25\", \"z\"),\n  M = 0.00,\n  SD = c(2, 1.5, 1.22, 1.08, 1.00)\n)\n\n# Create table\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    df = md(\"*df*\"),\n    M = md(\"*M*\"),\n    SD = md(\"*SD*\")\n  ) |&gt;\n  cols_align(\n    columns = c(df, M, SD),\n    align = \"center\"\n  )\n\n\n\n\nTable 5.1: Means and standard deviations for four t-Distributions and the standard normal distribution.\n\n\n\n\n\n\n\n\n\ndf\nM\nSD\n\n\n\n\n3\n0\n2.00\n\n\n5\n0\n1.50\n\n\n10\n0\n1.22\n\n\n25\n0\n1.08\n\n\nz\n0\n1.00\n\n\n\n\n\n\n\n\n\n\nIf we compare the means and standard deviations for these distributions, we find that the mean for all the t-distributions is 0, same as the standard normal distribution. All t-distributions are unimodal and symmetric around zero. The standard deviation for every t-distribution is higher than the standard deviation for the standard normal distribution. Mathematically, the variance for the t-distribution is:\n\\[\n\\sigma^2(t) = \\frac{\\nu}{\\nu-2}\n\\]\nwhere \\(\\nu\\) (the Greek letter nu) is the degrees of freedom. (Note that \\(\\nu \\geq 2\\).) Examining this formula, we find that Student t-distributions with higher df values have less variation. When \\(\\nu=+\\infty\\), the variance approaches 1, which is the same as the standard normal distribution.\nThere are four primary functions for working with Student’s t-distribution:\n\ndt() : To compute the probability density (point on the curve)\npt() : To compute the cumulative density (area under the PDF)\nqt() : To compute the quantile value given a particular probability\nrt() : To draw a random observation from the distribution\n\nEach of these requires the argument df=.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#the-f-distribution",
    "href": "02-01-mathematical-foundations-probability-density.html#the-f-distribution",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.4 The F-distribution",
    "text": "5.4 The F-distribution\nThe F-distribution, like the t-distribution, constitutes a family of distributions. They are positively skewed and generally have a lower-limit of 0. To parameterize an F-distribution we need two parameters, namely \\(\\nu_1\\) and \\(\\nu_2\\). These are both degrees of freedom. The exact shape of the F-distribution s governed by the two degrees of freedom parameters. The figure below shows several F-distributions with different degrees of freedom.\n\n\nCode\n# Create data for F(1,10)\nf1 = data.frame(\n  X = seq(from = 0, to = 10, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = df(x = X, df1 = 1, df2 = 10),\n    df = \"F(1, 10)\"\n    ) \n\n# Create data for F(5,25)\nf2 = data.frame(\n  X = seq(from = 0, to = 10, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = df(x = X, df1 = 5, df2 = 25),\n    df = \"F(5, 25)\"\n    )\n\n# Create data for F(15,30)\nf3 = data.frame(\n  X = seq(from = 0, to = 10, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = df(x = X, df1 = 15, df2 = 30),\n    df = \"F(15, 30)\"\n    )\n\n# Create data for F(30,60)\nf4 = data.frame(\n  X = seq(from = 0, to = 10, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = df(x = X, df1 = 30, df2 = 60),\n    df = \"F(30, 60)\"\n    )\n\n# Combine all datasets into one\nfig_07 = rbind(f1, f2, f3, f4)\n\n# Create plot\nggplot(data = fig_07, aes(x = X, y = Y, color = df, linetype = df)) +\n  geom_line() +\n  xlab(\"F\") +\n  ylab(\"Probability density\") +\n  theme_light() +\n  ggsci::scale_color_d3(name = \"\") +\n  scale_linetype_manual(name = \"\", values = 1:4)\n\n\n\n\n\n\n\n\nFigure 5.7: Plot of several F-Distributions with differing degrees of freedom.\n\n\n\n\n\nThe expected value (mean) and standard deviation of the F-distribution is:\n\\[\n\\begin{split}\nE(F) &= \\frac{\\nu_2}{\\nu_2 - 2} \\\\[1ex]\n\\sigma^2(F) &= \\frac{2\\nu_2^2(\\nu_1 + \\nu_2 - 2)}{\\nu_1(\\nu_2-2)^2(\\nu_2-4)}\n\\end{split}\n\\]\nwhere \\(\\nu_2 &gt; 2\\) for the mean and \\(\\nu_2 &gt; 4\\) for the variance.\nFrom these formulas we can see that as \\(\\nu_2 \\rightarrow +\\infty\\) the mean of the F-distribution approaches 1. We can also see that the variation in the F-distribution is a function of both parameters and the variance decreases as either parameter gets larger.\nThe means and standard deviations for our four example F-distributions are given in the table below.\n\n\nCode\n# Create data from table\ntab_02 = data.frame(\n  df1 = c(1, 15, 30, 5),\n  df2 = c(10, 30, 60, 25)\n  ) %&gt;%\n  mutate(       \n    M = df2 / (df2 - 2),\n    SD = sqrt((2*df2^2*(df1 + df2 - 2)) / (df1*(df2-2)^2*(df2-4)))\n  )\n\n# Create table\ntab_02 |&gt;\n  gt() |&gt;\n  cols_label(\n    df1 = md(\"*df1*\"),\n    df2 = md(\"*df2*\"),\n    M = md(\"*M*\"),\n    SD = md(\"*SD*\")\n  ) |&gt;\n  cols_align(\n    columns = c(df1, df2, M, SD),\n    align = \"center\"\n  ) |&gt;\n  fmt_number(\n    columns = c(M, SD),\n    decimals = 2\n  )\n\n\n\n\nTable 5.2: Means and standard deviations for four F-distributions.\n\n\n\n\n\n\n\n\n\ndf1\ndf2\nM\nSD\n\n\n\n\n1\n10\n1.25\n2.17\n\n\n15\n30\n1.07\n0.50\n\n\n30\n60\n1.03\n0.33\n\n\n5\n25\n1.09\n0.79\n\n\n\n\n\n\n\n\n\n\nBecause there is no negative side of the distribution, when we use the F-distribution to compute a p-value, we only compute the cumulative density GREATER THAN OR EQUAL TO the value of the F-statistic. For example, the figure below shows the \\(F(5,25)\\)-distribution and the shaded area corresponds to the p-value for an observed F-statistic of 2.\n\n\nCode\n# Create data for F(5,25)\nfig_08 = data.frame(\n  X = seq(from = 0, to = 10, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = df(x = X, df1 = 5, df2 = 25)\n    )\n\n# Filter data for shading\nshade_01 = fig_08 %&gt;%\n  filter(X &gt;= 2)\n\n# Create plot\nggplot(data = fig_08, aes(x = X, y = Y)) +\n  geom_ribbon(data = shade_01, ymin = 0, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"F\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\n\n\n\nFigure 5.8: Plot of the probability density function (PDF) for the \\(F(5,25)\\)-distribution. The cumulative density representing the p-value associated with an F-statistic of 2 is shaded in grey.\n\n\n\n\n\nHere we can use the pf() function to compute the p-value. Remember, pf() computes the proportion of the area under the curve TO THE LEFT of a particular value. Here we will need to compute the area to the RIGHT of \\(+2\\).\n\n# Compute the p-value based on F(5,25)=2\n1 - pf(q = 2, df1 = 5, df2 = 25)\n\n[1] 0.1134803\n\n\nThe probability of observing an F-statistic at least as extreme as 2, assuming the null hypothesis is true, is 0.113. This is not evidence against the null hypothesis since the data are consistent with the assumed hypothesis.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#creating-a-pdf-and-adding-shading-in-a-ggplot",
    "href": "02-01-mathematical-foundations-probability-density.html#creating-a-pdf-and-adding-shading-in-a-ggplot",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.5 Creating a PDF and Adding Shading in a ggplot",
    "text": "5.5 Creating a PDF and Adding Shading in a ggplot\nOne method to create the PDF for a distribution using ggplot() is to create a dataset that includes a sequence of X-values for which you want to show the PDF and compute the probability density for each of those values. Then you can use geom_line() to connect those probability densities.\nFor example, say we want to create the PDF of the \\(F(15, 100)\\)-distribution. Here I will define this for F-values from 0 to 10. (These are the x-values in my plot.) Then I need to compute the probability densities for each of those values using pf().\n\n# Create F-value and compute probability densities\nfig_09 = data.frame(\n  X = seq(from = 0, to = 10, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = df(x = X, df1 = 5, df2 = 25)\n    )\n\n# View data\nhead(fig_09)\n\n     X           Y\n1 0.00 0.000000000\n2 0.01 0.008319698\n3 0.02 0.022838243\n4 0.03 0.040722606\n5 0.04 0.060856280\n6 0.05 0.082557740\n\n\nThen we can plot the Y versus the X values and connect them using geom_line().\n\nggplot(data = fig_09, aes(x = X, y = Y)) +\n  geom_line() +\n  xlab(\"F\") +\n  ylab(\"Probability density\") +\n  theme_light() \n\n\n\n\n\n\n\nFigure 5.9: Plot of the probability density function (PDF) for the \\(F(5,25)\\)-distribution.\n\n\n\n\n\nTo add shading under the curve we need to create a new dataset that only includes the X and Y values in the shaded region. For example to shade the area under the PDF where \\(F &gt; 2\\), we need to create a new dataset where the X values are greater than 2. Below I do this using filter() and store the data in an object called shade_01.\n\n# Filter data included in the shaded region\nshade_09 = fig_09 %&gt;%\n  filter(X &gt;= 2)\n\n# View data\nhead(shade_09)\n\n     X         Y\n1 2.00 0.1558647\n2 2.01 0.1537083\n3 2.02 0.1515807\n4 2.03 0.1494816\n5 2.04 0.1474106\n6 2.05 0.1453676\n\n\nWe re-draw the PDF and then use geom_ribbon() to add shading. This layer requires us to define the area we want shaded. Here we want to shade from \\(Y=0\\) to \\(Y=\\) the probability density for each of the X values in the shading data. To carry this out we need to define x=, ymin= and ymax=.\nSince the X values are in a column called X and the probability densities are in a column called Y in the shaded dataa frame, we can call x=X and ymax=Y in the aes() of geom_ribbon(). The ymin= value of 0 is not a column in the data frame, so it is specified OUTSIDE the aes() function. We can then also set characteristics like color of the shading (color=) and transparency level (alpha=). Finally, to ensure that geom_ribbon() is shading only the region we want, we set data=shade_01.\n\n# Create plot\nggplot(data = fig_09, aes(x = X, y = Y))  +\n  geom_ribbon(data = shade_09, ymin = 0, aes(x = X, ymax = Y), \n              color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"F\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\n\n\nFigure 5.10: Plot of the probability density function (PDF) for the \\(F(5,25)\\)-distribution. The cumulative density representing the p-value associated with an F-statistic of 2 is shaded in grey.\n\n\n\n\n\n\nYou can click the Code button to see the underlying syntax for many of the figures created in this document.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#probability-distributions-in-regression",
    "href": "02-01-mathematical-foundations-probability-density.html#probability-distributions-in-regression",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.6 Probability Distributions in Regression",
    "text": "5.6 Probability Distributions in Regression\nTo illustrate how probability distributions are used in practice, we will will use the data in the file riverview.csv (see the data codebook for more information about these data) and fit a regression model that uses education level and seniority to predict variation in employee income. Some (most?) of this content should also be review from EPsy 8251.\n\n# Load libraries\nlibrary(broom)\nlibrary(tidyverse)\n\n# Import data\ncity = read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/riverview.csv\")\n\nRows: 32 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): gender, party\ndbl (3): education, income, seniority\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View data\ncity\n\n# A tibble: 32 × 5\n   education income seniority gender     party      \n       &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n 1         8   26.4         9 female     Independent\n 2         8   37.4         7 Not female Democrat   \n 3        10   34.2        16 female     Independent\n 4        10   25.5         1 female     Republican \n 5        10   47.0        14 Not female Democrat   \n 6        12   46.5        11 female     Democrat   \n 7        12   52.5        16 female     Independent\n 8        12   37.7        14 Not female Democrat   \n 9        12   50.3        24 Not female Democrat   \n10        14   32.6         5 female     Independent\n# ℹ 22 more rows\n\n\nTo begin, we will fit a multiple regression model that uses level of education and seniority to predict variation in employee’s incomes. The model is:\n\\[\n\\begin{split}\n\\mathrm{Income}_i &= \\beta_0 + \\beta_1(\\mathrm{Education}_i) + \\beta_2(\\mathrm{Seniority}_i) + \\epsilon_i \\\\\n& \\mathrm{where}\\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_{\\epsilon})\n\\end{split}\n\\]\nWe have four unknowns in this model that need to be estimated: \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\sigma^2_{\\epsilon}\\). This last unknown is the error (or residual) variance. As a side note, when you report results from a fitted regression model, you should report the estimated residual variance (or residual standard error) along with the coefficient estimates.\nAside from the estimates for the coefficients and RSE, we are also generally interested in the estimates of uncertainty for the coefficients (i.e., the standard errors). These uncertainty estimates also allow us to carry out hypothesis tests on the effects included in the model.\n\n# Fit regression model\nlm.1 = lm(income ~ 1 + education + seniority, data = city)\n\nIn practice, all of the estimates, SEs, and inferential output are available using functionality in R. For example, the model-level output, including \\(R^2\\), the F-statistic, the model and residual df, and the residual standard error are all outputted from the glance() function from the {broom} package. We can also partition the variation using the anova() function.\n\n# Model-level output\nglance(lm.1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic       p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.742         0.724  7.65      41.7 0.00000000298     2  -109.  226.  232.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# Partition the variation\nanova(lm.1)\n\nAnalysis of Variance Table\n\nResponse: income\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \neducation  1 4147.3  4147.3  70.944 2.781e-09 ***\nseniority  1  722.9   722.9  12.366   0.00146 ** \nResiduals 29 1695.3    58.5                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSimilarly the tidy() function from the {broom} package outputs coefficient-level output, including the coefficients, standard errors, t-values, and associated p-values.\n\n# Coefficient-level output\ntidy(lm.1, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic     p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    6.77      5.37       1.26 0.218         -4.22      17.8 \n2 education      2.25      0.335      6.73 0.000000220    1.57       2.94\n3 seniority      0.739     0.210      3.52 0.00146        0.309      1.17\n\n\nOur goal here is to understand how the probability distributions play a role in determining some of these values.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#model-level-inference-the-f-distribution",
    "href": "02-01-mathematical-foundations-probability-density.html#model-level-inference-the-f-distribution",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.7 Model-Level Inference: The F-Distribution",
    "text": "5.7 Model-Level Inference: The F-Distribution\nAt the model-level, we are interested in whether or not the model (as a whole) explains variation in the outcome. Our estimate of how much variation the model explains is based on partitioning the total variation of the outcome (\\(\\mathrm{SS}_{\\mathrm{Total}}\\)) into that which is explained by the model (\\(\\mathrm{SS}_{\\mathrm{Model}}\\)) and that which is not explained by the model (\\(\\mathrm{SS}_{\\mathrm{Residual}}\\)). From the anova() output:\n\\[\n\\begin{split}\n\\mathrm{SS}_{\\mathrm{Model}} &= 4147.3 + 722.9 = 4870.2\\\\[1ex]\n\\mathrm{SS}_{\\mathrm{Residual}} &= 1695.3\\\\[1ex]\n\\mathrm{SS}_{\\mathrm{Total}} &= 4870.2 + 1695.3 = 6565.5\n\\end{split}\n\\]\nThen we compute the a statistic called \\(R^2\\) by computing the ratio of the explained variation to the total variation.\n\\[\nR^2 = \\frac{4870.2}{6565.5} = 0.742\n\\]\nThe model (differences in education and seniority levels) explains 74.2% of the variation in employee’s incomes in the sample. We might also want to test whether this is more variation than we expect because of sampling error. To do this we want to test the hypothesis that:\n\\[\nH_0: \\rho^2 = 0\n\\]\nTo evaluate this we convert the sample \\(R^2\\) value into a test statistic using,\n\\[\nF = \\frac{R^2}{1 - R^2} \\times \\frac{\\mathit{df}_{\\mathrm{Error}}}{\\mathit{df}_{\\mathrm{Model}}}\n\\]\nThe degrees of freedom (df) is also partitioned in the anova() output:\n\\[\n\\begin{split}\n\\mathrm{df}_{\\mathrm{Model}} &= 1 + 1 = 2\\\\[1ex]\n\\mathrm{df}_{\\mathrm{Residual}} &= 29\\\\[1ex]\n\\mathrm{df}_{\\mathrm{Total}} &= 2 + 29 = 31\n\\end{split}\n\\]\nConverting our \\(R^2\\) value of 0.742 value to an F-statistic:\n\\[\n\\begin{split}\nF &= \\frac{0.742}{1-0.742} \\times \\frac{29}{2} \\\\\n&= 41.7\n\\end{split}\n\\]\nWe write this standardization of \\(R^2\\) as \\(F(2,29)=41.7\\).\n\n\n5.7.1 Computing F from the ANOVA Partitioning\nWe can also compute the model-level F-statistic directly using the partitioning of variation from the ANOVA table.\n\n# Partition the variation\nanova(lm.1)\n\nAnalysis of Variance Table\n\nResponse: income\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \neducation  1 4147.3  4147.3  70.944 2.781e-09 ***\nseniority  1  722.9   722.9  12.366   0.00146 ** \nResiduals 29 1695.3    58.5                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe F-statistic is a ratio of the mean square for the model and the mean square for the error. To compute a mean square we use the general formula:\n\\[\n\\mathrm{MS} = \\frac{\\mathrm{SS}}{\\mathrm{df}}\n\\]\nThe model includes both the education and seniority predictor, so we combine the SS and df. The MS model is:\n\\[\n\\begin{split}\n\\mathrm{MS}_{\\mathrm{Model}} &= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{4147.3 + 722.9}{1 + 1} \\\\[1ex]\n&= \\frac{4870.2}{2} \\\\[1ex]\n&= 2435.1\n\\end{split}\n\\]\nThe MS error is:\n\\[\n\\begin{split}\n\\mathrm{MS}_{\\mathrm{Error}} &= \\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\frac{1695.3 }{29} \\\\[1ex]\n&= 58.5\n\\end{split}\n\\]\nThen, we compute the F-statistic by computing the ratio of these two mean squares.\n\\[\n\\begin{split}\nF &= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\frac{2435.1}{58.5} \\\\[1ex]\n&= 41.6\n\\end{split}\n\\]\nSince a mean square represents the average amount of variation (per degree of freedom), we can see that F is a ratio between the average amount of variation explained by the model and the average amount of variation unexplained by the model. In our example, this ratio is 41.6; on average the model explains 41.6 times the variation that is unexplained.\nNote that this is an identical computation (although reframed) as the initial computation for F. We can use mathematics to show this equivalence:\n\\[\n\\begin{split}\nF &= \\frac{R^2}{1-R^2} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Total}}}}{\\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Total}}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\mathrm{MS}_{\\mathrm{Model}} \\times \\frac{1}{\\mathrm{MS}_{\\mathrm{Error}}}\\\\[1ex]\n&= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}}\n\\end{split}\n\\]\n\n\n\n5.7.2 Testing the Model-Level Null Hypothesis\nWe evaluate our test statistic (F in this case) in the appropriate test distribution, in this case an F-distribution with 2 and 29 degrees of freedom. The figure below, shows the \\(F(2,29)\\)-distribution as a solid, black line. The p-value is the area under the curve that is at least as extreme as the observed F-value of 41.7.\n\n\nCode\n# Create data\nfig_11 = data.frame(\n  X = seq(from = 0, to = 50, by = 0.01)\n  ) %&gt;%\n  mutate(\n    Y = df(x = X, df1 = 2, df2 = 29)\n    )\n\n# Filter shaded area\nshade = fig_11 %&gt;%\n  filter(X &gt;= 41.7)\n\n# Create plot\nggplot(data = fig_11, aes(x = X, y = Y)) +\n  geom_line() +\n  theme_bw() +\n  geom_ribbon(data = shade, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4)\n\n\n\n\n\n\n\n\nFigure 5.11: Plot of the probability density function (PDF) for the \\(F(2,~29)\\)-distribution. The cumulative density representing the p-value for a test evaluating whether \\(\\rho^2=0\\) using an observed F-statistic of 41.7 is also displayed.\n\n\n\n\n\nThe computation using the cumulative density function, pf(), to obtain the p-value is:\n\n# p-value for F(2,29)=41.7\n1 - pf(41.7, df1 = 2, df2 = 29)\n\n[1] 2.942114e-09\n\n\nBecause we want the upper-tail, rather than taking the difference from 1, we can also use the lower.tail=FALSE argument in pf().\n\n# p-value for F(2,29)=41.7\npf(41.7, df1 = 2, df2 = 29, lower.tail = FALSE)\n\n[1] 2.942114e-09\n\n\n\n\n\n5.7.3 Mean Squares are Variance Estimates\nMean squares are also estimates of the variance. Consider the computational formula for the sample variance,\n\\[\n\\hat{\\sigma}^2 = \\frac{\\sum(Y - \\bar{Y})^2}{n-1}\n\\]\nThis is the total sum of squares divided by the total df. The variance of the outcome variable is interpreted as the average amount of variation in the outcome variable (in the squared metric). Thus, it is also referred to as the mean square total.\nWhen we compute an F-statistic, we are finding the ratio of two different variance estimates—one based on the model (explained variance) and one based on the error (unexplained variance). Under the null hypothesis that \\(\\rho^2 = 0\\), we are assuming that all the variance is unexplained. In that case, our F-statistic would be close to zero. When the model explains a significant amount of variation, the numerator gets larger relative to the denominator and the F-value is larger.\nThe mean squared error (from the anova() output) plays a special role in regression analysis. It is the variance estimate for the conditional distributions of the residuals in our visual depiction of the distributional assumptions of the residuals underlying linear regression.\n\n\n\n\n\n\n\n\nFigure 5.12: Visual Depiction of the Distributional Assumptions of the Residuals Underlying Linear Regression\n\n\n\n\n\nRecall that we made implicit assumptions about the conditional distributions of the residuals, namely that they were identically and normally distributed with a mean of zero and some variance. Based on the estimate of the mean squared error, the variance of each of these distributions is 58.5.\nWhile the variance is a mathematical convenience, the standard deviation is often a better descriptor of the variation in a distribution since it is measured in the original metric. The standard deviation fro the residuals (error) is 7.6. Because the residuals are statistics (summaries computed from sample data), their standard deviation is referred to as a “standard error”.\n\nThe residual standard error (RSE) is sometimes referred to as the Root Mean Squared Error (RMSE).\n\n\n# Compute RMSE\nsqrt(58.5)\n\n[1] 7.648529\n\n\nWhy is this value important? It gives the expected variation in the conditional residual distributions, which is a measure of the average amount of error. For example, since all of the conditional distributions of the residuals are assumed to be normally distributed, we would expect that 95% of the residuals would fall between \\(\\pm2\\) standard errors from 0; or, in this case, between \\(-15.3\\) and \\(+15.3\\). Observations with residuals that are more extreme may be regression outliers.\nMore importantly, it is a value that we need to estimate in order to specify the model.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#coefficent-level-inference-the-t-distribution",
    "href": "02-01-mathematical-foundations-probability-density.html#coefficent-level-inference-the-t-distribution",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.8 Coefficent-Level Inference: The t-Distribution",
    "text": "5.8 Coefficent-Level Inference: The t-Distribution\nRecall that the coefficients and SEs for the coefficients are computed directly from the raw data based on the OLS estimation. These can then be uses to construct a test statistic (e.g., t) to carry out a hypothesis test or compute endpoints for a confidence interval. To see how this is done, we will consider the partial effect of education level (after controlling for differences in seniority) in our fitted model.\n\\[\n\\begin{split}\n\\hat\\beta_{\\mathrm{Education}}&=2.25 \\\\[1ex]\n\\mathrm{SE}(\\hat\\beta_{\\mathrm{Education}}) &=0.335\n\\end{split}\n\\]\nWe might want to test whether the partial effect of education level on income, after accounting for differences in seniority level, we observed in the data is more than we would expect because of sampling error. To answer this we need to evaluate the following hypothesis:\n\\[\nH_0: \\beta_{\\mathrm{Education}} = 0\n\\]\nWe begin by converting our estimated regression coefficient to a t-statistic using:\n\\[\nt_k = \\frac{\\hat\\beta_k}{\\mathrm{SE}(\\hat\\beta_k)}\n\\]\nIn our example,\n\\[\n\\begin{split}\nt_{\\mathrm{Education}} &= \\frac{2.25}{0.335} \\\\[1ex]\n&= 6.72\n\\end{split}\n\\]\nRemember this tells us the education coefficient of 2252 is 6.72 standard errors above 0. Since we are estimating the SE using sample data, our test statistic is likely t-distributed2. Which value should we use for df? Well, for that, statistical theory tells us that we should use the error df value from the model. In our example, this would be:\n\\[\nt(29) = 6.72\n\\]\nUsing the t-distribution with 29 df, we can compute the p-value associated with the two-tailed test that \\(\\beta_{\\mathrm{Education}=0}\\):\n\n# p-value for the two-tailed test of no effect of education\n2 * pt(q = -6.72, df = 29)\n\n[1] 2.257125e-07\n\n# alternatively\n2 * pt(q = 6.72, df = 29, lower.tail = FALSE)\n\n[1] 2.257125e-07\n\n\nThe p-value is 0.0000002. The data are inconsistent with the hypothesis that there is no partial effect of education level on income (after accounting for differences in seniority level).\nWe could also carry out these tests for the partial effect of seniority level and, if it is of interest, for the intercept. For both of those tests, we would use the same t-distribution, but our test statistic would be computed based on the coefficient estimates and standard errors for those terms, respectively.\n\n\n5.8.1 Confidence/Compatibility Intervals for the Coefficients\nThe confidence interval for the kth regression coefficient is computed as:\n\\[\n\\mathrm{CI} = \\hat\\beta_k \\pm t^{*}(\\mathrm{SE}_{\\hat\\beta_k})\n\\]\nwhere \\(t^*\\) is the quantile of the t-distribution that defines the confidence level for the interval. (This t-distribution, again, has degrees-of-freedom equal to the error df in the model.) The confidence level is related to the alpha level (type I error rate) used in inference. Namely,\n\\[\n\\mathrm{Confidence~Level} = 1 - \\alpha\n\\]\nSo, if you use \\(\\alpha=.05\\), then the confidence level would be \\(.95\\), and we would call this a 95% confidence interval. The alpha value also helps determine the quantile we use in the CI formula,\n\\[\nt^* = (1-\\frac{\\alpha}{2}) ~ \\mathrm{quantile}\n\\] For the example using \\(\\alpha=.05\\), a 95% confidence interval, the \\(t^*\\) value would be associated with the quantile of 0.975. We would denote this as:\n\\[\nt^{*}_{.975}\n\\]\nSay we wanted to find the 95% confidence interval for the education coefficient. We know that the estimated coefficient for education is 2.25, and the standard error for this estimate is 0.335. We also know that based on the model fitted, the residual df is 29. We need to find the 0.975th quantile in the t-distribution with 29 df.\n\n# Find 0.975th quantile\nqt(p = 0.975, df = 29)\n\n[1] 2.04523\n\n\nNow we can use all of this information to compute the confidence interval:\n\\[\n\\begin{split}\n95\\%~CI  &= 2.25 \\pm 2.04523(0.335) \\\\[1ex]\n&= \\big[1.56,~2.94\\big]\n\\end{split}\n\\]",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#learn-more",
    "href": "02-01-mathematical-foundations-probability-density.html#learn-more",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.9 Learn More",
    "text": "5.9 Learn More\nFor more information, you can also see the section Some Continuous Distributions in CHapter 5 of the Fox (2021) textbook.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#references",
    "href": "02-01-mathematical-foundations-probability-density.html#references",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.10 References",
    "text": "5.10 References\n\n\n\n\n\n\nFox, J. (2021). A mathematical primer for social statistics (2nd ed.). Sage.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#footnotes",
    "href": "02-01-mathematical-foundations-probability-density.html#footnotes",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "",
    "text": "Remember, the mean and standard deviations in the population are called “parameters”.↩︎\nWhether this is actually t-distributed depends on whether the model assumptions are met.↩︎",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html",
    "href": "02-03-likelihood-framework-for-evidence.html",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "",
    "text": "6.1 Preparation\nIn this set of notes, you will learn about the law of likelihood, and the use of likelihood ratios as statistical evidence for model selection. To do so, we will use the pew.csv dataset (see the data codebook) to fit a set of models that explain variation in American’s political knowledge.\n# Load libraries\nlibrary(broom)\nlibrary(educate)\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(tidyr)\n\n# Import data\npew = read_csv(file = \"https://raw.githubusercontent.com/zief0002/fluffy-ants/main/data/pew.csv\")\n\n# View data\npew\n\n# A tibble: 100 × 9\n      id knowledge  news   age education  male ideology party       engagement\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1     1        10    60  31.1        16     0     65.1 Democrat          19.8\n 2     2        18    20  44.4        12     1     41   Independent       71.4\n 3     3        88    59  53.7        16     1      3.4 Democrat          97.2\n 4     4        75    89  62.5        13     1     50.6 Independent       61.5\n 5     5        93    43  48.3        16     0     35.3 Democrat          53.4\n 6     6        13    61  24.3         9     0      0.1 Independent       41.1\n 7     7        37    45  59.2        12     0     13.7 Independent       96.3\n 8     8        66    59  93.9        16     1     27.1 Democrat          86.8\n 9     9        49    68  37          16     0     67.4 Independent       58.2\n10    10        50    44  37.1        14     1     54.8 Democrat          45.2\n# ℹ 90 more rows",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#research-questions-and-modeling-strategy",
    "href": "02-03-likelihood-framework-for-evidence.html#research-questions-and-modeling-strategy",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.2 Research Questions and Modeling Strategy",
    "text": "6.2 Research Questions and Modeling Strategy\nRecall that we had three research questions for these data:\n\nIs there an effect of news exposure on political knowledge?\nIs there an effect of news exposure on political knowledge after controlling for demographic and political covariates?\nDoes education level moderate the effect of news exposure on political knowledge after controlling for demographic and political covariates?\n\nAny analysis should begin with looking at plots and computing summary statistics of the sample data. (We already did this in a previous set of notes.) After the data exploration, we can begin to think about fitting one or more models to the data. It is good science to consider the modeling strategy you will be using before you begin fitting models. There are many modeling strategies that educational scientists use in practice (e.g., forward-selection, backward-elimination) and there is no one “right” method. As you consider a modeling strategy, think about how this strategy helps provide a narrative structure for answering your research question; sometimes this leads to one strategy being more productive than others.\nIn the previous set of notes we began by fitting a model that only included the main-effect of news exposure. Evaluating the effect of news exposure in this model helped us answer RQ 1. We then fitted a model that included news exposure along with the set of demographic and political covariates. Evaluating the effect of news exposure in this model helped us answer RQ 2. Finally, we fitted a model that included an interaction effect of news exposure and education in along with the demographic and political covariates. Evaluating the interaction effect allowed us to answer RQ 3. These three models are:\n\\[\n\\begin{split}\n\\mathbf{Model~1:~} \\quad \\mathrm{News~Knowledge}_i = &\\beta_0 + \\beta_1(\\mathrm{News~Exposure}_i) + \\epsilon_i \\\\[2ex]\n\\mathbf{Model~2:~} \\quad \\mathrm{News~Knowledge}_i = &\\beta_0 + \\beta_1(\\mathrm{Age}_i) + \\beta_2(\\mathrm{Education}_i) + \\beta_3(\\mathrm{Male}_i) + \\\\\n&\\beta_4(\\mathrm{Engagement}_i) + \\beta_5(\\mathrm{Ideology}_i)+ \\\\\n&\\beta_6(\\mathrm{Democrat}_i) + \\beta_7(\\mathrm{Republican}_i) + \\\\\n&\\beta_8(\\mathrm{News~Exposure}_i) + \\epsilon_i \\\\[2ex]\n\\mathbf{Model~3:~} \\quad \\mathrm{News~Knowledge}_i = &\\beta_0 + \\beta_1(\\mathrm{Age}_i) + \\beta_2(\\mathrm{Education}_i) + \\beta_3(\\mathrm{Male}_i) + \\\\\n&\\beta_4(\\mathrm{Engagement}_i) + \\beta_5(\\mathrm{Ideology}_i)+ \\\\\n&\\beta_6(\\mathrm{Democrat}_i) + \\beta_7(\\mathrm{Republican}_i) + \\\\\n&\\beta_8(\\mathrm{News~Exposure}_i) + \\beta_9(\\mathrm{News~Exposure}_i)(\\mathrm{Education}_i) + \\epsilon_i \\\\[2ex]\n\\end{split}\n\\]\nwhere \\(\\epsilon_i \\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\) for each of the models.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#classical-framework-of-evidence",
    "href": "02-03-likelihood-framework-for-evidence.html#classical-framework-of-evidence",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.3 Classical Framework of Evidence",
    "text": "6.3 Classical Framework of Evidence\nWhen we have looked at statistical evidence to this point, it has been from a hypothesis testing point of view. The primary piece of evidence we use in this paradigm is the p-value. For example, if we fit Model 1 and examine the evidence for the effect of news exposure on news knoledge, we find:\n\n# Fit Model 1\nlm.1 = lm(knowledge ~ 1 + news, data = pew)\n\n# Coefficient-level output\ntidy(lm.1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   39.7      5.20        7.63 1.56e-11\n2 news           0.327    0.0899      3.64 4.42e- 4\n\n\nThe p-value associated with the effect of news exposure (\\(p&lt;.001\\)), which suggests that the size of this effect is more than we would expect because of chance if the population effect was 0.\nInterpreting this p-values, we would say that the probability of seeing the empirical evidence we observed (or evidence that is more extreme) if the null hypothesis that there is no effect of news exposure on news knowledge is true, is 0.000442. This implies that our observed data are inconsistent with the hypothesized model that there is no effect of news exposure. In an applied setting, we might use such evidence to decide that the level of news exposure does indeed predict variation in American’s news knowledge.\nDespite being the predominant evidential paradigm used in the education and social sciences, hypothesis testing has many criticisms (e.g., Johansson, 2011; Weakliem, 2016). Among some of the stronger criticisms,\n\nThe p-value only measures evidence against the hypothesized model; not the evidence FOR a particular model.\nThe model we specify in the null hypothesis is often substantively untenable (how often is the effect 0? Generally as applied scientists the reason we include predictors is because we believe there is an effect.)\nThe p-value is based on data we haven’t observed (it is based on the observed data AND evidence that is more extreme).\n\nIf we write the p-value as a probability statement, it would be:\n\\[\np\\mbox{-}\\mathrm{value} = P(\\mathrm{Data~or~more~extreme~unobserved~data} \\mid \\mathrm{Model})\n\\]\nWhile hypothesis tests have filled a need in the educational and social science to have some standard for evaluating statistical evidence, it is unclear whether this is the approach we should be using. As statistician David Lindley so aptly states, “[significance tests] are widely used, yet are logically indefensible” (comment in Johnstone, 1986, p. 502). Psychologist Jacob Cohen was more pointed, saying “[hypothesis testing] has not only failed to support the advance of psychology as a science but also has seriously impeded it” (Cohen, 1994, p. 997).\n\n“The main purpose of a significance test is to inhibit the natural enthusiasm of the investigator” (Mosteller & Bush, 1954, pp. 331–332).",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#likelihood-paradigm-to-statistical-evidence",
    "href": "02-03-likelihood-framework-for-evidence.html#likelihood-paradigm-to-statistical-evidence",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.4 Likelihood Paradigm to Statistical Evidence",
    "text": "6.4 Likelihood Paradigm to Statistical Evidence\nIn applied science, we ideally would like to collect some evidence (data) and use that to say something about how likely a particular model (or hypothesis) is based on that evidence. Symbolically we want to know,\n\\[\nP(\\mathrm{Model} \\mid \\mathrm{Observed~data})\n\\]\nThis probability is known as the likelihood and is very different than the probability given by the p-value. In the likelihood paradigm, the likelihood is the key piece of statistical evidence used to evaluate models. For example if you were comparing Model A and Model B, you could compute the likelihood for each model and compare them. Whichever model has the higher likelihood has more empirical support. This is, in a nutshell what the Law of Likelihood states. What is even more attractive is that another axiom, the Likelihood Principle, tells us that if the goal is to compare the empirical support of competing models, all of the information in the data that can be used to do so, is contained in the ratio of the model likelihoods. That is, we can’t learn more about which model is more supported unless we collect additional data.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#joint-probability-density-a-roadstop-to-computing-likelihood",
    "href": "02-03-likelihood-framework-for-evidence.html#joint-probability-density-a-roadstop-to-computing-likelihood",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.5 Joint Probability Density: A Roadstop to Computing Likelihood",
    "text": "6.5 Joint Probability Density: A Roadstop to Computing Likelihood\nIn the preparation reading, you learned about the probability density of an observation \\(x_i\\). Now we will extend this idea to the probability density of a set of observations, say \\(x_1\\), \\(x_2\\), AND \\(x_k\\). The probability density of a set of observations is referred to as the joint probability density, or simply joint density.\nIf we can make an assumption about INDEPENDENCE, then the joint probability density would be the product of the individual densities:\n\\[\np(x_1, x_2, x_3, \\ldots, x_k) = p(x_1) \\times p(x_2) \\times p(x_3) \\times \\ldots \\times p(x_k)\n\\]\nSay we had three independent observations, \\(x =\\{60, 65, 67\\}\\), from a \\(\\sim\\mathcal{N}(50,10)\\) distribution. The joint density would be:\n\n# Compute joint density\ndnorm(x = 60, mean = 50, sd = 10) * dnorm(x = 65, mean = 50, sd = 10) * dnorm(x = 67, mean = 50, sd = 10)\n\n[1] 0.000002947448\n\n\nWe could also shortcut this computation,\n\n# Compute joint density\nprod(dnorm(x = c(60, 65, 67), mean = 50, sd = 10))\n\n[1] 0.000002947448\n\n\nThis value is the joint probability density. The joint probability density indicates the probability of observing the data (\\(x =\\{60, 65, 67\\}\\)) GIVEN (1) they are drawn from a normal distribution and (2) the normal distribution has a mean of 50 and a standard deviation of 10. In other words, the joint probability density is the probability of the data given a model and parameters of the model.\nSymbolically,\n\\[\n\\mathrm{Joint~Density} = P(\\mathrm{Data} \\mid \\mathrm{Model~and~Parameters})\n\\]",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#computing-likelihood",
    "href": "02-03-likelihood-framework-for-evidence.html#computing-likelihood",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.6 Computing Likelihood",
    "text": "6.6 Computing Likelihood\nLikelihood is the probability of a particular set of parameters GIVEN (1) the data, and (2) the data are generated from a particular model (e.g., normal distribution). Symbolically,\n\\[\n\\mathrm{Likelihood} = P(\\mathrm{Parameters} \\mid \\mathrm{Model~and~Data})\n\\]\nSymbolically we denote likelihood with a scripted letter “L” (\\(\\mathcal{L}\\)). For example, we might ask the question, given the observed data \\(x = \\{30, 20, 24, 27\\}\\) come from a normal distribution, what is the likelihood (probability) that the mean is 20 and the standard deviation is 4? We might denote this as,\n\\[\n\\mathcal{L}(\\mu = 20, \\sigma = 4 \\mid x)\n\\]\n\nFYI\nAlthough we need to specify the model this is typically not included in the symbolic notation; instead it is often a part of the assumptions.\n\n\n\n6.6.1 An Example of Computing and Evaluating Likelihood\nThe likelihood allows us to answer probability questions about a set of parameters. For example, what is the likelihood (probability) that the data (\\(x = \\{30, 20, 24, 27\\}\\)) were generated from a normal distribution with a mean of 20 and standard deviation of 4? To compute the likelihood we compute the joint probability density of the data under that particular set of parameters.\n\nprod(dnorm(x = c(30, 20, 24, 27), mean = 20, sd = 4))\n\n[1] 0.0000005702554\n\n\nWhat is the likelihood (probability) that the same set of data (\\(x = \\{30, 20, 24, 27\\}\\)) were generated from a normal distribution with a mean of 25 and standard deviation of 4?\n\nprod(dnorm(x = c(30, 20, 24, 27), mean = 25, sd = 4))\n\n[1] 0.00001774012\n\n\nGiven the data and the model, there is more empirical support that the parameters are \\(\\mathcal{N}(25,4^2)\\) rather than \\(\\mathcal{N}(20, 4^2)\\), because the likelihood is higher for the former set of parameters. We can compute a ratio of the two likelihoods to quantify the amount of additional support for the \\(\\mathcal{N}(25,4^2)\\).\n\\[\n\\begin{split}\n\\mathrm{Likelihood~Ratio} &= \\frac{0.00001774012}{0.0000005702554} \\\\[1ex]\n&= 31.11\n\\end{split}\n\\]\nThe empirical support for the \\(\\mathcal{N}(25,4^2)\\) parameterization is 31 times that of the \\(\\mathcal{N}(20, 4^2)\\) parameterization! In a practical setting, this would lead us to adopt a mean of 25 over a mean of 20.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#some-notes-and-caveats",
    "href": "02-03-likelihood-framework-for-evidence.html#some-notes-and-caveats",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.7 Some Notes and Caveats",
    "text": "6.7 Some Notes and Caveats\nIt is important to note that although we use the joint probability under a set of parameters to compute the likelihood of those parameters, theoretically joint density and likelihood are very different. Likelihood takes the data and model as given and computes the probability of a set of parameters. Whereas joint density assumes that the model and parameters are given and gives us the probability of the data.\n\nFYI\nLikelihood refers to the probability of the parameters and joint probability density refers to the probability of the data.\n\nOnce we collect the data, the probability of observing that set of data is 1; it is no longer unknown. The likelihood method treats our data as known and offers us a way of making probabilistic statements about the unknown parameters. This is more aligned with our scientific process than making some assumption about the parameter (e.g., \\(\\beta_1=0\\)) and then trying to determine the probability of the data under that assumption. Moreover, likelihood does not use unobserved data (e.g., data more extreme than what we observed) in the computation.\nIt is also important to acknowledge what likelihood and the likelihood ratio don’t tell us. First, they only tell us the probability of a set of parameters for the data we have. Future collections of data might change the amount of support or which set of parameters is supported. Since changing the data, changes the likelihood, this also means we cannot make cross study comparisons of the likelihood (unless the studies used the exact same data). Secondly, the model assumed is important. If a different model is assumed, the likelihood will be different, and again could change the amount of support or which set of parameters is supported.\nThe likelihood ratio (LR), while useful for comparing the relative support between parameterizations, does not tell you that a particular parameterization is correct. For example, the LR of 31.11 tells us that there is more empirical support for the \\(\\mathcal{N}(25,4^2)\\) parameterization than \\(\\mathcal{N}(20, 4^2)\\). But, there might be even more support for a parameterization we haven’t considered.\nThese shortcomings are not unique to the likelihood paradigm The also exist in the classical hypothesis testing paradigm for statistical evidence. All in all, the added advantages to the likelihood paradigm make it more useful to applied work than hypothesis testing.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#likelihood-in-regression-back-to-our-example",
    "href": "02-03-likelihood-framework-for-evidence.html#likelihood-in-regression-back-to-our-example",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.8 Likelihood in Regression: Back to Our Example",
    "text": "6.8 Likelihood in Regression: Back to Our Example\nWhen fitting a regression model, we make certain assumptions about the relationship between a set of predictors and the outcome. For example, in Model 1 from our earlier example, we assume that the relationship between news exposure and news knowledge can be described by the following model:\n\\[\n\\begin{split}\n\\mathrm{News~Knowledge}_i = &\\beta_0 + \\beta_1(\\mathrm{News~Exposure}_i) + \\epsilon_i \\\\[1ex]\n&\\mathrm{where~}\\epsilon_i \\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\sigma^2_{\\epsilon})\n\\end{split}\n\\]\nHere we use OLS to estimate the regression coefficients. Then we can use those, along with the observed data to obtain the residuals and the estimate for the residual standard error. The residuals are the GIVEN data and the set up distributional assumptions for the model (e.g., normal, mean of 0, constant variance) allow us to compute the likelihood for the entire set of parameters in this model (\\(\\beta_0\\), \\(\\beta_1\\), \\(\\sigma^2_{\\epsilon}\\)).\nBelow is a set of syntax to compute the likelihood, based on fitting lm.1. We use the resid() function to compute the residuals. (It is the same as grabbing the column called .resid from the augment() output.) We also use the estimated value of the residual standard error (\\(\\hat{\\sigma}_{\\epsilon} = 20.3\\)) from the glance() output.\n\n# Get RSE for use in likelihood\nglance(lm.1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.119         0.110  20.3      13.2 0.000442     1  -442.  890.  897.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# Compute likelihood for lm.1\nprod(dnorm(x = resid(lm.1), mean = 0, sd = 20.3))\n\n[1] 1.382925e-192\n\n\nThis value by itself is somewhat meaningless. It is only worthwhile when we compare it to the likelihood from another model. For example, let’s compute the likelihood for an intercept-only model (Model 0) and compare this to the likelihood for lm.1.\n\n# Fit Model 0\nlm.0 = lm(knowledge ~ 1, data = pew)\n\n# Get RSE for use in likelihood\nglance(lm.0)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         0             0  21.5        NA      NA    NA  -448.  900.  905.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# Compute likelihood for lm.2\nprod(dnorm(x = resid(lm.0), mean = 0, sd = 21.5))\n\n[1] 2.489266e-195\n\n\nThe likelihood value for lm.1 is higher than the likelihood value for lm.0. Computing the likelihood ratio:\n\n1.382925e-192 / 2.489266e-195\n\n[1] 555.5553\n\n\nThis suggests that given the data, Model 1 is 555.6 times more likely than Model 0. In practice, we would adopt Model 1 over Model 0 because it is more likely given the empirical evidence (data) we have.\n\n6.8.1 Mathematics of Likelihood\nBeing able to express the likelihood mathematically is important for quantitative methodologists as it allows us to manipulate and study the likelihood function and its properties. It also gives us insight into how the individual components of the likelihood affect its value.\nRemember, we can express the likelihood of the regression residuals mathematically as:\n\\[\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) = p(\\epsilon_1) \\times p(\\epsilon_2) \\times \\ldots \\times p(\\epsilon_n)\n\\]\nwhere the probability density of each residual (assuming normality) is:\n\\[\np(\\epsilon_i) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{(\\epsilon_i-\\mu)^2}{2\\sigma^2}\\right]\n\\]\nIn addition to normality, which gives us the equation to compute the PDF for each residual, the regression assumptions also specify that each conditional error distribution has a mean of 0 and some variance (that is the same for all conditional error distributions). We can call it \\(\\sigma^2_{\\epsilon}\\). Substituting these values into the density function, we get,\n\\[\n\\begin{split}\np(\\epsilon_i) &= \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{(\\epsilon_i-0)^2}{2\\sigma^2_{\\epsilon}}\\right] \\\\[1em]\n&= \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{(\\epsilon_i)^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\end{split}\n\\]\nNow we use this expression for each of the \\(p(\\epsilon_i)\\) values in the likelihood computation.\n\\[\n\\begin{split}\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) &= p(\\epsilon_1) \\times p(\\epsilon_2) \\times \\ldots \\times p(\\epsilon_n) \\\\[1em]\n&= \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{\\epsilon_1\n^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\\\\n&~~~~~~\\ldots \\times \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\end{split}\n\\]\nWe can simplify this:\n\\[\n\\begin{split}\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) &=\\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\exp\\left[-\\frac{\\epsilon_1^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\ldots \\\\\n&~~~~~~ \\times \\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\end{split}\n\\] We can also simplify this by using the product notation:\n\\[\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) =\\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\prod_{i=1}^n \\exp\\left[-\\frac{\\epsilon_i^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\] We can also write the residuals (\\(\\epsilon_i\\)) as a function of the regression parameters we are trying to find the likelihood for.\n\\[\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) =\\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\prod_{i=1}^n \\exp\\left[-\\frac{\\big[Y_i - \\beta_0 - \\beta_1(X_i)\\big]^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\]\nwhere \\(\\sigma^2_{\\epsilon} = \\frac{\\sum \\epsilon_i^2}{n}\\). Because the numerator of \\(\\sigma^2_{\\epsilon}\\) can be written as \\(\\sum_i^n\\big(Y_i - \\beta_0 - \\beta_1(X_i)\\big)^2\\), we see that the likelihood is a function of \\(n\\), and the regression coefficients, \\(\\beta_0\\) and \\(\\beta_1\\). Moreover, \\(n\\) is based on the data (which is given) and is thus is a constant. Mathematically, this implies that the only variables (values that can vary) in the likelihood function are the regression coefficients.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#log-likelihood",
    "href": "02-03-likelihood-framework-for-evidence.html#log-likelihood",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.9 Log-Likelihood",
    "text": "6.9 Log-Likelihood\nThe likelihood values are quite small since we are multiplying several probability densities (values between 0 and 1) together. Since it is hard to work with these smaller values, in practice, we often compute and work with the natural logarithm of the likelihood. So in our example, Model 0 (\\(\\mathcal{L}_0 = 2.489266 \\times 10^{-195}\\)) has a log-likelihood of:\n\n# Log-likelihood for Model 0\nlog(2.489266e-195)\n\n[1] -448.0921\n\n\nSimilarly, we can compute the log-likelihood for Model 1 as:\n\n# Log-likelihood for Model 1\nlog(1.382925e-192)\n\n[1] -441.7721\n\n\nWe typically denote log-likelihood using a scripted lower-case “l” (\\(\\mathcal{l}\\)). Here,\n\\[\n\\begin{split}\n\\mathcal{l}_0 &= -448.0921 \\\\[1ex]\n\\mathcal{l}_1 &= -441.7721 \\\\[1ex]\n\\end{split}\n\\]\nNote that the logarithm of a decimal will be negative, so the log-likelihood will be a negative value. Less negative log-likelihood values correspond to higher likelihood values, which indicate more empirical support. Here Model 1 has a log-likelihood value (\\(-441.8\\)) that is less negative than Model 0’s log-likelihood value (\\(-448.1\\)), which indicates there is more empirical support for Model 1 than Model 0.\n\nFYI\nWhile it is possible using algebra to express the likelihood ratio using log-likelihoods, it does not have the same interpretational value as the LR does. Because of this, the likelihood ratio is not often expresseds using log-likelihoods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.9.1 Mathematics of Log-Likelihood\nWe can express the log-likelihood of the regression residuals mathematically by taking the natural logarithm of the likelihood we computed earlier:\n\\[\n\\begin{split}\n\\ln \\Bigl(\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data})\\Bigr) &= \\ln \\Biggl( \\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\exp\\left[-\\frac{\\epsilon_1^2}{2\\sigma^2_{\\epsilon}}\\right] \\times  \\\\\n&~~~~~~ \\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\ldots \\times \\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right] \\Biggr) \\\\\n\\end{split}\n\\]\nUsing our rules for logarithms and re-arranging gives,\n\\[\n\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = -\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\epsilon_i^2\n\\]\nExamining this equation, we see that the log-likelihood is a function of \\(n\\), \\(\\sigma^2_{\\epsilon}\\) and the sum of squared residuals (SSR)1. We can of course, re-express this using the the regression parameters:\n\\[\n\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = -\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\big[Y_i - \\beta_0 - \\beta_1(X_i)\\big]^2\n\\]\nAnd, again, since \\(\\sigma^2_{\\epsilon}\\) is a function of the regression coefficients and \\(n\\), this means that the only variables in the log-likelihood function are the coefficients.\n\n\n\n6.9.2 Shortcut: The logLik() Function\nThe logLik() function can be used to obtain the log-likelihood directly from a fitted model object. For example, to find the log-likelihood for Model 1, we can use:\n\n# Compute log-likelihood for Model 1\nlogLik(lm.1)\n\n'log Lik.' -441.7579 (df=3)\n\n\nThe df output tells us how many total parameters are being estimated in the model. In our case the number of total parameters in Model 0 is three (\\(\\beta_0\\), \\(\\beta_{\\mathrm{News~Exposure}}\\), and \\(\\sigma^2_{\\epsilon}\\)). What is more important to us currently, is the log-likelihood value; \\(\\mathcal{l}_1=-450.2233\\).\nThis value is slightly different than the log-likelihood we just computed of \\(-441.7721\\). This is not because of rounding in this case. It has to do with how the model is being estimated; the logLik() function assumes the parameters are being estimated using maximum likelihood (ML) rather than ordinary least squares (OLS). You can learn more about ML estimation in the optional set of notes, but for now, we will just use logLik() to compute the log-likelihood.\nHere we compute the log-likelihood for Model 0 using the logLik() function. We also use the output to compute the likelihood for Model 0. To compute the likelihood from the log-likelihood we need to exponentiate (the reverse function of the logarithm) the the log-likelihood value using the exp() function. We also compute the log-likelihood and likelihood value for Model 1.\n\n# Compute log-likelihood for Model 0\nlogLik(lm.0)\n\n'log Lik.' -448.0884 (df=2)\n\n# Compute likelihood for Model 0\nexp(logLik(lm.0)[1])\n\n[1] 2.498531e-195\n\n# Compute log-likelihood for Model 1\nlogLik(lm.1)\n\n'log Lik.' -441.7579 (df=3)\n\n# Compute likelihood for Model 1\nexp(logLik(lm.1)[1])\n\n[1] 1.402758e-192\n\n\nBecause the output from logLik() includes extraneous information (e.g., df), we use indexing (square brackets) to extract only the part of the output we want. In this case, the [1] extracts the log-likelihood value from the logLik() output (ignoring the df part).",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#model-complexity",
    "href": "02-03-likelihood-framework-for-evidence.html#model-complexity",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.10 Model Complexity",
    "text": "6.10 Model Complexity\nOne aspect that we need to consider is that more complex models tend to have higher likelihoods and log-likelihoods. Therefore when we compare likelihoods (or log-likelihoods) we need to consider the complexity in addition to the likelihoods. One way to quantify a model’s complexity is to consider the number of parameters that are being estimated. The more parameters that we need to estimate, the more complex the model. Recall that the number of parameters for a model are given in the df value from the logLik() function’s output.\nIn our example, the df value for Model 0 is two, indicating that this model is estimating two parameters (\\(\\beta_0\\), and \\(\\sigma^2_{\\epsilon}\\)). For Model 1, the df value was three. This indicates that Model 1 is more complex than Model 0.\nAs we consider using the likelihood ratio (LR) or the difference in log-likelihoods for model selection, we also need to consider the model complexity. In our example, the likelihood ratio of 555.6 indicates that Model 1 has approximately 555.6 times the empirical support than Model 0. But, Model 1 is more complex than Model 0, so we would expect that it would be more empirically supported.\nIn this case, with a likelihood ratio of 555.6, it seems like the empirical data certainly support adopting Model 1 over Model 0. despite the added complexity of Model 1. But what if the LR was 10? Would that be enough additional support to warrant adopting Model 1 over Model 0? What about a LR of 5?",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#likelihood-ratio-test-for-nested-models",
    "href": "02-03-likelihood-framework-for-evidence.html#likelihood-ratio-test-for-nested-models",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.11 Likelihood Ratio Test for Nested Models",
    "text": "6.11 Likelihood Ratio Test for Nested Models\nOne key question that arises is, if the likelihood for a more complex model is higher than the likelihood for a simpler model, how large does the likelihood ratio have to be before we adopt the more complex model? In general, there is no perfect answer for this.\nIf the models being compared are nested, then we can carry out a hypothesis test2 to see if the LR is more than we would expect because of chance. Models are nested when the parameters in the simpler model are a subset of the parameters in the more complex model. For example, in our example, the parameters in Model 0 are a subset of the parameters in Model 1:\n\\[\n\\begin{split}\n\\mathbf{Model~1~Parameters:}&\\quad\\{\\beta_0,~\\beta_{\\mathrm{News~Exposure}},~\\sigma^2_{\\epsilon}\\} \\\\[1ex]\n\\mathbf{Model~0~Parameters:}&\\quad\\{\\beta_0,~\\sigma^2_{\\epsilon}\\} \\\\[1ex]\n\\end{split}\n\\]\nThe parameters for Model 0 all appear in the list of parameters for Model 1. Because of this we can say that Model 0 is nested in Model 1.\n\n\n6.11.1 Hypothesis Test of the LRT\nWhen we have nested models we can carry out a hypothesis test to decide between the following competing hypotheses:\n\\[\n\\begin{split}\nH_0:& ~\\theta_0 = \\{\\beta_0,~\\sigma^2_{\\epsilon}\\}\\\\[1ex]\nH_A:& \\theta_1 = \\{\\beta_0,~\\beta_{\\mathrm{News~Exposure}},~\\sigma^2_{\\epsilon}\\}\n\\end{split}\n\\]\nwhere \\(\\theta_0\\) refers to the simpler model and \\(\\theta_1\\) refers to the more complex model. This translates to adopting either the simpler model (fail to reject \\(H_0\\)) or the more complex model (reject \\(H_0\\)). To carry out this test, we translate our likelihood ratio to a test statistic called \\(\\chi^2\\) (pronounced chi-squared):\n\\[\n\\chi^2 = -2 \\ln \\bigg(\\frac{\\mathcal{L}({\\theta_0})}{\\mathcal{L}({\\theta_1})}\\bigg)\n\\]\nThat is we compute \\(-2\\) times the log of the likelihood ratio where the likelihood for the simpler model is in the numerator. (Note this is the inverse of how we have been computing the likelihood ratio!) Equivalently, we can compute this as:\n\\[\n\\chi^2 = -2 \\bigg(\\ln \\bigg[\\mathcal{L}({\\theta_0})\\bigg] - \\ln \\bigg[\\mathcal{L}({\\theta_1})\\bigg]\\bigg)\n\\]\nFor our example, we compute this using the following syntax:\n\n# Compute chi-squared\n-2 * (logLik(lm.0)[1] - logLik(lm.1)[1])\n\n[1] 12.66099",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#deviance-a-measure-of-the-modeldata-error",
    "href": "02-03-likelihood-framework-for-evidence.html#deviance-a-measure-of-the-modeldata-error",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.12 Deviance: A Measure of the Model–Data Error",
    "text": "6.12 Deviance: A Measure of the Model–Data Error\nIf we re-write the formula for the \\(\\chi^2\\)-statistic by distributing the \\(-2\\), we get a better glimpse of what this statistic is measuring.\n\\[\n\\chi^2 = -2 \\ln \\bigg[\\mathcal{L}({\\theta_0})\\bigg] - \\bigg(-2\\ln \\bigg[\\mathcal{L}({\\theta_1})\\bigg]\\bigg)\n\\]\nThe quantity \\(-2\\ln\\big[\\mathcal{L}(\\theta_k)\\big]\\) is referred to as the residual deviance3 of Model K. It measures the amount of misfit between the model and the data. (As such, when evaluating deviance values, lower is better.) For linear models, with the classic assumptions (\\(\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\)), the deviance is a function of the residual sum of squares (RSS):\n\\[\n\\mathrm{Deviance} = n \\ln\\big(2\\pi\\sigma^2_{\\epsilon}\\big) + \\frac{\\mathrm{RSS}}{\\sigma^2_{\\epsilon}}\n\\]\nwhere \\(\\mathrm{RSS}=\\sum\\epsilon_i^2\\) and \\(\\sigma^2_{\\epsilon} = \\frac{\\mathrm{RSS}}{n}\\). This formula illustrates that the residual deviance is a generalization of the residual sum of squares (RSS), and measures the model–data misfit.\n\n6.12.1 Mathematics of Deviance\nWe can express the deviance mathematically by multiplying the log-likelihood by \\(-2\\).\n\\[\n\\begin{split}\n\\mathrm{Deviance} &= -2 \\times\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) \\\\[1ex]\n&= -2 \\bigg(-\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\epsilon_i^2\\bigg) \\\\[1ex]\n&= -n\\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{1}{\\sigma^2_{\\epsilon}}\\sum \\epsilon_i^2 \\\\[1ex]\n&= -n\\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{\\mathrm{RSS}}{\\sigma^2_{\\epsilon}}\n\\end{split}\n\\]\nRewriting this using the parameters from the likelihood:\n\\[\n\\mathrm{Deviance} = -n\\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{\\sum_{i=1}^n \\big[Y_i-\\beta_0-\\beta_1(X_i)\\big]^2}{\\sigma^2_{\\epsilon}}\n\\]\nOnce again, we find that the only variables in the deviance function are the regression coefficients.\n\nIn practice, we will use the logLik() function to compute the deviance.\n\n# Compute the deviance for Model 0\n-2 * logLik(lm.0)[1]\n\n[1] 896.1768\n\n# Compute the deviance for Model 1\n-2 * logLik(lm.1)[1]\n\n[1] 883.5158\n\n\nHere the deviance for Model 1 (883.5) is less than the deviance for Model 0 (896.2). This indicates that the data have better fit to Model 1 than Model 0. How much better is the model–data fit for Model 1?\n\n# Compute difference in deviances\n896.2 - 883.5\n\n[1] 12.7\n\n\nModel 1 improves the fit (reduces the misfit) by 12.7 over Model 0. This is the value of our \\(\\chi^2\\)-statistic. That is, the \\(\\chi^2\\)-statistic is the difference in residual deviance values and measures the amount of improvement in the model–data misfit.\n\n\n6.12.2 Modeling the Variation in the Test Statistic\nIf the null hypothesis is true, the difference in deviances can be modeled using a \\(\\chi^2\\)-distribution. The degrees-of-freedom for this \\(\\chi^2\\)-distribution is based on the difference in the number of parameters between the complex and simpler model. In our case this difference is four (\\(3-2=1\\)):\n\\[\n\\chi^2(1) = 12.66\n\\]\n\n\nCode\n# Create dataset\nfig_01 = data.frame(\n  X = seq(from = 0, to = 20, by = 0.01)\n  ) %&gt;%\n  mutate(\n    Y = dchisq(x = X, df = 1)\n    )\n\n# Filter out X&lt;=65\nshaded = fig_01 %&gt;%\n  filter(X &gt;=12.66)\n\n# Create plot\nggplot(data = fig_01, aes(x = X, y = Y)) +\n  geom_line() +\n  xlab(\"Chi-squared\") +\n  ylab(\"Probability density\") +\n  theme_light() +\n  geom_ribbon(data = shaded, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4)\n\n\n\n\n\n\n\n\nFigure 6.1: Plot of the probability density function (PDF) for a \\(\\chi^2(1)\\) distribution. The grey shaded area represents the p-value based on \\(\\chi^2=12.66\\).\n\n\n\n\n\nTo compute the p-value we use the pchisq() function.\n\n# Compute p-value for X^2(1) = 14.50\n1 - pchisq(q = 12.66, df = 1)\n\n[1] 0.0003735622\n\n# Alternative method\npchisq(q = 12.66, df = 1, lower.tail = FALSE)\n\n[1] 0.0003735622\n\n\nBased on the p-value, we would reject the null hypothesis for the likelihood ratio test, which suggests that we should adopt the more complex model (Model 1). This means that the model that includes the effect of news exposure is more empirically supported than a model that includes no predictors. Note that we are making a holistic evaluation about the model rather than about individual predictors.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#using-the-lrtest-function",
    "href": "02-03-likelihood-framework-for-evidence.html#using-the-lrtest-function",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.13 Using the lrtest() Function",
    "text": "6.13 Using the lrtest() Function\nIn practice, we can also use the lrtest() function from the {lmtest} package to carry out a likelihood ratio test. We provide this function the name of the model object for the simpler model, followed by the name of the model object for the more complex model.\n\n# Load library\nlibrary(lmtest)\n\n# LRT to compare Model 0 and Model 1\nlrtest(lm.0, lm.1)\n\nLikelihood ratio test\n\nModel 1: knowledge ~ 1\nModel 2: knowledge ~ 1 + news\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   2 -448.09                         \n2   3 -441.76  1 12.661  0.0003734 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#evaluating-the-effect-of-news-exposure-in-research-question-2",
    "href": "02-03-likelihood-framework-for-evidence.html#evaluating-the-effect-of-news-exposure-in-research-question-2",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.14 Evaluating the Effect of News Exposure in Research Question 2",
    "text": "6.14 Evaluating the Effect of News Exposure in Research Question 2\nIn RQ 2, we are evaluating the effect of news exposure on news knowledge after controlling for the set of political and demographic covariates. To do this using a likelihood ratio test, we need to compare a baseline model that includes all of the covariates to a model that includes the covariates AND the effect of news exposure. That is, the only thing that is different between the two models is that the second model includes the effect of news exposure on top of the covariates. That is we will be comparing the following two models:\n\\[\n\\begin{split}\n\\mathbf{Simple~Model:~} \\quad \\mathrm{News~Knowledge}_i = &\\beta_0 + \\beta_1(\\mathrm{Education}_i) + \\beta_2(\\mathrm{Male}_i) + \\\\\n&\\beta_3(\\mathrm{Engagement}_i) + \\epsilon_i \\\\[2ex]\n\\mathbf{Complex~Model:~} \\quad \\mathrm{News~Knowledge}_i = &\\beta_0 + \\beta_1(\\mathrm{Education}_i) + \\beta_2(\\mathrm{Male}_i) + \\\\\n&\\beta_3(\\mathrm{Engagement}_i) + \\beta_8(\\mathrm{News~Exposure}_i) + \\epsilon_i \\\\[2ex]\n\\end{split}\n\\]\nBy comparing the parameters of the two models, we can see that the simpler model is nested in the more complex model.\n\\[\n\\begin{split}\n\\mathbf{Simple~Model~Parameters:}&\\quad\\{\\beta_0,~\\beta_{\\mathrm{Education}},~\\beta_{\\mathrm{Male}},~\\beta_{\\mathrm{Engagement}},~\\sigma^2_{\\epsilon}\\} \\\\[1ex]\n\\mathbf{Complex~Model~Parameters:}&\\quad\\{\\beta_0, ~\\beta_{\\mathrm{Education}},~\\beta_{\\mathrm{Male}},~\\beta_{\\mathrm{Engagement}},~\\beta_{\\mathrm{News~Exposure}},~\\sigma^2_{\\epsilon}\\} \\\\[1ex]\n\\end{split}\n\\]\nTo carry out the LRT we fit the two models, compute the difference in model deviances, and evaluate that difference in a \\(\\chi^2\\)-distribution with degrees-of-freedom equal to the difference in model complexity (based on the number of parameters being estimated).\n\n# Simple model\nlm.2 = lm(knowledge ~ 1 + education + male + engagement, data = pew)\n\n# Complex model\nlm.3 = lm(knowledge ~ 1 + education + male + engagement + news, data = pew)\n\n# Compute the difference in deviances between Model 1 and Model 2\n-2 * logLik(lm.2)[1] - (-2 * logLik(lm.3)[1])\n\n[1] 8.924345\n\n# Compute the difference in model complexity\n6 - 5\n\n[1] 1\n\n# Compute p-value for X^2(1) = 7.960401\npchisq(q = 8.924345, df = 1, lower.tail = FALSE)\n\n[1] 0.002813943\n\n\nThe p-value is 0.002813943, suggesting that there is an effect of news exposure on news knowledge, after controlling for the set of political and demographic covariates. Again, we could also obtain this same result via using the lrtest() function.\n\n# LRT to compare Model 2 and Model 3\nlrtest(lm.2, lm.3)\n\nLikelihood ratio test\n\nModel 1: knowledge ~ 1 + education + male + engagement\nModel 2: knowledge ~ 1 + education + male + engagement + news\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)   \n1   5 -420.78                        \n2   6 -416.32  1 8.9243   0.002814 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#evaluating-the-interaction-effect-in-research-question-3",
    "href": "02-03-likelihood-framework-for-evidence.html#evaluating-the-interaction-effect-in-research-question-3",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.15 Evaluating the Interaction Effect in Research Question 3",
    "text": "6.15 Evaluating the Interaction Effect in Research Question 3\nTo evaluate the potential interaction effect between news exposure and education on news knowledge, after controlling for demographic and political covariates, we need to fit the interaction model and then compare it to a model that includes all the same predictors EXCEPT the interaction effect.\n\n# Fit interaction model\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\n\nNote that Model 3 (fitted earlier) is the baseline comparison model to evaluate the interaction effect. Here we will use the lrtest() function to compare Models 3 and 4 to evaluate the interaction effect.\n\n# LRT to compare Model 1 and Model 2\nlrtest(lm.3, lm.4)\n\nLikelihood ratio test\n\nModel 1: knowledge ~ 1 + education + male + engagement + news\nModel 2: knowledge ~ 1 + education + male + engagement + news + news:education\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)  \n1   6 -416.32                       \n2   7 -413.74  1 5.1506    0.02324 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value is 0.02324, suggesting that there is an interaction effect between news exposure and education level after controlling for differences in the set of political and demographic covariates.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#evaluating-assumptions",
    "href": "02-03-likelihood-framework-for-evidence.html#evaluating-assumptions",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.16 Evaluating Assumptions",
    "text": "6.16 Evaluating Assumptions\nIf we were adopting a “final model”, the empirical evidence would support adopting Model 4. It is always important to evaluate any adopted final models’ assumptions.\n\n# Create residual plots\nresidual_plots(lm.4)\n\n\n\n\n\n\n\nFigure 6.2: Two residual plots for Model 4.\n\n\n\n\n\nBased on the density plot, the assumption of normality looks reasonably met. The scatterplot suggests the assumption that the average residual is 0 is generally met—the loess smoother suggests the average residual is close to 0 at all fitted values. The homoscadasticity assumption also seems reasonable with the range of residuals generally being constant across the different fitted values. Lastly, since the sample is a random sample of Americans, the independence assumption also seems tenable.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#presenting-the-results-from-the-lrts",
    "href": "02-03-likelihood-framework-for-evidence.html#presenting-the-results-from-the-lrts",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.17 Presenting the Results from the LRTs",
    "text": "6.17 Presenting the Results from the LRTs\nBelow we present a table summarizing the results of the likelihood ratio tests.\n\n\nCode\n# Load library\nlibrary(gt)\n\n# Set up data\nd = data.frame(\n  comparison = c(\"Model 0 vs. Model 1\", \"Model 2 vs. Model 3\", \"Model 3 vs. Model 4\"),\n  chi = c(\"$$\\\\chi^2(1) = 12.66$$\", \"$$\\\\chi^2(1) = 8.92$$\", \"$$\\\\chi^2(1) = 5.15$$\"),\n  p = c(\"&lt;.001\", \".003\", \".023\")\n)\n\n# Create table\nd |&gt;\n  gt() |&gt;\n  cols_label(\n    comparison = md(\"*Model Comparison*\"),\n    chi = \"LRT Result\",\n    p = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(comparison),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(chi, p),\n    align = \"center\"\n  ) |&gt; \n  tab_options(\n    table.width = pct(60),\n    quarto.disable_processing = TRUE\n    )\n\n\n\n\nTable 6.1: Results from a set of likelihood ratio tests (LRT) to compare sets of nested candidate models.\n\n\n\n\n\n\n  \n    \n      Model Comparison\n      LRT Result\n      p\n    \n  \n  \n    Model 0 vs. Model 1\n$$\\chi^2(1) = 12.66$$\n&lt;.001\n    Model 2 vs. Model 3\n$$\\chi^2(1) = 8.92$$\n.003\n    Model 3 vs. Model 4\n$$\\chi^2(1) = 5.15$$\n.023",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#reporting-individual-predictors-from-a-model",
    "href": "02-03-likelihood-framework-for-evidence.html#reporting-individual-predictors-from-a-model",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.18 Reporting Individual Predictors From a Model",
    "text": "6.18 Reporting Individual Predictors From a Model\nIt is also good practice to report the coefficient-level and model-level estimates from any adopted models in a regression table. We will report results from Model 4. At the coefficient-level, the coefficients and standard errors can be reported from the tidy() output. However, since we are using a likelihood framework, the p-values from the tidy() output are incorrect! We need to compute and report likelihood-based p-values.\nFor example, to evaluate the effect of education on news knowledge we would essentially want to test the hypothesis that:\n\\[\nH_0: \\beta_{\\mathrm{Education}} = 0\n\\]\nTo do this with a LRT, we need to compare two models:\n\nOne model that includes all of the predictors from Model 4;\nOne model that includes everything from Model 4 except the effect of education.\n\nThe only difference between these two models is the inclusion of the effect of education in the second model. That means any additional empirical support for the second model over the first is completely due to the effect of education. And, because the second model is nested in the first, we can evaluate this via a LRT.\n\n# Fit full model 4\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\n\n# Fit model without education\nlm.4_education = lm(knowledge ~ 1 + male + engagement + news + news:education, data = pew)\n\n# Carry out LRT\nlrtest(lm.4_education, lm.4)\n\nLikelihood ratio test\n\nModel 1: knowledge ~ 1 + male + engagement + news + news:education\nModel 2: knowledge ~ 1 + education + male + engagement + news + news:education\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -421.05                         \n2   7 -413.74  1 14.621  0.0001314 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value associated with the test of whether or not there is an effect of education (at least for the main effect) is 0.0001314. We will need to obtain the likelihood-based p-value for all of the other effects in a similar way. We will\n\n# Effect of male\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\nlm.4_male = lm(knowledge ~ 1 + education + engagement + news + news:education, data = pew)\nlrtest(lm.4_male, lm.4) # Carry out LRT\n\nLikelihood ratio test\n\nModel 1: knowledge ~ 1 + education + engagement + news + news:education\nModel 2: knowledge ~ 1 + education + male + engagement + news + news:education\n  #Df  LogLik Df Chisq Pr(&gt;Chisq)    \n1   6 -420.14                        \n2   7 -413.74  1  12.8  0.0003466 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Effect of engagement\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\nlm.4_engage = lm(knowledge ~ 1 + education + male + news + news:education, data = pew)\nlrtest(lm.4_engage, lm.4) # Carry out LRT\n\nLikelihood ratio test\n\nModel 1: knowledge ~ 1 + education + male + news + news:education\nModel 2: knowledge ~ 1 + education + male + engagement + news + news:education\n  #Df  LogLik Df Chisq Pr(&gt;Chisq)    \n1   6 -422.71                        \n2   7 -413.74  1 17.94  0.0000228 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Effect of new exposure (main-effect)\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\nlm.4_news = lm(knowledge ~ 1 + education + male + engagement + news:education, data = pew)\nlrtest(lm.4_news, lm.4) # Carry out LRT\n\nLikelihood ratio test\n\nModel 1: knowledge ~ 1 + education + male + engagement + news:education\nModel 2: knowledge ~ 1 + education + male + engagement + news + news:education\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)   \n1   6 -417.46                        \n2   7 -413.74  1 7.4479   0.006351 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Effect of interaction\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\nlm.4_interaction = lm(knowledge ~ 1 + education + male + engagement + news, data = pew)\nlrtest(lm.4_interaction, lm.4) # Carry out LRT\n\nLikelihood ratio test\n\nModel 1: knowledge ~ 1 + education + male + engagement + news\nModel 2: knowledge ~ 1 + education + male + engagement + news + news:education\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)  \n1   6 -416.32                       \n2   7 -413.74  1 5.1506    0.02324 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Intercept\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\nlm.4_intercept = lm(knowledge~ 0 + education + male + engagement + news + news:education, data = pew)\nlrtest(lm.4_intercept, lm.4) # Carry out LRT\n\nLikelihood ratio test\n\nModel 1: knowledge ~ 0 + education + male + engagement + news + news:education\nModel 2: knowledge ~ 1 + education + male + engagement + news + news:education\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -419.43                         \n2   7 -413.74  1 11.382  0.0007417 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can then replace the p-values from the tidy() output with these likelihood-based p-values. Also, since these p-values are based on chi-squared (not a t-value), we should replace the t-values from the tidy() output with the \\(\\chi^2\\)-values from the LRTs.\n\ntidy(lm.4) |&gt;\n  mutate(\n    statistic = c(11.38, 14.62, 12.80, 17.94, 7.45, 5.15),\n    p.value = c(0.0007417, 0.0001314, 0.0003466, 0.0000228, 0.006351, 0.02324)\n  ) \n\n# A tibble: 6 × 5\n  term           estimate std.error statistic   p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    -80.8      24.0        11.4  0.000742 \n2 education        6.44      1.68       14.6  0.000131 \n3 male            11.3       3.15       12.8  0.000347 \n4 engagement       0.391     0.0910     17.9  0.0000228\n5 news             1.17      0.436       7.45 0.00635  \n6 education:news  -0.0663    0.0298      5.15 0.0232   \n\n\nIf we were reporting this for publication, we could round the p-values to three decimal places. We would also want to indicate that these are likelihood-based p-values, and that the LRT is based on 1 df.\n\n\nCode\n# Load library\nlibrary(gt)\nlibrary(gtsummary) # for formatting p-values\n\n# Create table\ntidy(lm.4) |&gt;\n  mutate(\n    term = c(\"Intercept\", \"Education-level\", \"Male\", \"Political engagement\", \"News exposure\", \"Education-level x News exposure\"),\n    statistic = c(11.38, 14.62, 12.80, 17.94, 7.45, 5.15),\n    p.value = style_pvalue(c(0.001, 0.0001314, 0.0003466, 0.0000228, 0.006351, 0.02324), digits = 3)\n  )  |&gt;\n  gt() |&gt;\n  cols_label(\n    term = md(\"*Predictor*\"),\n    estimate = md(\"*B*\"),\n    std.error = md(\"*SE*\"),\n    statistic = \"$$\\\\chi^2$$\",\n    p.value = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(term),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(estimate, std.error, statistic, p.value),\n    align = \"center\"\n  ) |&gt; \n  fmt_number(\n    columns = c(estimate, std.error),\n    decimals = 2,\n    use_seps = FALSE\n  ) |&gt;\n  tab_options(\n    table.width = pct(70)\n    ) |&gt;\n  tab_footnote(\n    footnote = md(\"Male is a dummy-coded predictor with non-male as the reference group.\"),\n    locations =  cells_body(columns = term, rows = 3)\n  ) \n\n\n\n\nTable 6.2: Coefficients and standard errors for Model 4. The \\(\\chi^2\\) values and p-values are based on likelihood ratio tests (LRT) with 1 degree-of-freedom to evaluate each predictor.\n\n\n\n\n\n\n\n\n\nPredictor\nB\nSE\n$$\\chi^2$$\np\n\n\n\n\nIntercept\n−80.79\n24.00\n11.38\n0.001\n\n\nEducation-level\n6.44\n1.68\n14.62\n&lt;0.001\n\n\nMale1\n11.30\n3.15\n12.80\n&lt;0.001\n\n\nPolitical engagement\n0.39\n0.09\n17.94\n&lt;0.001\n\n\nNews exposure\n1.17\n0.44\n7.45\n0.006\n\n\nEducation-level x News exposure\n−0.07\n0.03\n5.15\n0.023\n\n\n\n1 Male is a dummy-coded predictor with non-male as the reference group.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#references",
    "href": "02-03-likelihood-framework-for-evidence.html#references",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.19 References",
    "text": "6.19 References\n\n\n\n\n\n\nCohen, J. (1994). The earth is round (\\(p &lt; .05\\)). American Psychologist, 49(12), 997–1003.\n\n\nJohansson, T. (2011). Hail the impossible: P-values, evidence, and likelihood. Scandinavian Journal of Psychology, 52, 113–125. https://doi.org/10.1111/j.1467-9450.2010.00852.x\n\n\nJohnstone, D. J. (1986). Tests of significance in theory and practice. The Statistician, 35, 491–504.\n\n\nMosteller, F., & Bush, R. B. (1954). Selected quantitative techniques. In G. Lindzey (Ed.), Handbook of social psychology (pp. 289–334). Addison-Wesley.\n\n\nWeakliem, D. L. (2016). Hypothesis testing and model selection in the social sciences. The Guilford Press.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#footnotes",
    "href": "02-03-likelihood-framework-for-evidence.html#footnotes",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "",
    "text": "Sometimes this is also referred to a the sum of squared errors (SSE).↩︎\nThis is in some sense mixing the paradigms of likelihood-based evidence and classical hypothesis test-based evidence. In a future set of notes we will learn about information criteria which eliminate the need to mix these two paradigms.↩︎\nThe use of the term “residual deviance” is not universal. Some textbooks omit the “residual” part and just refer to it as the “deviance”. Others use the term “model deviance”.↩︎",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html",
    "href": "02-04-likelihood-framework-for-estimation.html",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "",
    "text": "7.1 Likelihood\nIn this set of notes, you will learn about the method of maximum likelihood to estimate model parameters.\nRemember that likelihood is the probability of a particular set of parameters GIVEN (1) the data, and (2) the data are from a particular distribution (e.g., normal). Symbolically,\n\\[\n\\mathrm{Likelihood} = P(\\mathrm{Parameters} \\mid \\mathrm{Distribution~and~Data})\n\\]\nAlso recall, that to compute the likelihood we compute the joint probability density of the data under that particular set of parameters. For example, to compute \\(\\mathcal{L}(\\mu = 20, \\sigma = 4 \\mid x = \\{30, 20, 24, 27\\}; \\mathcal{N})\\), we use the following syntax:\n# Compute likelihood mu=20, sigma=4\nprod(dnorm(x = c(30, 20, 24, 27), mean = 20, sd = 4))\n\n[1] 5.702554e-07",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#likelihood-as-a-framework-for-estimation",
    "href": "02-04-likelihood-framework-for-estimation.html#likelihood-as-a-framework-for-estimation",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.2 Likelihood as a Framework for Estimation",
    "text": "7.2 Likelihood as a Framework for Estimation\nLikelihood can be used as the basis for estimating parameters given a model and data. The idea is that the “best” estimates for the parameters are those that produce the highest likelihood given the data and model. Consider this example: Which set of parameters,\\(\\mathcal{N}(20,4)\\) or \\(\\mathcal{N}(25,4)\\), was more likely to generate the data \\(x = \\{30, 20, 24, 27\\}\\)? To answer this, we compute the likelihood for both candidate sets of parameters.\n\n# Compute likelihood mu=20, sigma=4\nprod(dnorm(x = c(30, 20, 24, 27), mean = 20, sd = 4))\n\n[1] 5.702554e-07\n\n# Compute likelihood mu=25, sigma=4\nprod(dnorm(x = c(30, 20, 24, 27), mean = 25, sd = 4))\n\n[1] 1.774012e-05\n\n\nSince the second set of parameters produced a higher likelihood, it is more probable that the data were generated from the \\(\\mathcal{N}(25,4)\\) distribution than from the \\(\\mathcal{N}(20,4)\\) distribution. (Using the likelihood ratio, we can also say how much more likely the data were to be generated from this distribution.)\nSo now we come to the crux of Maximum Likelihood Estimation (MLE). The goal of MLE is to find a set of parameters that MAXIMIZES the likelihood given the data and a distribution. For example, given the observed data \\(x = \\{30, 20, 24, 27\\}\\) were generated from a normal distribution, what are the values for the parameters of this distribution (mean and standard deviation) that produce the HIGHEST (or maximum) value of the likelihood?\nWhichever parameters produce the highest likelihood end up being the parameter estimates. We will illustrate this a couple examples.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#example-1-using-mle-to-estimate-the-mean",
    "href": "02-04-likelihood-framework-for-estimation.html#example-1-using-mle-to-estimate-the-mean",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.3 Example 1: Using MLE to Estimate the Mean",
    "text": "7.3 Example 1: Using MLE to Estimate the Mean\nFor our first example, we will keep it simple by only trying to estimate a single parameter value using MLE. For this example, we will try to estimate the mean value for the four data values, \\(x=\\{30,20,24,27\\}\\). To do this we will assume that the data were generated from a normal distribution and that the standard deviation in this normal distribution was \\(\\sigma=4.272\\).\nWhat we need to do, is compute the likelihood for several different \\(\\mu\\) values, and determine which produces the highest likelihood value. Here we try five different values for \\(\\mu\\).\n\n# Compute L(mu = 10)\nprod(dnorm(c(30, 20, 24, 27), mean = 10, sd = 4.272))\n\n[1] 1.449116e-16\n\n# Compute L(mu = 15)\nprod(dnorm(c(30, 20, 24, 27), mean = 15, sd = 4.272))\n\n[1] 1.695637e-10\n\n# Compute L(mu = 20)\nprod(dnorm(c(30, 20, 24, 27), mean = 20, sd = 4.272))\n\n[1] 8.27684e-07\n\n# Compute L(mu = 25)\nprod(dnorm(c(30, 20, 24, 27), mean = 25, sd = 4.272))\n\n[1] 1.685382e-05\n\n# Compute L(mu = 30)\nprod(dnorm(c(30, 20, 24, 27), mean = 30, sd = 4.272))\n\n[1] 1.431642e-06\n\n\nBased on this it looks like a \\(\\mu\\approx25\\) produces the highest likelihood. We can also plot the likelihood versus the candidate parameter values.\n\n\nCode\n# Create data\nexample_01 = data.frame(\n  mu = c(10, 15, 20, 25, 30)\n  ) |&gt;\n  rowwise() |&gt;\n  mutate(\n    L = prod(dnorm(c(30, 20, 24, 27), mean = mu, sd = 4.272))\n    )\n\n\n# Plot\nggplot(data = example_01, aes(x = mu, y = L)) +\n  geom_line(color = \"darkgrey\", size = 0.5) +\n  geom_point() +\n  xlab(expression(mu)) +\n  ylab(\"Likelihood\") +\n  theme_light()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nFigure 7.1: Plot of the likelihood values for five candidate parameter values.\n\n\n\n\n\nWe could then continue to try values around 30 to hone in on the \\(\\mu\\) value that produces the highest likelihood (e.g., \\(\\mu=\\{24.7, 24.8, 24.9, 25.1, 25.2, 25.3\\}\\)). This methodology essentially boils down to continuing to narrow the search space by determining the likelihood value for more and more precisely defined values of the parameter.\nWe could also carry out this search computationally. Here for example, I set up a data frame that includes candidates for \\(\\mu\\). This search space is looks at all values of \\(\\mu\\) between 10.00 and 30.00. This is called the search space. (Given our search space, we will be able to determine \\(\\mu\\) to within the nearest hundredth.) We are then going to compute the likelihood based on each of those values. We use rowwise() so that the likelihood (in the mutate() layer) is carried out correctly; using the \\(\\mu\\) value in each row. Because rowwise() At the end of the chain, we use ungroup() to return the results to an ungrouped data frame.\n\n# Set up parameter search space\n# Compute likelihood\nexample_01_grid = data.frame(\n  mu = seq(from = 10, to = 30, by = 0.01)\n  ) |&gt;\n  rowwise() |&gt;\n  mutate(\n    L = prod(dnorm(c(30, 20, 24, 27), mean = mu, sd = 4.272))\n    ) |&gt;\n  ungroup()\n\n# View results\nhead(example_01_grid)\n\n# A tibble: 6 × 2\n     mu        L\n  &lt;dbl&gt;    &lt;dbl&gt;\n1  10   1.45e-16\n2  10.0 1.50e-16\n3  10.0 1.55e-16\n4  10.0 1.60e-16\n5  10.0 1.66e-16\n6  10.0 1.71e-16\n\n\nWe can then plot the likelihood versus the parameter values for these 2,001 parameter candidates. Typically, when there are many values, we do this using a line plot. This is what is referred to as a profile plot.\n\n# Plot the likelihood versus the parameter values\nggplot(data = example_01_grid, aes(x = mu, y = L)) +\n  geom_line() +\n  xlab(expression(mu)) +\n  ylab(\"Likelihood\") +\n  theme_light()\n\n\n\n\n\n\n\nFigure 7.2: Likelihood profile for several parameter values assuming a normal distribution and a standard deviation of 4.272.\n\n\n\n\n\nUsing the profile plot, we can see that the \\(\\mu\\) value that produces the largest value for the likelihood is a bit higher than 25. We can find the exact value in our search space by arranging the rows in our search data frame by their likelihood values, and then using the slice_max() function to find the row with the highest likelihood value. The n=1 argument finds the maximum row. If you wanted the highest two rows, we would use n=2\n\n# Find mu with maximum likelihood\nexample_01_grid |&gt;\n  slice_max(L, n = 1)\n\n# A tibble: 1 × 2\n     mu         L\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  25.2 0.0000170\n\n\nIn the candidate values for \\(\\mu\\) that we included in the search space, \\(\\mu=25.25\\) produces the highest likelihood. Thus, given the data and that the data were generated from a normal distribution with \\(\\sigma=4.272\\), the most probable value for \\(\\mu\\) is 25.25. This is our maximum likelihood estimate for the mean!\n\nYou can get more precision in the estimate by changing the by= argument in the seq() function when you are initially setting up your search space. For example if you need the estimate to the nearest 1000th, set by=.001.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#example-2-using-mle-to-estimate-the-mean-and-standard-deviation",
    "href": "02-04-likelihood-framework-for-estimation.html#example-2-using-mle-to-estimate-the-mean-and-standard-deviation",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.4 Example 2: Using MLE to Estimate the Mean and Standard Deviation",
    "text": "7.4 Example 2: Using MLE to Estimate the Mean and Standard Deviation\nIn the previous example, we assumed that we knew the value for \\(\\sigma\\). In practice, this often needs to be estimates along with \\(\\mu\\). To do this, you need to set up a search space that includes different combinations of \\(\\mu\\) and \\(\\sigma\\). Here we search \\(\\mu = \\{10.0, 10.1, 10.2,\\ldots, 30.0\\}\\) and \\(\\sigma=\\{0.0, 0.1, 0.2,\\ldots,10.0\\}\\) values from 0.1 to 10.0. (Remember, that \\(\\sigma\\geq0\\)).\nThe crossing() function creates every combination of \\(\\mu\\) and \\(\\sigma\\) that we define in our search space. So, for example, [\\(\\mu=10.0; \\sigma=0.0\\)], ]\\(\\mu=10.0; \\sigma=0.1\\)], [\\(\\mu=10.0; \\sigma=0.2\\)], etc. Since we have included 201 \\(\\mu\\) values and 101 \\(\\sigma\\) values, the search space is \\(201 \\times 101 = 20,301\\) parameter combinations.\n\n# Set up search space\n# Compute the likelihood\nexample_02 = crossing(\n  mu = seq(from = 10, to = 30, by = 0.1),\n  sigma = seq(from = 0, to = 10, by = 0.1)\n  ) |&gt;\n  rowwise() |&gt;\n  mutate(\n    L = prod(dnorm(c(30, 20, 24, 27), mean = mu, sd = sigma))\n    ) |&gt;\n  ungroup()\n\n# Find row with highest likelihood\nexample_02 |&gt;\n  slice_max(L, n = 1)\n\n# A tibble: 2 × 3\n     mu sigma         L\n  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1  25.2   3.7 0.0000183\n2  25.3   3.7 0.0000183\n\n\nThe parameters that maximize the likelihood (in our search space) are a mean of 25.2 and a standard deviation of 3.7. Again, if you need to be more precise in these estimates, you can increase the precision in the by= argument of the seq() functions.\nIn computer science, this method for finding the MLE is referred to as a grid search. This is because the combinations of parameter values in the search space constitute a grid. In the figure below, the search space for each parameter is listed in the first row/column of the table. Every other cell of the table (the “grid”) constitutes a particular combination of the parameters. We are then computing the likelihood for each combination of parameters and searching for the cell with the highest likelihood.\n\n\n\n\n\n\n\n\nFigure 7.3: Grid showing the combinations of parameter values used in the search space.\n\n\n\n\n\n\nWhen we have two (or more) parameters we need to estimate the time taken to carry out a grid search is increased in a non-linear way. For example, combining 100 values of each parameter does not result in a search space of 200, but a search space of 10,000. So increasing the precision of both parameters to by=.01 increases each the number of candidates from \\(20,301\\) to \\(2001 \\times 1001 = 2,003,001\\). This increase the computational time it takes to solve the problem.\nIf you are relying on grid search, it is often better to operate with less precision initially, and then identify smaller parts of the grid that can be searched with more precision.\n\n\n\n7.4.1 Likelihood Profile for Multiple Parameters\nWe could also plot the profile of the likelihood for our search space, but this time there would be three dimensions: one dimension for \\(\\mu\\) (x-axis), one dimension for \\(\\sigma\\) (y-axis), and one dimension for the likelihood (z-axis). When we plot the likelihood profile across both \\(\\mu\\) and \\(\\sigma\\), the profile looks like an asymmetrical mountain. The highest likelihood value is at the summit of the mountain and corresponds to \\(\\mu=25.2\\) and \\(\\sigma=3.7\\).\n\n\nCode\n# Load library\nlibrary(plot3D)\n\nscatter3D(x = example_02$mu, y = example_02$sigma, z = example_02$L, \n          pch = 18, cex = 2, theta = 45, phi = 20, ticktype = \"detailed\",\n          xlab = expression(mu), ylab = expression(sigma), zlab = \"Likelihood\",\n          colkey = FALSE,\n          colvar = example_02$L,\n          col = ramp.col(col = c(\"#f6eff7\", \"#bdc9e1\", \"#67a9cf\", \"#1c9099\", \"#016c59\"), n = 100, alpha = 1)\n)\n\n\n\n\n\n\n\n\nFigure 7.4: Likelihood profile for the search space of both \\(\\mu\\) and \\(\\sigma\\) assuming a normal distribution.\n\n\n\n\n\nIf we extend our estimation to three or more parameters, we can still use the computational search to find the maximum likelihood estimates (MLEs), but it would be difficult to plot (there would be four or more dimensions). In general, the profile plots are more useful as a pedagogical tool rather than as a way of actually finding the MLEs.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#log-likelihood",
    "href": "02-04-likelihood-framework-for-estimation.html#log-likelihood",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.5 Log-Likelihood",
    "text": "7.5 Log-Likelihood\nThe likelihood values are quite small since we are multiplying several probability densities together. To alleviate this issue, it is typical to compute the natural logarithm of the likelihood and operate on it, rather than on the likelihood itself. For example, in our first example, we would compute the log-likelihood1 and then determine the \\(\\mu\\) value that has the highest log-likelihood value.\n\n# Set up parameter search space\n# Compute likelihood and\nexample_01 = data.frame(\n  mu = seq(from = 10, to = 30, by = 0.01)\n  ) |&gt;\n  rowwise() |&gt;\n  mutate(\n    L = prod(dnorm(c(30, 20, 24, 27), mean = mu, sd = 4.272)),\n    ln_L = log(L)\n    ) |&gt;\n  ungroup()\n\n# Find mu with maximum log-likelihood\nexample_01 |&gt;\n  slice_max(ln_L, n = 1)\n\n# A tibble: 1 × 3\n     mu         L  ln_L\n  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1  25.2 0.0000170 -11.0\n\n\nThe profile of the log-likelihood looks a little different than that of the likelihood. What is important here is that the \\(\\mu\\) value that produces the highest value for the log-likelihood, is the same \\(\\mu\\) value that produces the highest likelihood.\n\n# Plot the log-likelihood versus the parameter values\nggplot(data = example_01, aes(x = mu, y = ln_L)) +\n  geom_line() +\n  xlab(expression(mu)) +\n  ylab(\"Log-likelihood\") +\n  theme_light()\n\n\n\n\n\n\n\nFigure 7.5: Log-likelihood profile for several parameter values assuming a normal distribution and a standard deviation of 4.272.\n\n\n\n\n\nMaximizing the log-likelihood gives the same parameter values as maximizing the likelihood. Remember that , so maximizing the log-likelihood is the same as maximizing the likelihood2.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#example-3-using-mle-to-estimate-regression-parameters",
    "href": "02-04-likelihood-framework-for-estimation.html#example-3-using-mle-to-estimate-regression-parameters",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.6 Example 3: Using MLE to Estimate Regression Parameters",
    "text": "7.6 Example 3: Using MLE to Estimate Regression Parameters\nIn estimating parameters for a regression model, we want to maximize the likelihood (or log-likelihood) for a given set of residuals that come from a normal distribution. We use the residuals since that is what we make distributional assumptions about in the model (e.g., normality, homogeneity of variance, independence). Our goal in regression is to estimate a set of parameters (\\(\\beta_0\\), \\(\\beta_1\\)) that maximize the likelihood of the residuals.\nTo understand this, consider the following a toy example of \\(n=10\\) observations.\n\n\nCode\n# create toy data\nexample_03 = data.frame(\n  x = c(4, 0, 3, 4, 7, 0, 0, 3, 0, 2),\n  y = c(53, 56, 37, 55, 50, 36, 22, 75, 37, 42)\n) \n\n# Create table\nexample_03 |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(x, y),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    x = md(\"*x*\"),\n    y = md(\"*y*\")\n  ) |&gt;\n  tab_options(\n   table.width = pct(30) \n  )\n\n\n\n\nTable 7.1: Toy data set that includes predictor (x) and outcome (y) values.\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n4\n53\n\n\n0\n56\n\n\n3\n37\n\n\n4\n55\n\n\n7\n50\n\n\n0\n36\n\n\n0\n22\n\n\n3\n75\n\n\n0\n37\n\n\n2\n42\n\n\n\n\n\n\n\n\n\n\nWe initially enter these observations into two vectors, x and y.\n\n# Enter data into vectors\nx = c(4, 0, 3, 4, 7, 0, 0, 3, 0, 2)\ny = c(53, 56, 37, 55, 50, 36, 22, 75, 37, 42)\n\nNext, we will write a function to compute the log-likelihood of the residuals given a set of coefficient estimates. The bones for how we will create such a function is show below.\nll = function(b_0, b_1){\n\n  *Compute and output the log-likelihood*\n}\nWe use the function() function to write new functions. In our example, this function will be called ll. The arguments to the function() function are the inputs that a user of the function needs to input. Here we are asking users to input the two regression coefficients for a simple linear regression, namely \\(\\hat{\\beta}_0\\) (b_0) and \\(\\hat{\\beta}_1\\) (b_1).\nAll the computation that the function is going to execute is placed in-between the curly braces. For us this means we need to:\n\nCompute the residuals based on the inputs to the function;\nCompute the log-likelihood based on the residuals; and\nOutput the log-likelihood value.\n\nTo compute the residuals, we need to compute the fitted values, and subtract those from the outcome values. This means that we need x and y defined inside our function3.\nOnce we have the residuals, we compute the log-likelihood by incorporating the assumptions of the regression model. Since we assume the residuals are normally distributed, we compute the log-likelihood using the dnorm() function. The regression assumptions also specify that the mean residual value is 0; which implies that we should use the argument mean=0 in the dnorm() function.\nThe assumption about the standard deviation is that the conditional distributions all have the same SD, but it doesn’t specify what that value is. However, the SD of the errors seems like a reasonable value, so we will use that.\nFinally, we can output values from a function using the return() function. Below, we will write a function called ll() that takes two arguments as input, b0= and b1=, and outputs the log-likelihood.\n\n# Function to compute the log-likelihood\nll = function(b_0, b_1){\n\n  # Use the following x and y values\n  x = c(4, 0, 3, 4, 7, 0, 0, 3, 0, 2)\n  y = c(53, 56, 37, 55, 50, 36, 22, 75, 37, 42)\n\n  # Compute the yhat and residuals based on the two input values\n  yhats = b_0 + b_1*x\n  errors = y - yhats\n\n  # Compute the sd of the residuals\n  sigma = sd(errors)\n\n  # Compute the log-likelihood\n  log_lik = sum(dnorm(errors, mean = 0, sd = sigma, log = TRUE))\n\n  # Output the log-likelihood\n  return(log_lik)\n\n}\n\nNow we read in our function by highlighting the whole thing and running it. Once it has been read in, we can use it just like any other function. For example to find the log-likelihood for the parameters \\(\\beta_0=10\\) and \\(\\beta_1=3\\) we use:\n\n# Compute log-likelihood for b_0=10 and b_1=3\nll(b_0 = 10, b_1 = 3)\n\n[1] -64.29224\n\n\nWe can also use our function to compute the log-likelihood in a grid search. Remember, our goal is to estimate the regression coefficients, so we are searching across values of \\(\\beta_0\\) and \\(\\beta_1\\).\n\n# Create data set of search values and log-likelihoods\nexample_03 = crossing(\n  B0 = seq(from = 30, to = 50, by = 0.1),\n  B1 = seq(from = -5, to = 5, by = 0.1)\n) |&gt;\n  rowwise() |&gt;\n  mutate(\n    ln_L = ll(b_0 = B0, b_1 = B1)\n    ) |&gt;\n  ungroup()\n\n# Find parameters that produce highest log-likelihood\nexample_03 |&gt;\n  slice_max(ln_L, n = 1)\n\n# A tibble: 1 × 3\n     B0    B1  ln_L\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  40.1   2.7 -39.5\n\n\nHere the parameter values that maximize the likelihood are \\(\\beta_0 = 40.1\\) and \\(\\beta_1=2.7\\). We can also compute what the standard deviation for the residual distributions was using the estimated parameter values. Remember, this value is an estimate of the RMSE.\n\n# Compute residuals using MLE estimate\nerrors = y - 40.1 - 2.7*x\n\n# Compute estimate of RMSE\nsd(errors)\n\n[1] 13.18665\n\n\nHere the maximum likelihood estimates for our three parameters are: \\(\\hat{\\beta}_0=40.1\\), \\(\\hat{\\beta}_1=2.7\\), and \\(\\hat{\\sigma}_{\\epsilon}=13.2\\).\n\n\n7.6.1 Complications with Grid Search\nIn practice, there are several issues with the grid search methods we have employed so far. The biggest is that you would not have any idea which values of \\(\\beta_0\\) and \\(\\beta_1\\) to limit the search space to. Essentially you would need to search an infinite number of values unless you could limit the search space in some way. For many common methods (e.g., linear regression) finding the ML estimates is mathematically pretty easy (if we know calculus; see the section Using Calculus to Determine the MLEs). For more complex methods (e.g., mixed-effect models) there is not a mathematical solution. Instead, mathematics is used to help limit the search space and then a grid search is used to hone in on the estimates.\nAlthough not a complication, we made an assumption about the value of the residual standard error, that it was equivalent to sigma(errors). In practice, this value would also need to be estimated, along with the coefficients.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#estimating-regression-parameter-ols-vs.-ml-estimation",
    "href": "02-04-likelihood-framework-for-estimation.html#estimating-regression-parameter-ols-vs.-ml-estimation",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.7 Estimating Regression Parameter: OLS vs. ML Estimation",
    "text": "7.7 Estimating Regression Parameter: OLS vs. ML Estimation\nTo compute ML estimates of the coefficients we will use the mle2() function from the {bbmle} package. To use the mle2() function, we need to provide a user-written function that returns the negative log-likelihood given a set of parameter inputs. Below we adapt the function we wrote earlier to return the negative log-likelihood. Since we are also interested in estimating the residual standard error (RSE), we also include this as an input into the function and use that inputted value in the dnorm() function.\n\n# Function to output the negative log-likelihood\nneg_ll = function(b_0, b_1, rse){\n\n  # Use the following x and y values\n  x = c(4, 0, 3, 4, 7, 0, 0, 3, 0, 2)\n  y = c(53, 56, 37, 55, 50, 36, 22, 75, 37, 42)\n\n  # Compute the yhat and residuals based on the two input values\n  yhats = b_0 + b_1*x\n  errors = y - yhats\n\n  # Compute the negative log-likelihood\n  neg_log_lik = -sum(dnorm(errors, mean = 0, sd = rse, log = TRUE))\n\n  # Output the log-likelihood\n  return(neg_log_lik)\n\n}\n\nNow we can use the mle2() function to estimate the three parameters. This function requires the argument, minuslogl=, which takes the user written function returning the negative log-likelihood. It also requires a list of starting values (initial guesses) for the input parameters in the user-written function.\n\n# Load library\nlibrary(bbmle)\n\nLoading required package: stats4\n\n\n\nAttaching package: 'bbmle'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n# Fit model using ML\nmle.results = mle2(minuslogl = neg_ll, start = list(b_0 = 20.0, b_1 = 5.0, rse = 10))\n\nWarning in dnorm(errors, mean = 0, sd = rse, log = TRUE): NaNs produced\n\n# View results\nsummary(mle.results)\n\nMaximum likelihood estimation\n\nCall:\nmle2(minuslogl = neg_ll, start = list(b_0 = 20, b_1 = 5, rse = 10))\n\nCoefficients:\n    Estimate Std. Error z value     Pr(z)    \nb_0  40.0072     5.6721  7.0533 1.748e-12 ***\nb_1   2.7361     1.7674  1.5481    0.1216    \nrse  12.5097     2.7973  4.4721 7.745e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n-2 log L: 78.90883 \n\n\nWe also obtain the OLS estimates:\n\n# Create data\nx = c(4, 0, 3, 4, 7, 0, 0, 3, 0, 2)\ny = c(53, 56, 37, 55, 50, 36, 22, 75, 37, 42)\n\n# Fit model with OLS\nlm.1 = lm(y ~ 1 + x)\n\n# Get estimates\nglance(lm.1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.193        0.0926  14.0      1.92   0.203     1  -39.5  84.9  85.8\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\ntidy(lm.1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    40.0       6.34      6.31 0.000231\n2 x               2.74      1.98      1.38 0.203   \n\n\nBoth sets of parameter estimates are presented in Table 7.2.\n\n\nCode\n# create toy data\nestimates = data.frame(\n  Estimate = c(\"$$\\\\hat{\\\\beta}_0$$\", \"$$\\\\hat{\\\\beta}_1$$\", \"$$\\\\hat{\\\\sigma}^2_{\\\\epsilon}$$\"),\n  ML = c(40.01, 2.74, 12.51),\n  OLS = c(40.01, 2.74, 13.99)\n) \n\n# Create table\nestimates |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(Estimate, ML, OLS),\n    align = \"center\"\n  ) |&gt;\n  tab_options(\n   table.width = pct(40) \n  )\n\n\n\n\nTable 7.2: Parameter estimates using maximum likelihood (ML) and ordinary least squares (OLS) estimation.\n\n\n\n\n\n\n\n\n\nEstimate\nML\nOLS\n\n\n\n\n$$\\hat{\\beta}_0$$\n40.01\n40.01\n\n\n$$\\hat{\\beta}_1$$\n2.74\n2.74\n\n\n$$\\hat{\\sigma}^2_{\\epsilon}$$\n12.51\n13.99\n\n\n\n\n\n\n\n\n\n\nComparing the coefficient estimates (\\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)) to those obtained through ordinary least squares, we find they are quite similar. The estimate of the residual standard error (\\(\\sigma_{\\epsilon}\\)), however, differs between the two estimation methods (although they are somewhat close in value).\n\n\n7.7.1 Estimating the Residual Variation: Maximum Likelihood vs. Ordinary Least Squares\nThe estimates of the residual standard error differ because the two estimation methods use different criteria to optimize over; OLS estimation finds the estimates that minimize the sum of squared errors, and ML finds the estimates that maximize the likelihood. Because of the differences, it is important to report how the model was estimated in any publication.\nBoth estimation methods have been well studied, and the resulting residual standard error from these estimation methods can be computed directly once we have the coefficient estimates (which are the same for both methods). Namely, the residual standard error resulting from OLS estimation is:\n\\[\n\\hat\\sigma_{\\epsilon}= \\sqrt{\\frac{\\left(Y_i - \\hat{Y}_i\\right)^2}{n-p-1}}\n\\]\nwhere p is the number of predictors in the model. And the residual standard error resulting from ML estimation is:\n\\[\n\\hat\\sigma_{\\epsilon}=\\sqrt{\\frac{\\left(Y_i - \\hat{Y}_i\\right)^2}{n}},\n\\]\nThe smaller denominator from the OLS estimate produces a higher overall estimate of the residual variation (more uncertainty). When n is large, the differences between the OLS and ML estimates of the residual standard error are minimal and can safely be ignored. When n is small, however, these differences can impact statistical results. For example, since the residual standard error is used to compute the standard error estimates for the coefficients, the choice of ML or OLS will have an effect on the size of the t- and p-values for the coefficients. (In practice, it is rare to see the different estimation methods producing substantively different findings, especially when fitting general linear models.)\nLastly, we note that the value of log-likelihood is the same for both the ML and OLS estimated models. The result from the ML output was:\n\\[\n\\begin{split}\n-2 \\ln(\\mathrm{Likelihood}) &= 78.91 \\\\[1ex]\n\\ln(\\mathrm{Likelihood}) &= -39.45\n\\end{split}\n\\]\nThe log-likelihood for the OLS estimated model is:\n\n# Log-likelihood for OLS model\nlogLik(lm(y ~ 1 + x))\n\n'log Lik.' -39.45442 (df=3)\n\n\nThis is a very useful result. It allows us to use lm() to estimate the coefficients from a model and then use its log-likelihood value in the same way as if we had fitted the model using ML. This will be helpful when we compute measure such as information criteria later in the course.\n\nIn many applications of estimation, it is useful to use a criterion which is modified variant of the likelihood. This variant omits “nuisance parameters” (parameters which are not of direct interest and subsequently not needed in the estimation method) from the computation of the likelihood. This restricted version of the likelihood is then maximized and the estimation method using this modified likelihood is called Restricted Maximum Likelihood (REML).\nWhen REML is used to estimate parameters, the residual standard error turns out to be the same as that computed in the OLS estimation. As such, sometimes this estimate is referred to as the REML estimate of the residual standard error.\n\n\n\n\n7.7.2 Using Calculus to Determine the MLEs\nA more convenient method to determine the ML estimates of the regression parameters is to use mathematics; specifically calculus. Remember, we can express the likelihood of the regression residuals mathematically as:\n\\[\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) = p(\\epsilon_1) \\times p(\\epsilon_2) \\times \\ldots \\times p(\\epsilon_n)\n\\]\nwhere the probability density of each residual (assuming normality) is:\n\\[\np(\\epsilon_i) = \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{(\\epsilon_i-\\mu)^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\]\nIn addition to normality, which gives us the equation to compute the PDF for each residual, the regression assumptions also specify that each conditional error distribution has a mean of 0 and some variance (that is the same for all conditional error distributions). We can call it \\(\\sigma^2_{\\epsilon}\\). Substituting these values into the density function, we get,\n\\[\n\\begin{split}\np(\\epsilon_i) &= \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{(\\epsilon_i-0)^2}{2\\sigma^2_{\\epsilon}}\\right] \\\\[1em]\n&= \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{(\\epsilon_i)^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\end{split}\n\\]\nNow we substitute this expression for each of the \\(p(\\epsilon_i)\\) values in the likelihood computation.\n\\[\n\\begin{split}\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) &= p(\\epsilon_1) \\times p(\\epsilon_2) \\times \\ldots \\times p(\\epsilon_n) \\\\[1em]\n&= \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{\\epsilon_1\n^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\ldots \\times\\\\ &~~~~\\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\end{split}\n\\]\nWe can simplify this:\n\\[\n\\begin{split}\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) = &\\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\exp\\left[-\\frac{\\epsilon_1^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\ldots \\times \\\\\n&\\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\end{split}\n\\]\nNow we will take the natural logarithm of both sides of the expression:\n\\[\n\\begin{split}\n\\ln \\Bigl(\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data})\\Bigr) = &\\ln \\Biggl( \\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\exp\\left[-\\frac{\\epsilon_1^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\ldots \\times \\\\\n&\\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right] \\Biggr)\n\\end{split}\n\\]\nUsing our rules for logarithms and re-arranging the terms gives us,\n\\[\n\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = -\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\epsilon_i^2\n\\]\nExamining this equation, we see that the log-likelihood is a function of n, \\(\\sigma^2_{\\epsilon}\\) and the sum of squared residuals (SSR). The observed data define n (the sample size) and the other two components come from the residuals, which are a function of the parameters and the data.\nOnce we have this function, calculus can be used to find the analytic maximum. Typically before we do this, we replace \\(\\epsilon_i\\) with \\(Y_i - \\hat\\beta_0 - \\hat\\beta_1(X_i)\\); writing the residuals as a function of the parameters (which we are solving for) and the data.\n\\[\n\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = -\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2\n\\]\nIn optimization, maximizing the log-likelihood is mathematically equivalent to minimizing the negative log-likelihood. (Note, this is what the mle2() function is doing.) That means we could also optimize over:\n\\[\n-\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = \\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2\n\\]\nThis has the advantage that we are removing the negative signs on the right-hand side of the equation. To find the analytic minimum (or maximum), we compute the partial derivatives with respect to \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), and \\(\\hat\\sigma^2_{\\epsilon}\\), and set these equal to zero and solve for each of the three parameters, respectively. That is:\n\\[\n\\begin{split}\n\\frac{\\partial}{\\partial \\beta_0} \\bigg[\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2\\bigg] &= 0 \\\\[1em]\n\\frac{\\partial}{\\partial \\beta_1} \\bigg[\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2\\bigg] &= 0 \\\\[1em]\n\\frac{\\partial}{\\partial \\sigma^2_{\\epsilon}} \\bigg[\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2\\bigg] &= 0\n\\end{split}\n\\]\nWithin each partial derivative, the parameters that are not being partialled can be treated as constants, which often makes the derivative easier to solve. For example in the first two partial derivatives the residual variance can be treated as a mathematical constant. Since all constant terms can be removed from the derivative, this leads to an interesting result:\n\\[\n\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\bigg[ -\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data})\\bigg] = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\bigg[ \\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2 \\bigg]\n\\]\nThis means that minimizing the negative log-likelihood is equivalent to minimizing the sum of squared residuals! This implies that the coefficients we get for OLS and ML estimation are the same.\nWhen we solve the third partial derivative for the residual standard error, we find that:\n\\[\n\\sigma^2_{\\epsilon} = \\frac{\\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2}{n}\n\\]\nThat is, the residual variance is equal to the sum of squared residuals divided by the sample size. In OLS estimation, the residual variance is the sum of squared residuals divided by the error degrees of freedom for the model. In the simple regression model the residual variance estimated using OLS would be:\n\\[\n\\sigma^2_{\\epsilon} = \\frac{\\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2}{n-2}\n\\]\nThis is why the residual standard errors were different when we used OLS and ML to carry out the estimation; the criteria we are optimizing over (sum of squared residuals vs. log-likelihood) impact the value of the residual variance estimate. Again, when n is large, the estimation method does not make a difference (i.e., \\(n \\approx n-2\\)).",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#references",
    "href": "02-04-likelihood-framework-for-estimation.html#references",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.8 References",
    "text": "7.8 References",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#footnotes",
    "href": "02-04-likelihood-framework-for-estimation.html#footnotes",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "",
    "text": "We could also compute the log-likelihood directly using sum(dnorm(c(30, 20, 24, 27), mean = mu, sd = 4.272, log = TRUE)).↩︎\nThis is because taking the logarithm of a set of numbers keeps the same ordination of values as the original values.↩︎\nAlternatively, x and y could be included as additional inputs into the function.↩︎",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "03-00-modeling-nonlinearity.html",
    "href": "03-00-modeling-nonlinearity.html",
    "title": "Modeling Nonlinearity",
    "section": "",
    "text": "Many relationships in the educational sciences are nonlinear. In this section, you will be introduced to several methods for modeling nonlinear relationships. Specifically, you will learn how to add a polynomial effect and also how to use logarithms to transform nonlinear relationships into linear relationships. Each of these is useful, depending on the specific type of nonlinearity inherent in the relationship.\nAdditionally, you will learn about using information criteria to evaluate models instead of p-values. Information criteria have many advantages over p-values when evaluating statistical models, including quantifying the uncertainty in selecting one model over another.",
    "crumbs": [
      "Modeling Nonlinearity"
    ]
  },
  {
    "objectID": "03-01-polynomial-effects.html",
    "href": "03-01-polynomial-effects.html",
    "title": "8  Polynomial Effects",
    "section": "",
    "text": "8.1 Examine Relationship between Graduation Rate and SAT Scores\nIn this chpater, you will learn one method of dealing with nonlinearity. Specifically, we will look at the inclusion of polynomial effects into a model. To do so, we will use the mn-schools.csv dataset (see the data codebook) to examine if (and how) academic “quality” of the student-body (measured by SAT score) is related to institutional graduation rate.\nAs always, we begin the analysis by graphing the data. We will create a scatterplot of the graduation rates versus the median SAT scores for the 33 institutions. We will also add a loess smoother to the plot, which is one empirical method that statisticians use to help determine the functional form to use in the regression model.\n# Scatterplot\nggplot(data = mn, aes(x = sat, y = grad)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"#ff2d21\") +\n  xlab(\"Estimated median SAT score (in hundreds)\") +\n  ylab(\"Six-year graduation rate\") +\n  theme_light() +\n  theme(\n    text = element_text(size = 20)\n  )\n\n\n\n\n\n\n\nFigure 8.1: Six-year graduation rate plotted as a function of median SAT score. The loess smoother (red, solid line) is also displayed.\nThe loess smoother suggests that the relationship between SAT scores and graduation rate may be nonlinear. Nonlinearity implies that a the effect of SAT on graduation rates is not constant across the range of SAT scores; for colleges with lower values of SAT (say SAT \\(&lt;1100\\)) the effect of SAT has a rather high, positive effect (steep slope), while for colleges with higher values of SAT (\\(\\geq 1100\\)) the effect of SAT is positive and moderate (the slope is less steep). Another way of saying this is that for schools with lower SAT scores, a one-unit difference in SAT is associated with a larger change in graduation rates than the same one-unit change for schools with higher SAT values.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Polynomial Effects</span>"
    ]
  },
  {
    "objectID": "03-01-polynomial-effects.html#examine-relationship-between-graduation-rate-and-sat-scores",
    "href": "03-01-polynomial-effects.html#examine-relationship-between-graduation-rate-and-sat-scores",
    "title": "8  Polynomial Effects",
    "section": "",
    "text": "8.1.1 Residual Plot: Another Way to Spot Nonlinearity\nSometimes, the nonlinear relationship is difficult to detect from the scatterplot of Y versus X. Often it helps to fit the linear model and then examine the assumption of linearity in the residuals. It is sometimes easier to detect nonlinearity in the scatterplot of the residuals versus the fitted values.\n\n# Fit linear model\nlm.1 = lm(grad ~ 1 + sat, data = mn)\n\n# Residual plot: Scatterplot\nresidual_plots(lm.1, type = \"s\") +\n  theme(\n    text = element_text(size = 20)\n  )\n\n\n\n\n\n\n\nFigure 8.2: Standardized residuals versus the fitted values for a model regressing six-year graduation rates on median SAT scores. The line \\(Y=0\\) (black) and the loess smoother (blue) are also displayed.\n\n\n\n\n\nThis plot suggests that the assumption of linearity may be violated; the average residual is not zero at each fitted value. For low fitted values it appears as though the average residual may be less than zero, for moderate fitted values it appears as though the average residual may be more than zero, and for high fitted values it appears as though the average residual may be less than zero.\nNotice that the pattern displayed in the residuals is consistent with the pattern of the observed data in the initial scatterplot (Figure 1). If we look at the data relative to the regression smoother we see that there is not even vertical scatter around this line. At low and high SAT scores the observed data tends to be below the regression line (the regression is over-estimating the average graduation rate), while for moderate SAT scores the observed data tends to be above the regression line (the regression is under-estimating the average graduation rate).",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Polynomial Effects</span>"
    ]
  },
  {
    "objectID": "03-01-polynomial-effects.html#polynomial-effects",
    "href": "03-01-polynomial-effects.html#polynomial-effects",
    "title": "8  Polynomial Effects",
    "section": "8.2 Polynomial Effects",
    "text": "8.2 Polynomial Effects\nOne way of modeling nonlinearity is by including polynomial effects. In regression, polynomial effects are predictors that have a power greater than one. For example, \\(x^2\\) (quadratic term), or \\(x^3\\) (cubic term). The lowest order polynomial effect is the quadratic, which mathematically is,\n\\[\nx^2 = x \\times x.\n\\]\nSo the quadratic term, \\(x^2\\) is a product of x times itself. Recall that products are how we express interactions. Thus the quadratic term of \\(x^2\\) is really the interaction of x with itself. To model this, we simply (1) create the product term, and (2) include the product term and all constituent main-effects in the regression model.\n\n# Create quadratic term in the data\nmn = mn |&gt;\n  mutate(\n    sat_quadratic = sat * sat\n  )\n\n# View data\nmn\n\n# A tibble: 33 × 7\n      id name                            grad public   sat tuition sat_quadratic\n   &lt;dbl&gt; &lt;chr&gt;                          &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n 1     1 Augsburg College                65.2      0  10.3    39.3         106. \n 2     3 Bethany Lutheran College        52.6      0  10.6    30.5         113. \n 3     4 Bethel University, Saint Paul…  73.3      0  11.4    39.4         131. \n 4     5 Carleton College                92.6      0  14      54.3         196  \n 5     6 College of Saint Benedict       81.1      0  11.8    43.2         140. \n 6     7 Concordia College at Moorhead   69.4      0  11.4    36.6         131. \n 7     8 Concordia University-Saint Pa…  47.9      0   9.9    37.8          98.0\n 8     9 Crossroads College              26.9      0   9.7    25.3          94.1\n 9    10 Crown College                   51.3      0  10.3    33.2         106. \n10    11 Gustavus Adolphus College       81.7      0  12.2    43.8         150. \n# ℹ 23 more rows\n\n\nAfter creating the quadratic term in the data, we fit the regression model that includes both the linear and quadratic effects as predictors in the model. We then compare the two models using a likelihood ratio test since the linear model is nested in the quadratic model.\n\n# Fit model\nlm.2 = lm(grad ~ 1 + sat + sat_quadratic, data = mn)\n\n# Likelihood ratio test\nlrtest(lm.1, lm.2)\n\nLikelihood ratio test\n\nModel 1: grad ~ 1 + sat\nModel 2: grad ~ 1 + sat + sat_quadratic\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)   \n1   3 -113.55                        \n2   4 -109.56  1 7.9778   0.004735 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBased on computing the likelihood ratio, the empirical support for the quadratic model is 54.05 times that for the linear model. This is more support than we expect because of sampling error, \\(\\chi^2(1)=7.98\\), \\(p=.005\\), and suggests that we should adopt the quadratic model. Once we have adopted a model, we can examine the model-level output from that model to obtain the estimate for the residual standard error and other pertinent summaries (e.g., \\(R^2\\)) that we want to report.\n\n# Model-level output\nglance(lm.2) |&gt;\n  print(width = Inf)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.835         0.824  7.02      76.0 1.81e-12     2  -110.  227.  233.\n  deviance df.residual  nobs\n     &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1    1478.          30    33\n\n\nModel 2 explains 83.5% of the variation in graduation rates. The residual standard error estimate is \\(\\hat\\sigma_{\\epsilon}=7.79\\).\n\n\n8.2.1 Are the Assumptions More Tenable for this Model?\nMore important than whether the p-value is small, is whether including the quadratic effect improved the assumption violation we noted earlier. To evaluate this, we will examine the two residual plots for the quadratic model: (1) a density plot of the standardized residuals; and (2) a plot of the standardized residuals versus the fitted values.\n\n# Residual plots\nresidual_plots(lm.2) +\n  theme(\n    text = element_text(size = 20)\n  )\n\n\n\n\n\n\n\nFigure 8.3: Residual plots for the quadratic model regressing six-year graduation rate on median SAT scores. LEFT: Density plot of the standardized residuals. The confidence envelope for a normal reference distribution (blue shaded area) is also displayed. RIGHT: Standardized residuals versus the fitted values. The line \\(Y=0\\) (black) and confidence envelope (grey shaded area) for that line are shown, along with the loess smoother (blue) esimating the mean pattern of the residuals.\n\n\n\n\n\nThe plot of the standardized residuals versus the fitted values suggests that the residuals for the quadratic model are far better behaved; indicating much more consistency with the assumption that the average residual is zero at each fitted value than the linear model. This is the evidence that we would use to justify retaining the quadratic effect in the model. This plot also suggests that the homoskedasticity assumption is tenable. Finally, the empirical density of the residuals also seems consistent with the assumption of normality for this model.\n\nREMINDER\nThe assumption of independence can’t be evaluated from the plots, but is vary important given we rely on it to be able to correctly compute the likelihood.\n\nFrom the design of the study, we have not sampled the schools randomly, nor randomly assigned the schools to their levels of the predictor(s), so we can’t infer independence from the design. So the question becomes, does knowing the graduation rate of a school in the population give us information about the graduation rate for other schools in the population with the same median SAT score? If the answer is “no”, then we can call the independence assumption tenable. If the answer is “yes”, the independence assumption is violated and we should not use the lm() function or believe that results from that function are valid.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Polynomial Effects</span>"
    ]
  },
  {
    "objectID": "03-01-polynomial-effects.html#interpretation-of-a-polynomial-term",
    "href": "03-01-polynomial-effects.html#interpretation-of-a-polynomial-term",
    "title": "8  Polynomial Effects",
    "section": "8.3 Interpretation of a Polynomial Term",
    "text": "8.3 Interpretation of a Polynomial Term\nHow do we interpret the quadratic effect of SAT? First, we will write out the fitted model based on the estimated coefficients. Note we could also obtain these using tidy(), but because we are using a likelihood framework of evidence, the t- and p-values are misleading.\n\n# Coefficients\ntidy(lm.2) |&gt;\n  select(-statistic, -p.value)\n\n# A tibble: 3 × 3\n  term          estimate std.error\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    -366.      98.6  \n2 sat              62.7     17.3  \n3 sat_quadratic    -2.15     0.751\n\n\nThe fitted equation is:\n\\[\n\\widehat{\\mathrm{Graduation~Rate}_i} = -366.34 + 62.72(\\mathrm{SAT}_i) - 2.15(\\mathrm{SAT}^2_i)\n\\]\nSince the quadratic term is an interaction, we can interpret this term as we do any other interaction, namely that:\n\nThe effect of median SAT score on six-year graduation rates depends on the magnitude of median SAT score.\n\nTo better understand the nature of this effect we will plot the fitted equation and use the plot to aid our interpretation.\n\nIMPORTANT\nNote that the t- and p-values from the tidy() output should not be reported or even considered in the likelihood framework. Remember, they are based in the classical hypothesis testing framework. If you need coefficient-level p-values, you will need to fit the appropriate nested models to evaluate the individual effects.\n\n\n\n8.3.1 Graphical Interpretation\nTo plot the fitted equation, we use the geom_function() layer to add the fitted curve. (Note that we can no longer use geom_abline() since the addition of the polynomial effect implies that a line is no longer suitable.) This layer takes the argument fun= which describes a function that will be plotted as a line. To describe a function, use the syntax function(){...}. Here we use this syntax to describe the function of x (which in the ggplot() global layer is mapped to the sat variable). The fitted equation is then also written in terms of x and placed inside the curly braces. (Note that it is best to be more exact in the coefficient values—don’t round—when you create this plot as even minor differences can grossly change the plot.)\n\n# Scatterplot\nggplot(data = mn, aes(x = sat, y = grad)) +\n    geom_point(alpha = 0.3) +\n  geom_function(\n    fun = function(x) {-366.34 + 62.72*x - 2.15 * x^2},\n    color = \"#0072b2\"\n    ) +\n  xlab(\"Estimated median SAT score (in hundreds)\") +\n  ylab(\"Six-year graduation rate\") +\n    theme_light() +\n  theme(\n    text = element_text(size = 20)\n  ) \n\n\n\n\n\n\n\nFigure 8.4: Scatterplot of six-year graduation rate versus median SAT score. The fitted curve from the quadratic regression model (blue line) is also displayed.\n\n\n\n\n\nThe fitted curve helps us interpret the nature of the relationship between median SAT scores and graduation rates. The effect of median SAT score on graduation rate depends on SAT score (definition of an interaction). For schools with low SAT scores, the effect of SAT score on graduation rate is positive and fairly high. For schools with high SAT scores, the effect of SAT score on graduation rate remains positive, but it has a smaller effect on graduation rates; the effect diminishes.\n\n\n\n8.3.2 Algebraic Interpretation\nFrom algebra, you may remember that the coefficient in front of the quadratic term (\\(-2.2\\)) informs us of whether the quadratic is an upward-facing U-shape, or a downward-facing U-shape. Since our term is negative, the U-shape is downward-facing. This is consistent with what we saw in the plot. What the algebra fails to show is that, within the range of SAT scores in our data, we only see part of the entire downward U-shape.\nThis coefficient also indicates whether the U-shape is skinny or wide. Although “skinny” and “wide” are only useful as relative comparisons. Algebraically, the comparison is typically to a quadratic coefficient of 1, which is generally not useful in our interpretation. The intercept and coefficient for the linear term help us locate the U-shape in the coordinate plane (moving it right, left, up, or down from the origin). You could work all of this out algebraically.\n\nFYI\nYou can see how different values of these coefficients affect the curve on Wikipedia. Here is an interactive graph that lets you explore how changing the different coefficients changes the parabola.\n\nWhat can be useful is to find where the minimum (upward facing U-shape) or maximum (downward facing U-shape) occurs. This point is referred to as the vertex of the parabola and can be algebraically determined. To do this we determine the x-location of the vertex by\n\\[\nx_\\mathrm{Vertex} = -\\frac{\\hat{\\beta}_1}{2 \\times \\hat\\beta_2}\n\\]\nwhere, \\(\\hat{\\beta}_1\\) is the estimated coefficient for the linear term and \\(\\hat{\\beta}_2\\) is the estimated coefficient for the quadratic term. The y coordinate for the vertex can then be found by substituting the x-coordinate into the fitted equation. For our example,\n\\[\nx_\\mathrm{Vertex} = -\\frac{62.72}{2 \\times -2.15} = 14.58\n\\]\nand\n\\[\ny_\\mathrm{Vertex} = -366.34 + 62.72(14.58) - 2.15(14.58^2) = 91.08\n\\]\nThis suggests that at a median SAT score of 1458 we predict a six-year graduate rate of 91.08. This x-value also represents the value at which the direction of the effect changes. In our example recall that for higher values of SAT the effect of SAT on graduation rate was diminishing. This is true for schools with median SAT scores up to 1458. For schools with higher SAT scores the effect of SAT score on graduation rate would theoretically be negative, and would be more negative for higher values.\nThis is all theoretical as our data only includes median SAT scores up to 1400. Everything past that value (including the vertex) is extrapolation. Extrapolation is exceedingly sketchy when we start fitting non-linear models. For example, do we really think that the average graduation rate for schools with a median SAT scores higher than 1458 would actually be smaller than for schools at 1458? It is more likely that the effect just flattens out.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Polynomial Effects</span>"
    ]
  },
  {
    "objectID": "03-01-polynomial-effects.html#alternative-r-syntax-fit-the-polynomial-term-using-the-i-function",
    "href": "03-01-polynomial-effects.html#alternative-r-syntax-fit-the-polynomial-term-using-the-i-function",
    "title": "8  Polynomial Effects",
    "section": "8.4 Alternative R Syntax: Fit the Polynomial Term using the I() Function",
    "text": "8.4 Alternative R Syntax: Fit the Polynomial Term using the I() Function\nWe can also use a method of fitting polynomial terms directly in the lm() function. To do this, we create the polynomial directly in the model using the I() function. When you use the I() function to create the polynomial term, you do not need to create a new column of squared values in the data set. Here we fit the same quadratic model using this method of fitting the model. (Note: You cannot use the colon notation (:) to fit a polynomial term in the model.)\n\n# Fit model using I() function\nlm.2 = lm(grad ~ 1 + sat + I(sat ^ 2), data = mn)\n\n# Model-level output\nglance(lm.2)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.835         0.824  7.02      76.0 1.81e-12     2  -110.  227.  233.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# Coefficients\ntidy(lm.2) |&gt;\n  select(-statistic, -p.value)\n\n# A tibble: 3 × 3\n  term        estimate std.error\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -366.      98.6  \n2 sat            62.7     17.3  \n3 I(sat^2)       -2.15     0.751",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Polynomial Effects</span>"
    ]
  },
  {
    "objectID": "03-01-polynomial-effects.html#adding-covariates-main-effects-model",
    "href": "03-01-polynomial-effects.html#adding-covariates-main-effects-model",
    "title": "8  Polynomial Effects",
    "section": "8.5 Adding Covariates: Main Effects Model",
    "text": "8.5 Adding Covariates: Main Effects Model\nWe can also include covariates in a polynomial model (to control for other predictors), the same way we do in a linear model, by including them as additive terms in the lm() model. Below we include the public dummy-coded predictor to control for the effects of sector.\n\n# Fit model\nlm.3 = lm(grad ~ 1 + sat + I(sat^2) + public, data = mn)\n\n# Compare Model 2 and Model 3\nlrtest(lm.2, lm.3)\n\nLikelihood ratio test\n\nModel 1: grad ~ 1 + sat + I(sat^2)\nModel 2: grad ~ 1 + sat + I(sat^2) + public\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   4 -109.56                         \n2   5 -101.80  1 15.511 0.00008203 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBased on computing the likelihood ratio, the empirical support for Model 3 is 2344.9 times that for the Model 2. This is more support than we expect because of sampling error, \\(\\chi^2(1)=15.51\\), \\(p&lt;.001\\), and suggests that we should adopt Model 3.\nWe can also examine the model-level output from Model 3 to summarize the variance accounted for and the residual standard error.\n\n# Model-level output\nglance(lm.3) |&gt;\n  print(width = Inf)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.897         0.886  5.64      84.1 2.05e-14     3  -102.  214.  221.\n  deviance df.residual  nobs\n     &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1     924.          29    33\n\n\nModel 3 explains 89.7% of the variation in graduation rates, an increase of 6.2 percentage points from Model 2. The residual standard error for Model 3, \\(\\hat\\sigma_{\\epsilon}=7.02\\), has also decreased from that for Model 2 indicating that this model has less error than Model 2. Next, we turn to the coefficient-level output.\n\n# Coefficients\ntidy(lm.3) |&gt;\n  select(-statistic, -p.value)\n\n# A tibble: 4 × 3\n  term        estimate std.error\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -384.      79.4  \n2 sat            67.0     13.9  \n3 I(sat^2)       -2.37     0.606\n4 public         -9.12     2.19 \n\n\nInterpreting the coefficient-level output:\n\nThe predicted average graduation rate for private institutions with a median SAT score of 0 is \\(-384.2\\). This is extrapolation.\nWe will not interpret the linear effect of SAT since we have included a higher-order polynomial effect of SAT. (Lower-order effects of interactions are not interpreted.)\nThe quadratic effect of SAT indicates that the effect of median SAT score on graduation rate varies by median SAT scores, after controlling for differences in sector.\nPublic institutions have a graduation rate that is 9.12 percentage points lower than private institutions, on average, controlling for differences in median SAT scores.\n\nWe can also write out the fitted equations for private and public institutions.\nPrivate Institutions\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} &= -384.16 + 67.04(\\mathrm{SAT}_i) - 2.37(\\mathrm{SAT}^2_i) - 9.12(0) \\\\[1ex]\n&= -384.16 + 67.04(\\mathrm{SAT}_i) - 2.37(\\mathrm{SAT}^2_i)\n\\end{split}\n\\]\nPublic Institutions\n\\[\n\\begin{split}\n\\mathbf{Public:~} \\hat{\\mathrm{Graduation~Rate}_i} &= -384.16 + 67.04(\\mathrm{SAT}_i) - 2.37(\\mathrm{SAT}^2_i) - 9.12(1) \\\\[1ex]\n&= -393.29 + 67.04(\\mathrm{SAT}_i) - 2.37(\\mathrm{SAT}^2_i)\n\\end{split}\n\\]\nNote that the coefficients associated with the linear and quadratic effects of median SAT score are identical for both private and public institutions. This implies that the effect of SAT on graduation rates, despite being nonlinear, is the same for both sectors. The difference in intercepts reflects the sector effect on average graduation rates. To visualize these effects, we can plot each fitted curve by including each fitted equation in a separate geom_function() layer.\n\n# Plot of the fitted model\nggplot(data = mn, aes(x = sat, y = grad)) +\n    geom_point(alpha = 0) +\n  # Private institutions\n    geom_function(\n      fun = function(x) {-384.16 + 67.04*x - 2.37 * x^2},\n      color = \"#2ec4b6\",\n      linetype = \"dashed\"\n      ) +\n  # Public institutions\n  geom_function(\n    fun = function(x) {-393.29 + 67.04*x - 2.37 * x^2},\n    color = \"#ff9f1c\",\n    linetype = \"solid\"\n    ) +\n  xlab(\"Estimated median SAT score (in hundreds)\") +\n  ylab(\"Six-year graduation rate\") +\n    theme_light() +\n  theme(\n    text = element_text(size = 20)\n  )\n\n\n\n\n\n\n\nFigure 8.5: Six-year graduation rate as a function of median SAT score for private (orange, solid line) and public (blue, dashed line) institutions.\n\n\n\n\n\nThe curvature (linear and quadratic slopes) of the lines shows the linear and quadratic effect of median SAT scores on graduation rate. For both sectors, the effect of median SAT on graduation rates is positive (institutions with higher median SAT scores tend to have higher graduation rates. But, this effect diminishes for institutions with increasingly higher SAT scores. The main effect of sector is also visualized since private schools have higher graduation rates, on average, than public schools for all levels of median SAT score. This difference, regardless of median SAT score, is constantly that public schools have a lower graduation rate than private schools by 9.12 percentage points.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Polynomial Effects</span>"
    ]
  },
  {
    "objectID": "03-01-polynomial-effects.html#interactions-with-polynomial-terms",
    "href": "03-01-polynomial-effects.html#interactions-with-polynomial-terms",
    "title": "8  Polynomial Effects",
    "section": "8.6 Interactions with Polynomial Terms",
    "text": "8.6 Interactions with Polynomial Terms\nWe can also fit interactions between predictors in polynomial models. In this example, an interaction between median SAT score and sector would indicate that the fitted curves for the public and private institutions are not parallel. In other words, the effect of median SAT score on graduation rates does not have the exact same positive, diminishing effect for both public and private institutions.\nWith polynomial models, there are many ways a potential interaction can play out. In our example, sector can interact with the linear effect of median SAT score or the quadratic effect of median SAT score. Remember that if we include an interaction, we need to include all lower order effects as well. In a polynomial model this means that if we include an interaction with a higher order polynomial term, we also need to include interactions with the lower order polynomial terms.\nFor us this implies that if we want to include an interaction between sector and the quadratic effect of median SAT score, we also need to include the interaction between sector and the linear effect of median SAT score. Because of this there are two potential interaction models we can fit.\n\n# Interaction between sector and linear effect of SAT\nlm.4 = lm(grad ~ 1 + sat + I(sat^2) + public + public:sat, data = mn)\n\n# Interaction between sector and linear and quadratic effects of SAT\nlm.5 = lm(grad ~ 1 + sat + I(sat^2) + public + public:sat + public:I(sat^2), data = mn)\n\nWe now have three candidate models describing the effects of median SAT scores and sector to graduation rates. In increasing levels of complexity these are:\n\nMain effects model (Model 3);\nInteraction effect between linear SAT term and sector (Model 4); and\nInteraction effect between quadratic SAT term and sector AND linear SAT term and sector (Model 5)\n\nSince each of these models is nested in the subsequent model, we can compare them using a set of likelihood ratio tests.\n\n# Likelihood ratio tests\nlrtest(lm.3, lm.4, lm.5)\n\nLikelihood ratio test\n\nModel 1: grad ~ 1 + sat + I(sat^2) + public\nModel 2: grad ~ 1 + sat + I(sat^2) + public + public:sat\nModel 3: grad ~ 1 + sat + I(sat^2) + public + public:sat + public:I(sat^2)\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)  \n1   5 -101.80                       \n2   6 -100.28  1 3.0505    0.08071 .\n3   7 -100.06  1 0.4459    0.50431  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nComparing Model 3 to Model 4, we find a modicum of empirical support for including the interaction between sector and the linear effect of SAT; \\(LR=4.57\\), \\(\\chi^2(1)=3.05\\), \\(p=.081\\). However, there is not empirical support for also including the interaction between sector and the quadratic effect of SAT; \\(LR=1.25\\), \\(\\chi^2(1)=0.44\\), \\(p=.504\\).\nWhether you ultimately adopt Model 3 or Model 4 depends on your research approach and the decisions you made about the the level of evidence you would need to adopt one model over another. Here, since we have taken an exploratory approach to the analysis, we will adopt Model 4, but will acknowledge that the there is uncertainty in this model selection.\n\n\n8.6.1 Summarizing the Adopted Interaction Model\nAgain, we will obtain model- and coefficient-level output and use those to summarize the results of the fitted model.\n\n# Model-level output\nglance(lm.4) |&gt;\n  print(width = Inf)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.906         0.893  5.48      67.5 5.71e-14     4  -100.  213.  222.\n  deviance df.residual  nobs\n     &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1     842.          28    33\n\n\nModel 4 explains 90.6% of the variation in graduation rates, an increase of 0.9 percentage points from Model 3. The residual standard error for Model 4, \\(\\hat\\sigma_{\\epsilon}=5.64\\), has also decreased from that for Model 3 indicating that this model has less error than Model 3.\nThe coefficient-level output is:\n\n# Coefficients\ntidy(lm.4) |&gt;\n  select(-statistic, -p.value)\n\n# A tibble: 5 × 3\n  term        estimate std.error\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -414.      79.2  \n2 sat            71.7     13.8  \n3 I(sat^2)       -2.54     0.598\n4 public         35.1     26.9  \n5 sat:public     -4.11     2.50 \n\n\nWe can use this to obtain the fitted equation:\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} = -&413.80 + 71.65(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i) + \\\\\n&35.07(\\mathrm{Public}_i) - 4.11(\\mathrm{SAT}_i)(\\mathrm{Public}_i)\n\\end{split}\n\\]\nThe interaction tells us that the effect of SAT on graduation rate differs for public and private institutions. Rather than parsing this difference by trying to interpret each of the effects, we will again plot the fitted curves for private and public institutions, and use the plot to aid our interpretation.\nPrivate Institutions\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} &= -413.80 + 71.65(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i) + 35.07(0) - 4.11(\\mathrm{SAT}_i)(0) \\\\\n&= -413.80 + 71.65(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i)\n\\end{split}\n\\]\nPublic Institutions\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} &= -413.80 + 71.65(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i) + 35.07(1) - 4.11(\\mathrm{SAT}_i)(1) \\\\\n&= -378.73 + 67.54(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i)\n\\end{split}\n\\]\nComparing these two fitted equations, we see that not only is the intercept in the two fitted models different (due to the main effect of sector), but now the coefficient for the interaction between sector and the linear effect of SAT also differs; it is lower for public institutions (by 4.11 units) than for private institutions. The quadratic effect of SAT is the same for both public and private institutions. (If we had adopted the model with both interaction terms, the quadratic effect would also be different across sectors.)\nTo visualize the effect of SAT, we can again plot each fitted curve by including each in a separate geom_function() layer.\n\n# Plot of the fitted model\nggplot(data = mn, aes(x = sat, y = grad)) +\n    geom_point(alpha = 0) +\n  # Private institutions\n  geom_function(\n    fun = function(x) {-413.80 + 71.65 * x - 2.54 * x^2},\n    color = \"#ff9f1c\",\n    linetype = \"solid\"\n    ) +\n  # Public institutions\n    geom_function(\n      fun = function(x) {-378.73 + 67.54 * x - 2.54 * x^2},\n      color = \"#2ec4b6\",\n      linetype = \"dashed\"\n      ) +\n  xlab(\"Estimated median SAT score (in hundreds)\") +\n  ylab(\"Six-year graduation rate\") +\n    theme_light() +\n  theme(\n    text = element_text(size = 20)\n  ) \n\n\n\n\n\n\n\nFigure 8.6: Six-year graduation rate as a function of median SAT score for private (orange, solid line) and public (blue, dashed line) institutions.\n\n\n\n\n\nThe curvature (linear and quadratic slopes) of the lines now looks different for the two sectors because of the interaction term. (Note that interactions with the linear effect of SAT or the quadratic effect of SAT contribute to a change in the curvature.) For both sectors, the effect of median SAT on graduation rates is positive (institutions with higher median SAT scores tend to have higher graduation rates. But, this effect diminishes for institutions with increasingly higher SAT scores. Private schools have higher graduation rates, on average, than public schools for all levels of median SAT score. Moreover, this difference in graduation rates is getting larger at higher levels of SAT (that is the interaction!).",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Polynomial Effects</span>"
    ]
  },
  {
    "objectID": "03-01-polynomial-effects.html#presenting-results-from-the-analysis",
    "href": "03-01-polynomial-effects.html#presenting-results-from-the-analysis",
    "title": "8  Polynomial Effects",
    "section": "8.7 Presenting Results from the Analysis",
    "text": "8.7 Presenting Results from the Analysis\nIt is important to indicate the set of candidate models considered in the model adoption process and also present the statistical evidence used to select among them. However, there is no one preferred way to do this. For example, some educational scientists present this in the prose (text), while others may present this information in tables. Here, is one possible method of presenting the models and the evidence from the set of likelihood ratio tests we used in the analysis.\nWe fitted the following set of five candidate models:\n\\[\n\\begin{split}\n\\mathbf{Model~1:~}\\mathrm{Graduation~Rate}_i &= \\beta_0 + \\beta_1(\\mathrm{SAT}_i) + \\epsilon_i \\\\[1ex]\n\\mathbf{Model~2:~}\\mathrm{Graduation~Rate}_i &= \\beta_0 + \\beta_1(\\mathrm{SAT}_i) + \\beta_2(\\mathrm{SAT}^2_i) + \\epsilon_i \\\\[1ex]\n\\mathbf{Model~3:~}\\mathrm{Graduation~Rate}_i &= \\beta_0 + \\beta_1(\\mathrm{SAT}_i) + \\beta_2(\\mathrm{SAT}^2_i) + \\beta_3(\\mathrm{Public}_i) + \\epsilon_i \\\\[1ex]\n\\mathbf{Model~4:~}\\mathrm{Graduation~Rate}_i &= \\beta_0 + \\beta_1(\\mathrm{SAT}_i) + \\beta_2(\\mathrm{SAT}^2_i) + \\beta_3(\\mathrm{Public}_i) + \\\\\n&~~~~~\\beta_4(\\mathrm{SAT}_i)(\\mathrm{Public}_i) + \\epsilon_i \\\\[1ex]\n\\mathbf{Model~5:~}\\mathrm{Graduation~Rate}_i &= \\beta_0 + \\beta_1(\\mathrm{SAT}_i) + \\beta_2(\\mathrm{SAT}^2_i) + \\beta_3(\\mathrm{Public}_i) +\\\\\n&~~~~~\\beta_4(\\mathrm{SAT}_i)(\\mathrm{Public}_i) + \\beta_5(\\mathrm{SAT}^2_i)(\\mathrm{Public}_i) + \\epsilon_i \\\\[1ex]\n\\end{split}\n\\]\nTo evaluate these candidate models, likelihood ratio tests (LRTs) were used to compare each subsequently more complex model to the previous candidate model (e.g., Model 2 was compared to Model 1, Model 3 was compared to Model 2). More complex models were adopted if the p-value for the LRT was below a threshold of .10; a more liberal criterion than .05 adopted due to the exploratory nature of the analysis. The results of these tests are presented in the table below.\n\n\nCode\n# Load library\nlibrary(gt)\n\n# Create data frame of model results\nmy_models = data.frame(\n  model = c(\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\", \"Model 5\"),\n  df = c(3, 4, 5, 6, 7),\n  log_lik = c(-113.55, -109.56, -101.80, -100.28, -100.06),\n  lr = c(NA, 54.05, 2344.90, 4.57, 1.25),\n  lr_test = c(NA, \"$$\\\\chi^2(1)=7.98,~p=.005$$\", \"$$\\\\chi^2(1)=15.51,~p&lt;.001$$\", \"$$\\\\chi^2(1)=3.05,~p=.081$$\", \"$$\\\\chi^2(1)=0.45,~p=.504$$\")\n)\n\n# Create table\nmy_models |&gt;\n  gt() |&gt;\n  cols_label(\n    model = md(\"*Model*\"),\n    df = md(\"*df*\"),\n    log_lik = md(\"*Log-likelihood*\"),\n    lr = md(\"*LR*\"),\n    lr_test = md(\"*LRT*\")\n  ) |&gt;\n  cols_align(\n    columns = c(model),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(df, log_lik, lr, lr_test),\n    align = \"center\"\n  ) |&gt;\n  fmt_missing(\n  missing_text = \"\"\n)\n\n\n\n\nTable 8.1: Results from a set of likelihood ratio tests (LRT) to compare a set of nested candidate models. Models are compared to the model in the previous row.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\ndf\nLog-likelihood\nLR\nLRT\n\n\n\n\nModel 1\n3\n-113.55\n\n\n\n\n\n\nModel 2\n4\n-109.56\n54.05\n$$\\chi^2(1)=7.98,~p=.005$$\n\n\nModel 3\n5\n-101.80\n2344.90\n$$\\chi^2(1)=15.51,~p&lt;.001$$\n\n\nModel 4\n6\n-100.28\n4.57\n$$\\chi^2(1)=3.05,~p=.081$$\n\n\nModel 5\n7\n-100.06\n1.25\n$$\\chi^2(1)=0.45,~p=.504$$\n\n\n\n\n\n\n\n\n\n\nYou should also include the estimates and any summary measures from the model(s) you adopt. Here we would for sure want to report the estimates and summaries for Model 4. Since the number of candidate models here is small (only five models) I would probably present the estimates and summaries from all five candidate models.\n\nCode\n# Load library\nlibrary(texreg)\n\n# Create the table\nhtmlreg(\n  l = list(lm.1, lm.2, lm.3, lm.4, lm.5),\n  stars = numeric(0),    #No p-value stars\n  digits = 2,\n  padding = 20,          #Add space around columns (you may need to adjust this via trial-and-error)\n  include.adjrs = FALSE, #Omit Adjusted R^2\n  include.nobs = FALSE,  #Omit sample size\n  include.rmse = TRUE,   #Include RMSE\n  custom.model.names = c(\"M1\", \"M2\", \"M3\", \"M4\", \"M5\"),\n  custom.coef.names = c(\"Intercept\", \"Median SAT score (L)\", \"Median SAT score (Q)\",\n                        \"Public\", \"Median SAT score (L) x Public\",\n                        \"Median SAT score (Q) x Public\"),\n  custom.note = \"Note. (L) = Linear effect. (Q) = Quadratic effect.\",\n  reorder.coef = c(2:6, 1), #Put intercept at bottom of table\n  caption.above = TRUE, #Move caption above table\n  inner.rules = 1, #Include line rule before model-level output\n  outer.rules = 1,  #Include line rules around table\n  caption = NULL #\"Table 2: Coefficients (and standard errors) for five candidate models (M1--M5) predicting variation in six-year graduation rates.\"\n  )\n\n\n\n\nTable 8.2: Coefficients (and standard errors) for five candidate models (M1–M5) predicting variation in six-year graduation rates.\n\n\n\n\n\n\n \n\n\nM1\n\n\nM2\n\n\nM3\n\n\nM4\n\n\nM5\n\n\n\n\n\n\nMedian SAT score (L)\n\n\n13.35\n\n\n62.72\n\n\n67.04\n\n\n71.65\n\n\n73.12\n\n\n\n\n \n\n\n(1.24)\n\n\n(17.27)\n\n\n(13.93)\n\n\n(13.82)\n\n\n(14.19)\n\n\n\n\nMedian SAT score (Q)\n\n\n \n\n\n-2.15\n\n\n-2.37\n\n\n-2.54\n\n\n-2.61\n\n\n\n\n \n\n\n \n\n\n(0.75)\n\n\n(0.61)\n\n\n(0.60)\n\n\n(0.61)\n\n\n\n\nPublic\n\n\n \n\n\n \n\n\n-9.12\n\n\n35.07\n\n\n304.22\n\n\n\n\n \n\n\n \n\n\n \n\n\n(2.19)\n\n\n(26.92)\n\n\n(444.95)\n\n\n\n\nMedian SAT score (L) x Public\n\n\n \n\n\n \n\n\n \n\n\n-4.11\n\n\n-52.64\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(2.50)\n\n\n(80.12)\n\n\n\n\nMedian SAT score (Q) x Public\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n2.17\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(3.59)\n\n\n\n\nIntercept\n\n\n-86.08\n\n\n-366.34\n\n\n-384.16\n\n\n-413.80\n\n\n-422.16\n\n\n\n\n \n\n\n(13.68)\n\n\n(98.62)\n\n\n(79.41)\n\n\n(79.24)\n\n\n(81.33)\n\n\n\n\nR2\n\n\n0.79\n\n\n0.84\n\n\n0.90\n\n\n0.91\n\n\n0.91\n\n\n\n\nRMSE\n\n\n7.79\n\n\n7.02\n\n\n5.64\n\n\n5.48\n\n\n5.55\n\n\n\n\n\n\nNote. (L) = Linear effect. (Q) = Quadratic effect.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Polynomial Effects</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html",
    "href": "03-02-information-criteria-and-model-selection.html",
    "title": "9  Information Criteria and Model Selection",
    "section": "",
    "text": "9.1 Working Hypotheses and Candidate Models\nIn this chapter, you will use information theoretic approaches (e.g., information criteria) to select one (or more) empirically supported model from a set of candidate models. To do so, we will use the mn-schools.csv dataset (see the data codebook) to examine if (and how) academic “quality” of the student-body (measured by SAT score) is related to institutional graduation rate.\nWe will re-visit the following research questions:\nLet’s begin by considering three potential models that explain variation in graduation rates:\n\\[\n\\begin{split}\n\\mathbf{Model~0:~} \\quad \\mathrm{Graduation~Rate}_i = &\\beta_0 + \\epsilon_i \\\\[2ex]\n\\mathbf{Model~1:~} \\quad \\mathrm{Graduation~Rate}_i = &\\beta_0 + \\beta_1(\\mathrm{Median~SAT}_i) + \\epsilon_i \\\\[2ex]\n\\mathbf{Model~2:~} \\quad \\mathrm{Graduation~Rate}_i = &\\beta_0 + \\beta_1(\\mathrm{Median~SAT}_i) + \\beta_2(\\mathrm{Median~SAT}^2_i) + \\epsilon_i \\\\\n\\end{split}\n\\]\nwhere all three models have \\(\\epsilon_i\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\).\n# Fit candidate models\nlm.0 = lm(grad ~ 1, data = mn)\nlm.1 = lm(grad ~ 1 + sat, data = mn)\nlm.2 = lm(grad ~ 1 + sat + I(sat^2), data = mn)\nEach of these models correspond to a different scientific working hypothesis about what explains institutional differences in graduation rates:\nOne method for choosing among a set of candidate models is to pick based on the residuals. In this method, we adopt the model for which the data best meet the assumptions, and employing the principle of parsimony—adopting the more parsimonious model when there are multiple models that produce equally “good” residuals. Additionally, we can use the likelihood ratio test (LRT) to help make this decision. This method provides additional empirical evidence about which model is supported when the candidate models are nested.\nUnfortunately, we cannot always use the LRT to select models, since there are many times when the set of candidate models are not all nested. Information criteria give us metrics to compare the empirical support for models whether they are nested or not. In the remainder of these notes, we will examine several of the more popular information criteria metrics used for model selection.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#working-hypotheses-and-candidate-models",
    "href": "03-02-information-criteria-and-model-selection.html#working-hypotheses-and-candidate-models",
    "title": "9  Information Criteria and Model Selection",
    "section": "",
    "text": "Model 0 represents the working hypothesis (H0:) that graduation rates are not a function of anything systematic and are just a fucntion of the individual institution.\nModel 1 represents the working hypothesis (H1:) that graduation rates are a linear function of median SAT scores.\nModel 2 represents the working hypothesis (H2:) that graduation rates are a non-linear function of median SAT scores.\n\n\nThese working hypotheses are typically created from the theory and previous empirical work in a substantive area. They need to be translated into a statistical models, which can be quite difficult, especially if there is not a lot of theory to guide this translation.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#akiakes-information-criteria-aic",
    "href": "03-02-information-criteria-and-model-selection.html#akiakes-information-criteria-aic",
    "title": "9  Information Criteria and Model Selection",
    "section": "9.2 Akiake’s Information Criteria (AIC)",
    "text": "9.2 Akiake’s Information Criteria (AIC)\nThe AIC metric is calculated by adding a penalty term to the model’s deviance (\\(-2\\ln(\\mathrm{Likelihood})\\)).\n\\[\n\\begin{split}\nAIC &= \\mathrm{Deviance} + 2k \\\\[1ex]\n&= -2\\ln(\\mathcal{L}) + 2k\n\\end{split}\n\\]\nwhere k is the number of parameters (including the residual standard error) being estimated in the model. (Recall that the value for k is given as df in the logLik() output.)\nRemember that deviance is similar to error (it measures model-data misfit), and so models that have lower values of deviance are more empirically supported. The problem with deviance, however, is that more complex models tend to have smaller deviance values and are, therefore, more supported than their simpler counterparts. Unfortunately, these complex models tend not to generalize as well as simpler models (this is called overfitting). The AIC metric’s penalty term penalizes the deviance more when a more complex model is fitted; it is offsetting the lower degree of model-data misfit in complex models by increasing the ‘misfit’ based on the number of parameters.\nThis penalty-adjusted measure of ‘misfit’ is called Akiake’s Information Criteria (AIC). We compute the AIC for each of the candidate models and the model with the lowest AIC is selected. This model, we say, is the candidate model with the most empirical support given the data and the set of candidate models evaluated. Below we compute the AIC for the first candidate model.\n\n# Compute AIC for Model 1\n# logLik(lm.1)\n-2*-113.5472 + 2*3\n\n[1] 233.0944\n\n\nWe could also use the AIC() function to compute the AIC value directly. Here we compute the AIC for each of the candidate models.\n\n# Model 0\nAIC(lm.0)\n\n[1] 282.5998\n\n# Model 1\nAIC(lm.1)\n\n[1] 233.0944\n\n# Model 2\nAIC(lm.2)\n\n[1] 227.1166\n\n\nBased on the AIC values, the candidate model with the most empirical evidence, given the data and the set of candidate models evaluated, is Model 2; it has the lowest AIC.\nLastly, we note that the AIC value is produced as a column in the model-level output from the glance() function. (Note that the df column from glance() does NOT give the number of model parameters.)\n\n# Model-level output for H2\nglance(lm.2) |&gt;\n  print(width = Inf)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.835         0.824  7.02      76.0 1.81e-12     2  -110.  227.  233.\n  deviance df.residual  nobs\n     &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1    1478.          30    33\n\n\n\n\n9.2.1 Empirical Support is for the Working Hypotheses\nBecause the models are proxies for the scientific working hypotheses, the AIC ends up being a measure of empirical support for any particular working hypothesis. Using the AIC, we can rank order the models (and subsequently the working hypotheses) based on their level of empirical support. Ranked in order of empirical support, the three scientific working hypotheses are:\n\nH2: Graduation rates are a non-linear function of median SAT scores.\nH1: Graduation rates are a linear function of median SAT scores.\nH0: Graduation rates are not a function of anything systematic and are just a fucntion of the individual institution.\n\nIt is important to remember that the phrase given the data and the set of candidate models evaluated is highly important. The ranking of models/working hypotheses is a relative ranking of the models’ level of empirical support contingent on the candidate models we included in the comparison and the data we used to compute the AIC.\nAs such, this method is not able to rank any hypotheses that you did not consider as part of the candidate set of scientific working hypotheses. Moreover, the AIC is a direct function of the likelihood which is based on the actual model fitted as a proxy for the scientific working hypothesis. If the predictors used in any of the models had been different, it would lead to different likelihood and AIC values, and potentially a different rank ordering of the hypotheses.\nThe ranking of models is also based on the data we have. If we had different ranges of the data or a different set of variables, the evidence might support a different model. This is very important. Model 2 is the most empirically supported candidate model GIVEN the three candidate models we compared and the data we used to compute the AIC metric.\n\nThe model selection and ranking is contingent on both the set of candidate models you are evaluating, and the data you have.\n\nBased on the AIC values for the three candidate models we ranked the hypotheses based on the amount of empirical support:\n\n\nCode\ncand_models = data.frame(\n  Model = c(\"Model 2\", \"Model 1\", \"Model 0\"),\n  k = c(4, 3, 2),\n  AIC = c(AIC(lm.2), AIC(lm.1), AIC(lm.0))\n) \n\ncand_models |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(Model),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(k, AIC),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    Model = md(\"*Model*\"),\n    k = md(\"*k*\"),\n    AIC = md(\"*AIC*\")\n  ) |&gt;\n  tab_options(\n    table.width = pct(40)\n  )\n\n\n\n\nTable 9.1: Models rank-ordered by the amount of empirical support as measured by the AIC.\n\n\n\n\n\n\n\n\n\nModel\nk\nAIC\n\n\n\n\nModel 2\n4\n227.1166\n\n\nModel 1\n3\n233.0944\n\n\nModel 0\n2\n282.5998",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#corrected-aic-aicc-adjusting-for-model-complexity-and-sample-size",
    "href": "03-02-information-criteria-and-model-selection.html#corrected-aic-aicc-adjusting-for-model-complexity-and-sample-size",
    "title": "9  Information Criteria and Model Selection",
    "section": "9.3 Corrected AIC (AICc): Adjusting for Model Complexity and Sample Size",
    "text": "9.3 Corrected AIC (AICc): Adjusting for Model Complexity and Sample Size\nAlthough AIC has a penalty correction that should account for model complexity, it turns out that when the number of parameters is large relative to the sample size, AIC is still biased in favor of models that have more parameters. This led Hurvich & Tsai (1989) to propose a second-order bias corrected AIC measure (AICc) computed as:\n\\[\n\\mathrm{AIC_c} = \\mathrm{Deviance} + 2k\\left( \\frac{n}{n - k - 1} \\right)\n\\]\nwhere k is, again, the number of estimated parameters (including residual standard error), and n is the sample size used to fit the model. Note that when n is very large (especially relative to k) that the last term is essentially 1 and the AICc value would basically reduce to the AIC value. When n is small relative to k this will add more of a penalty to the deviance.\n\nThe statistical recommendation is to pretty much always use AICc rather than AIC when selecting models.\n\nBelow, we will compute the AICc for the Model 2. (Note that we use \\(n=33\\) cases for the computation for all the models in this data.)\n\nn = 33\nk = 4\n\n# Compute AICc for linear hypothesis\n-2 * logLik(lm.2)[[1]] + 2 * k * (n / (n - k - 1))\n\n[1] 228.5452\n\n\nIn practice, we will use the AICc() function from the {AICcmodavg} package to compute the AICc value directly. Here we compute the AICc for the four candidate models.\n\n# Model 0\nAICc(lm.0)\n\n[1] 282.9998\n\n# Model 1\nAICc(lm.1)\n\n[1] 233.922\n\n# Model 2\nAICc(lm.2)\n\n[1] 228.5452\n\n\nBased on the AICc values, the model with the most empirical support given the data and three candidate models is, again, Model 2. Using these results, we can, again, rank order the models (which correspond to working hypotheses) based on the empirical support for each.\n\n\nCode\ncand_models = data.frame(\n  Model = c(\"Model 2\", \"Model 1\", \"Model 0\"),\n  k = c(4, 3, 2),\n  AIC = c(AIC(lm.2), AIC(lm.1), AIC(lm.0)),\n  AICc = c(AICc(lm.2), AICc(lm.1), AICc(lm.0))\n)\n\n\ncand_models |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(Model),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(k, AICc),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    Model = md(\"*Model*\"),\n    k = md(\"*k*\"),\n    AICc = md(\"*AICc*\")\n  ) |&gt;\n  tab_options(\n    table.width = pct(40)\n  )\n\n\n\n\nTable 9.2: Models rank-ordered by the amount of empirical support as measured by the AICc.\n\n\n\n\n\n\n\n\n\nModel\nk\nAIC\nAICc\n\n\n\n\nModel 2\n4\n227.1166\n228.5452\n\n\nModel 1\n3\n233.0944\n233.9220\n\n\nModel 0\n2\n282.5998\n282.9998",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#quantifying-model-selection-uncertainty",
    "href": "03-02-information-criteria-and-model-selection.html#quantifying-model-selection-uncertainty",
    "title": "9  Information Criteria and Model Selection",
    "section": "9.4 Quantifying Model-Selection Uncertainty",
    "text": "9.4 Quantifying Model-Selection Uncertainty\nWhen we adopt one model over another, we are introducing some degree of uncertainty into the scientific process, in this case model selection uncertainty. This is true whether you are selecting models based on p-values or using information criteria. It would be nice if we can quantify and report this uncertainty, and this is the real advantage of using information criteria for model selection; it allows us to quantify the uncertainty we have when we select any particular candidate model.\nThe amount of model selection uncertainty we have depends on the amount of empirical support each of the candidate models has. For example, if one particular candidate model has a lot of empirical support and the rest have very little empirical support we would have less model selection uncertainty than if all of the candidate models had about the same amount of empirical support.\nSince we quantify the empirical support for each model/working hypothesis by computing the AICc, we can also quantify how much more empirical support the most supported hypothesis has relative to each of the other working hypotheses by computing the difference in AICc values between the model with the most empirical support and each of the other candidate models. This measure is referred to as \\(\\Delta\\)AICc.\nIn our example, the hypothesis with the most empirical support was Model 2. The \\(\\Delta\\)AICc values can then be computed for each candidate model by subtracting the AICc for Model 2 from the AICc for that candidate model.\n\n# Compute delta AICc value for Model 0\nAICc(lm.0) - AICc(lm.2)\n\n[1] 54.45462\n\n# Compute delta AICc value for Model 1\nAICc(lm.1) - AICc(lm.2)\n\n[1] 5.37686\n\n# Compute delta AICc value for Model 2\nAICc(lm.2) - AICc(lm.2)\n\n[1] 0\n\n\n\n\nCode\ncand_models = cand_models |&gt;\n  mutate(\n    delta_a = AICc - AICc(lm.2)\n  )\n\ncand_models |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(Model),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(k, AICc, delta_a),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    Model = md(\"*Model*\"),\n    k = md(\"*k*\"),\n    AICc = md(\"*AICc*\"),\n    delta_a = html(\"&#916;AICc\")\n  ) |&gt;\n  tab_options(\n    table.width = pct(50)\n  )\n\n\n\n\nTable 9.3: Models rank-ordered by the amount of empirical support as measured by the AICc. Other evidence includes the ΔAICc for each model.\n\n\n\n\n\n\n\n\n\nModel\nk\nAIC\nAICc\nΔAICc\n\n\n\n\nModel 2\n4\n227.1166\n228.5452\n0.00000\n\n\nModel 1\n3\n233.0944\n233.9220\n5.37686\n\n\nModel 0\n2\n282.5998\n282.9998\n54.45462\n\n\n\n\n\n\n\n\n\n\nBurnham et al. (2011, p. 25) give rough guidelines for interpreting \\(\\Delta\\)AICc values. They suggest that hypotheses with \\(\\Delta\\)AICc values less than 2 are plausible, those in the range of 4–7 have some empirical support, those in the range of 9–11 have relatively little support, and those greater than 13 have essentially no empirical support. Using these criteria:\n\nModel 2 is plausible.\nModel 1 has some empirical support.\nModel 0 has virtually no empirical support.\n\nThis implies that we would have a fair amount of certainty actually selecting Model 1 and 2 over the Model 0. There is a little uncertainty adopting Model 2 over Model 1. When multiple models are supported by the empirical evidence it suggests that more than one model would need to be adopted since the empirical evidence wouldn’t really differentiate which model is “better”.\nIn our example, we can safely say that there does seem to be an effect of median SAT score on graduation rates. (This answers our first research question!) We are still a little uncertain whether that effect is linear or non-linear, but the evidence is pointing in the direction of non-linearity.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#relative-likelihood-and-evidence-ratios",
    "href": "03-02-information-criteria-and-model-selection.html#relative-likelihood-and-evidence-ratios",
    "title": "9  Information Criteria and Model Selection",
    "section": "9.5 Relative Likelihood and Evidence Ratios",
    "text": "9.5 Relative Likelihood and Evidence Ratios\nOne way we can mathematically formalize the strength of evidence for each model is to compute the relative likelihood. The relative likelihood provides the likelihood of each of the candidate models, given the set of candidate models and the data. To compute the relative likelihood, we use:\n\\[\n\\mathrm{Relative~Likelihood} = e ^ {−\\frac{1}{2} (\\Delta AICc)}\n\\]\n\n# Relative likelihood for Model 0\nexp(-1/2 * 54.45462)\n\n[1] 1.497371e-12\n\n# Relative likelihood for Model 1\nexp(-1/2 * 5.37686)\n\n[1] 0.0679876\n\n# Relative likelihood for Model 2\nexp(-1/2 * 0)\n\n[1] 1\n\n\n\n\nCode\ncand_models = cand_models |&gt;\n  mutate(\n    rel_lik = exp(-1/2 * delta_a)\n  )\n\ncand_models |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(Model),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(k, AICc, delta_a, rel_lik),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    Model = md(\"*Model*\"),\n    k = md(\"*k*\"),\n    AICc = md(\"*AICc*\"),\n    delta_a = html(\"&#916;AICc\"),\n    rel_lik = html(\"Rel(&#8466;)\")\n  ) |&gt;\n  tab_options(\n    table.width = pct(70)\n  ) |&gt;\n  tab_footnote(\n    footnote = html(\"Rel(&#8466;) = Relative likelihood. Values are displayed using scientific notation.\"),\n    locations = cells_column_labels(columns = rel_lik)\n  )\n\n\n\n\nTable 9.4: Models rank-ordered by the amount of empirical support as measured by the AICc. Other evidence includes the ΔAICc, and relative likelihood for each model.\n\n\n\n\n\n\n\n\n\nModel\nk\nAIC\nAICc\nΔAICc\nRel(ℒ)1\n\n\n\n\nModel 2\n4\n227.1166\n228.5452\n0.00000\n1.000000e+00\n\n\nModel 1\n3\n233.0944\n233.9220\n5.37686\n6.798761e-02\n\n\nModel 0\n2\n282.5998\n282.9998\n54.45462\n1.497373e-12\n\n\n\n1 Rel(ℒ) = Relative likelihood. Values are displayed using scientific notation.\n\n\n\n\n\n\n\n\n\n\n\nThe relative likelihood values allow us to compute evidence ratios, which are evidentiary statements for comparing any two model or scientific hypotheses. Evidence ratios quantify how much more empirical support one model/hypothesis has versus another. To obtain an evidence ratio, we divide the relative likelihood for any two models/hypotheses. As an example, the evidence ratio comparing Model 2 to Model 0 is computed as:\n\n# ER comparing Model 2 to Model 0\n1 / 1.497373e-12\n\n[1] 667836270589\n\n\nThat is,\n\nThe empirical support for Model 2 is 667,836,270,589 (667 billion) times that of the empirical support for Model 0.\n\nWe can similarly compute the evidence ratio to make an evidentiary statement comparing the empirical support for any two models. For example to compare the empirical support for Model 2 versus Model 1:\n\n# ER comparing Model 2 to Model 1\n1 / 6.798761e-02\n\n[1] 14.70856\n\n\n\nThe empirical support for Model 2 is 14.71 times that of the empirical support for Model 1. This gives us a more quantitative lens to view the the empirical evidence for the non-linear vs the linear effect of median SAT score on graduation rates. While there is unceratinty in the model selection, knowing that the empirical support for the non-linear effect is almost 15 times that of the empirical support for the linear effect helps us think about that model selection uncertainty a bit differently than just saying there is some support for Model 1.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#model-probabilities",
    "href": "03-02-information-criteria-and-model-selection.html#model-probabilities",
    "title": "9  Information Criteria and Model Selection",
    "section": "9.6 Model Probabilities",
    "text": "9.6 Model Probabilities\nAnother method of quantifying the relative empirical support for each working hypothesis is to compute the model probability for each of the candidate models. Also referred to as an Akaike Weight (\\(w_i\\)), a model probability provides a numerical measure of the probability of each model given the data and the candidate set of models. It can be computed as:\n\\[\nw_i = \\frac{\\mathrm{Relative~Likelihood~for~Model~J}}{\\sum_j \\mathrm{All~Relative~Likelihoods}}\n\\]\nHere we compute the model probability for all three candidate models.\n\n# Compute sum of relative likelihoods\nsum_rel = 1.497373e-12 + 6.798761e-02 + 1\n\n# Model probability for Model 0\n1.497373e-12 / sum_rel\n\n[1] 1.402051e-12\n\n# Model probability for Model 1\n6.798761e-02 / sum_rel\n\n[1] 0.06365955\n\n# Model probability for Model 2\n1 / sum_rel\n\n[1] 0.9363405\n\n\nWe also add these to our table of model evidence.\n\n\nCode\ncand_models = cand_models |&gt;\n  mutate(\n    model_prob = rel_lik / sum(rel_lik)\n  )\n\ncand_models |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(Model),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(k, AICc, delta_a, rel_lik, model_prob),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    Model = md(\"*Model*\"),\n    k = md(\"*k*\"),\n    AICc = md(\"*AICc*\"),\n    delta_a = html(\"&#916;AICc\"),\n    rel_lik = html(\"Rel(&#8466;)\"),\n    model_prob = md(\"*AICc Wt.*\")\n  ) |&gt;\n  tab_options(\n    table.width = pct(75)\n  ) |&gt;\n  tab_footnote(\n    footnote = html(\"Rel(&#8466;) = Relative likelihood. Values are displayed using scientific notation.\"),\n    locations = cells_column_labels(columns = rel_lik)\n  )\n\n\n\n\nTable 9.5: Models rank-ordered by the amount of empirical support as measured by the AICc. Other evidence includes the ΔAICc, relative likelihood, and model probability (AICc weight) for each model.\n\n\n\n\n\n\n\n\n\nModel\nk\nAIC\nAICc\nΔAICc\nRel(ℒ)1\nAICc Wt.\n\n\n\n\nModel 2\n4\n227.1166\n228.5452\n0.00000\n1.000000e+00\n9.363405e-01\n\n\nModel 1\n3\n233.0944\n233.9220\n5.37686\n6.798761e-02\n6.365955e-02\n\n\nModel 0\n2\n282.5998\n282.9998\n54.45462\n1.497373e-12\n1.402051e-12\n\n\n\n1 Rel(ℒ) = Relative likelihood. Values are displayed using scientific notation.\n\n\n\n\n\n\n\n\n\n\n\nSince the models are proxies for the working hypotheses, the model probabilities can be used to provide probabilities of each working hypothesis as a function of the empirical support. Given the data and the candidate set of working hypotheses:\n\nThe probability of the zeroth working scientific hypothesis is 0.0000000000014.\nThe probability of the first working scientific hypothesis is 0.064.\nThe probability of the second working scientific hypothesis is 0.936.\n\nThis suggests that the second working scientific hypothesis is the most probable. There is a tiny bit of e,mpirical support for Hypothesis 1, and essentially no support for Hypotheses 0 (it has some non-negligible probability). Again, this confirms that we are very certain that there is an effect of median SAT score on graduation rates. And we are pretty convinced that the effect is non-linear.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#answering-rq-3",
    "href": "03-02-information-criteria-and-model-selection.html#answering-rq-3",
    "title": "9  Information Criteria and Model Selection",
    "section": "9.7 Answering RQ 3",
    "text": "9.7 Answering RQ 3\nThe third research question asked about whether the effect of student-body quality on graduation rates persists after controlling for differences in institutional sector. This represents a new scientific hypothesis:\n\nH3: There is an effect of median SAT scores on graduation rates even after accounting for differences in educational sector.\n\nTo evaluate this hypothesis, we need to compare a main-effects model that includes the effect of educational sector AND the effect of median SAT scores to a model that only includes educational sector. Since we decided that the effect of median SAT scores was non-linear in the previous analysis, we need to evaluate the empirical support for the following models:\n\\[\n\\begin{split}\n\\mathbf{Model~3:~} \\quad \\mathrm{Graduation~Rate}_i = &\\beta_0 + \\beta_1(\\mathrm{Public}_i) + \\epsilon_i \\\\[2ex]\n\\mathbf{Model~5:~} \\quad \\mathrm{Graduation~Rate}_i = &\\beta_0 + \\beta_1(\\mathrm{Public}_i) + \\beta_2(\\mathrm{Median~SAT}_i) + \\beta_3(\\mathrm{Median~SAT}^2_i) + \\epsilon_i \\\\\n\\end{split}\n\\]\nIf there was still uncertainty about whether the effect of median SAT scores was non-linear or linear, you could also include a model that included the the effect of educational sector and the linear effect of median SAT scores.\n\\[\n\\mathbf{Model~4:~} \\quad \\mathrm{Graduation~Rate}_i = \\beta_0 + \\beta_1(\\mathrm{Public}_i) + \\beta_2(\\mathrm{Median~SAT}_i) + \\epsilon_i\n\\]\nWe fit these models and evaluate the empirical evidence by computing the AICc, relative likelihood, and model probability for each of the candidate models.\n\n# Fit models\nlm.3 = lm(grad ~ 1 + public, data = mn)\nlm.4 = lm(grad ~ 1 + public + sat, data = mn)\nlm.5 = lm(grad ~ 1 + public + sat + I(sat^2), data = mn)\n\nThe table of model evidence is presented below.\n\n\nCode\ncand_models = data.frame(\n  Model = c(\"Model 5\", \"Model 4\", \"Model 3\"),\n  k = c(5, 4, 3),\n  AICc = c(AICc(lm.5), AICc(lm.4), AICc(lm.3))\n) |&gt;\n  mutate(\n    delta_a = AICc - AICc(lm.5),\n    rel_lik = exp(-1/2 * delta_a),\n    model_prob = rel_lik / sum(rel_lik)\n  )\n\ncand_models |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(Model),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(k, AICc, delta_a, rel_lik, model_prob),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    Model = md(\"*Model*\"),\n    k = md(\"*k*\"),\n    AICc = md(\"*AICc*\"),\n    delta_a = html(\"&#916;AICc\"),\n    rel_lik = html(\"Rel(&#8466;)\"),\n    model_prob = md(\"*AICc Wt.*\")\n  ) |&gt;\n  tab_options(\n    table.width = pct(70)\n  ) |&gt;\n  tab_footnote(\n    footnote = html(\"Rel(&#8466;) = Relative likelihood. Values are displayed using scientific notation.\"),\n    locations = cells_column_labels(columns = rel_lik)\n  )\n\n\n\n\nTable 9.6: Models rank-ordered by the amount of empirical support as measured by the AICc. Other evidence includes the ΔAICc, relative likelihood, and model probability (AICc weight) for each model.\n\n\n\n\n\n\n\n\n\nModel\nk\nAICc\nΔAICc\nRel(ℒ)1\nAICc Wt.\n\n\n\n\nModel 5\n5\n215.8278\n0.00000\n1.000000e+00\n9.963040e-01\n\n\nModel 4\n4\n227.0215\n11.19362\n3.709671e-03\n3.695960e-03\n\n\nModel 3\n3\n279.7700\n63.94213\n1.303595e-14\n1.298777e-14\n\n\n\n1 Rel(ℒ) = Relative likelihood. Values are displayed using scientific notation.\n\n\n\n\n\n\n\n\n\n\n\nGiven the data and the candidate models evaluated, Models 4 (model prob. = 0.996) and 5 (model prob. = 0.003) have more empirical support than Model 4 (model prob. = 0.00000000000001). The empirical evidence supports the persistance of an effect of student-body quality on graduation rates, even after accounting for differences in educational sector. Moreover, the \\(\\Delta\\)AICc values, relative likelihoods, and model probabilities are suggesting overwhelming empirical support for the non-linear effect of student-body quality on graduation rates.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#answering-rq-4",
    "href": "03-02-information-criteria-and-model-selection.html#answering-rq-4",
    "title": "9  Information Criteria and Model Selection",
    "section": "9.8 Answering RQ 4",
    "text": "9.8 Answering RQ 4\nThe fourth research questions asked whether the effect of student-body quality on graduation rates was different for public and private institutions. This again represents a new scientific hypothesis:\n\nH4: There is an effect of median SAT scores on graduation rates, and that effect varies for different educational sectors.\n\nTo evaluate this hypothesis, we need to compare a main-effects model that includes the effect of educational sector AND the effect of median SAT scores to an interaction model that includes the main-effects educational sector and median SAT scores, and the interaction effect between them. Again, for completeness, I will explore a set of models that allow for the linear and non-linear effects of median SAT score. Since we have already fitted the main-effects models (Models 4 and 5), here we only add the interaction models.\n\\[\n\\begin{split}\n\\mathbf{Model~6:~} \\quad \\mathrm{Graduation~Rate}_i = &\\beta_0 + \\beta_1(\\mathrm{Public}_i) + \\beta_2(\\mathrm{Median~SAT}_i) + \\beta_3(\\mathrm{Public}_i)(\\mathrm{Median~SAT}_i) + \\epsilon_i \\\\[2ex]\n\\mathbf{Model~7:~} \\quad \\mathrm{Graduation~Rate}_i = &\\beta_0 + \\beta_1(\\mathrm{Public}_i) + \\beta_2(\\mathrm{Median~SAT}_i) + \\beta_3(\\mathrm{Median~SAT}^2_i) +\\\\\n&\\beta_4(\\mathrm{Public}_i)(\\mathrm{Median~SAT}_i) + \\epsilon_i \\\\[2ex]\n\\mathbf{Model~8:~} \\quad \\mathrm{Graduation~Rate}_i = &\\beta_0 + \\beta_1(\\mathrm{Public}_i) + \\beta_2(\\mathrm{Median~SAT}_i) + \\beta_3(\\mathrm{Median~SAT}^2_i) +\\\\\n&\\beta_4(\\mathrm{Public}_i)(\\mathrm{Median~SAT}_i) + \\beta_5(\\mathrm{Public}_i)(\\mathrm{Median~SAT}^2_i) + \\epsilon_i\n\\end{split}\n\\]\nWe fit these models and evaluate the empirical evidence by computing the AICc, relative likelihood, and model probability for each of the candidate models.\n\n# Fit models\nlm.6 = lm(grad ~ 1 + public + sat + public:sat, data = mn)\nlm.7 = lm(grad ~ 1 + public + sat + I(sat^2) + public:sat, data = mn)\nlm.8 = lm(grad ~ 1 + public + sat + I(sat^2) + public:sat + public:I(sat^2), data = mn)\n\nThe table of model evidence is presented below.\n\n\nCode\ncand_models = data.frame(\n  Model = c(\"Model 7\", \"Model 5\", \"Model 8\", \"Model 4\", \"Model 6\"),\n  k = c(6, 5, 7, 4, 5),\n  AICc = c(AICc(lm.7), AICc(lm.5), AICc(lm.8), AICc(lm.4), AICc(lm.6))\n) |&gt;\n  mutate(\n    delta_a = AICc - AICc(lm.5),\n    rel_lik = exp(-1/2 * delta_a),\n    model_prob = rel_lik / sum(rel_lik)\n  )\n\ncand_models |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(Model),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(k, AICc, delta_a, rel_lik, model_prob),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    Model = md(\"*Model*\"),\n    k = md(\"*k*\"),\n    AICc = md(\"*AICc*\"),\n    delta_a = html(\"&#916;AICc\"),\n    rel_lik = html(\"Rel(&#8466;)\"),\n    model_prob = md(\"*AICc Wt.*\")\n  ) |&gt;\n  tab_options(\n    table.width = pct(70)\n  ) |&gt;\n  tab_footnote(\n    footnote = html(\"Rel(&#8466;) = Relative likelihood. Values are displayed using scientific notation.\"),\n    locations = cells_column_labels(columns = rel_lik)\n  )\n\n\n\n\nTable 9.7: Models evaqluating the interaction effects rank-ordered by the amount of empirical support as measured by the AICc. Other evidence includes the ΔAICc, relative likelihood, and model probability (AICc weight) for each model.\n\n\n\n\n\n\n\n\n\nModel\nk\nAICc\nΔAICc\nRel(ℒ)1\nAICc Wt.\n\n\n\n\nModel 7\n6\n215.7859\n-0.04193024\n1.021186434\n0.4483754119\n\n\nModel 5\n5\n215.8278\n0.00000000\n1.000000000\n0.4390730203\n\n\nModel 8\n7\n218.5893\n2.76143809\n0.251397722\n0.1103819569\n\n\nModel 4\n4\n227.0215\n11.19362410\n0.003709671\n0.0016288165\n\n\nModel 6\n5\n229.2266\n13.39876393\n0.001231673\n0.0005407943\n\n\n\n1 Rel(ℒ) = Relative likelihood. Values are displayed using scientific notation.\n\n\n\n\n\n\n\n\n\n\n\nGiven the data and the candidate models evaluated, Models 7, 5, and 8 are all empirically supported. There is virtually no empirical support for Models 4 and 6. While we can rule out the main-effects and interaction models that are based on the linear effect of median SAT scores on graduation rates, it is unclear that in the models that include the quadratic effect of median SAT score whether or not there is an interaction effect between median SAT scores and educational sector nor whether the interaction is between(i.e., there is a fair amount of model selection uncertainty).\n\nIn practice I would adopt and report all three quadratic models (Models 7, 5, and 8) and mention the model selection uncertainty that makes it hard to rule any of these out. This uncertainty almost certainly is a function of the small sample size.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#using-r-to-create-a-table-of-model-evidence",
    "href": "03-02-information-criteria-and-model-selection.html#using-r-to-create-a-table-of-model-evidence",
    "title": "9  Information Criteria and Model Selection",
    "section": "9.9 Using R to Create a Table of Model Evidence",
    "text": "9.9 Using R to Create a Table of Model Evidence\nWe can use the aictab() function from the {AICcmodavg} package to compute and create a table of model evidence values directly from the lm() fitted models. This function takes a list of models in the candidate set (it actually has to be an R list). The argument modnames= is an optional argument to provide model names that will be used in the output. –&gt;\n\n#Create table of model evidence\nmodel_evidence = aictab(\n  cand.set = list(lm.4, lm.5, lm.6, lm.7, lm.8),\n  modnames = c(\"Model 4\", \"Model 5\", \"Model 6\", \"Model 7\", \"Model 8\")\n  )\n\n# View output\nmodel_evidence\n\n\nModel selection based on AICc:\n\n        K   AICc Delta_AICc AICcWt Cum.Wt      LL\nModel 7 6 215.79       0.00   0.45   0.45 -100.28\nModel 5 5 215.83       0.04   0.44   0.89 -101.80\nModel 8 7 218.59       2.80   0.11   1.00 -100.05\nModel 4 4 227.02      11.24   0.00   1.00 -108.80\nModel 6 5 229.23      13.44   0.00   1.00 -108.50\n\n\nThe model evidence provided for each model includes the number of parameters (K), AICc value (AICc), \\(\\Delta\\)AICc value (Delta_AICc), the Akiake weight/model probability (AICcWt), cumulative weight (Cum.Wt), and log-likelihood (LL). The output also rank orders the models based on the AICc criterion. The models are printed in order from the model with the most empirical evidence (Model 7) to the models with the least amount of empirical evidence (Model 6) based on the AICc.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#evidence-based-on-all-nine-candidate-models",
    "href": "03-02-information-criteria-and-model-selection.html#evidence-based-on-all-nine-candidate-models",
    "title": "9  Information Criteria and Model Selection",
    "section": "9.10 Evidence Based on All Nine Candidate Models",
    "text": "9.10 Evidence Based on All Nine Candidate Models\nAnother approach to the analysis is to include all nine models in the analysis from the start.\n\n#Create table of model evidence\nmodel_evidence = aictab(\n  cand.set = list(lm.0, lm.1, lm.2, lm.3, lm.4, lm.5, lm.6, lm.7, lm.8),\n  modnames = c(\"Model 0\", \"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\", \"Model 5\", \"Model 6\", \"Model 7\", \"Model 8\")\n  )\n\n# View output\nmodel_evidence\n\n\nModel selection based on AICc:\n\n        K   AICc Delta_AICc AICcWt Cum.Wt      LL\nModel 7 6 215.79       0.00   0.45   0.45 -100.28\nModel 5 5 215.83       0.04   0.44   0.89 -101.80\nModel 8 7 218.59       2.80   0.11   1.00 -100.05\nModel 4 4 227.02      11.24   0.00   1.00 -108.80\nModel 2 4 228.55      12.76   0.00   1.00 -109.56\nModel 6 5 229.23      13.44   0.00   1.00 -108.50\nModel 1 3 233.92      18.14   0.00   1.00 -113.55\nModel 3 3 279.77      63.98   0.00   1.00 -136.47\nModel 0 2 283.00      67.21   0.00   1.00 -139.30\n\n\nBased on this table of model evidence we see:\n\nModel 0 and Model 3 have the least amount of empirical support and their model probabilities are both essentially 0. These are the only models that do not include the effect of median SAT scores. This implies that there is likely an effect of median SAT score on graduation rates.\nThe models that include the quadratic effect of median SAT score on graduation rates all have more empirical evidence than their counterpart models (with the same covariates or interactions) that only include the linear effect—Model 2 has more empirical evidence than Model 1; Model 5 has more empirical evidence than Model 4; and Models 7 and 8 have more empirical evidence than Model 6. This implies that the effect of median SAT score on graduation rates is likely non-linear.\nBoth models that include educational sector and the effect of median SAT score on graduation rates (Models 4 and 5) have more empirical evidence than the model that only includes educational sector (Model 3). This implies that the effect of median SAT score on graduation rates persists after accounting for differences in educational sector.\nBased on the empirical evidence there is model selection uncertainty when it comes to choosing between Models 5, 7, and 8. All are plausible given the data and candidate models being evaluated. This implies that we do not have enough empirical evidence to say whether or not there is an interaction effect between educational sector and median SAT scores on graduation rates.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#some-final-thoughts",
    "href": "03-02-information-criteria-and-model-selection.html#some-final-thoughts",
    "title": "9  Information Criteria and Model Selection",
    "section": "9.11 Some Final Thoughts",
    "text": "9.11 Some Final Thoughts\nRecall that the information criteria are a function of the log-likelihood. Log-likelihood values, and thus information criteria, from different models can be compared, so long as:\n\nThe exact same data is used to fit the models;\nThe exact same outcome is used to fit the models; and\nThe assumptions underlying the likelihood (independence, distributional assumptions) are met for each candidate model.\n\nIn all nine models we are using the same data set and outcome. However, the assumptions do not seem tenable for Models 1, 4, and 61. That means that we should not really have included Model 1, 4, nor 6 in our candidate set of models/working hypotheses.\nRemoving models from the candidate set will change some of the metrics in the table of model evidence (e.g., the model probabilities). This is especially true if we remove one of the more empirically supported models. To see this, here we remove Model 7, and the models that don’t meet the assumptions.\n\n#Create table of model evidence\nmodel_evidence = aictab(\n  cand.set = list(lm.0, lm.2, lm.3, lm.5, lm.8),\n  modnames = c(\"Model 0\", \"Model 2\", \"Model 3\", \"Model 5\", \"Model 8\")\n  )\n\n# View output\nmodel_evidence\n\n\nModel selection based on AICc:\n\n        K   AICc Delta_AICc AICcWt Cum.Wt      LL\nModel 5 5 215.83       0.00    0.8    0.8 -101.80\nModel 8 7 218.59       2.76    0.2    1.0 -100.05\nModel 2 4 228.55      12.72    0.0    1.0 -109.56\nModel 3 3 279.77      63.94    0.0    1.0 -136.47\nModel 0 2 283.00      67.17    0.0    1.0 -139.30\n\n\nIn practice, you wouldn’t remove a model that meets the assumptions and has a lot of empirical support, but pedagogically you need to understand that changing which models are included in the candidate set change your results! Similarly, changing the predictors in the candidate models, or adding other models to the candidate set can impact the amount of empirical support and the rank-ordering of hypotheses.\nIf we had a different set of data, we may also have a whole new ranking of models or interpretation of empirical support. The empirical support is linked to the data. The empirical support is very much relative to the candidate set of models and the data we have.\nLastly, it is important to note that although information criteria can tell you about the empirical support among a candidate set of models, it cannot say whether that is actually a “good” model. For that you need to look at the assumptions and other measures (e.g., \\(R^2\\)). You still need to do all of the work associated with model-building (e.g., selecting predictors from the substantive literature, exploring functional forms to meet the assumptions).",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#statistical-inference-and-information-criteria",
    "href": "03-02-information-criteria-and-model-selection.html#statistical-inference-and-information-criteria",
    "title": "9  Information Criteria and Model Selection",
    "section": "9.12 Statistical Inference and Information Criteria",
    "text": "9.12 Statistical Inference and Information Criteria\nFinally, it is important to mention that philosophically, information-criteria and statistical inference are two very different ways of evaluating statistical evidence. When we use statistical inference for variable selection, the evidence, the p-values, is a measure of how rare an observed statistic (e.g., \\(\\hat\\beta_k\\), t-value) is under the null hypothesis. The AIC, on the other hand, is a measure of the model-data compatibility accounting for the complexity of the model.\nIn general, the use of p-values is not compatible with the use of information criteria-based model selection methods; see Anderson (2008) for more detail. Because of this, it is typical to not report p-values when using information criteria for model selection. We should, however, reprt the standard errors or confidence intervals for the coefficient estimates, especially for any “best” model(s). This gives information about the statistical uncertainty that arises because of sampling error.\n\nIMPORTANT\nIt is important that you decide how you will be evaluating evidence and making decisions about variable and model selection prior to actually examining the data. Mixing and matching is not cool!",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#pretty-printing-tables-of-model-evidence-for-quarto-documents",
    "href": "03-02-information-criteria-and-model-selection.html#pretty-printing-tables-of-model-evidence-for-quarto-documents",
    "title": "9  Information Criteria and Model Selection",
    "section": "9.13 Pretty Printing Tables of Model Evidence for Quarto Documents",
    "text": "9.13 Pretty Printing Tables of Model Evidence for Quarto Documents\nWe can format the output from aictab() to be used in the gt() function. Because there are multiple classes associated with the output from the aictab() function, we first pipe model_evidence object into the data.frame() function. Viewing this, we see that the data frame, also includes an additional column that gives the relative likelihoods (ModelLik).\n\n#Create table of model evidence\nmodel_evidence = aictab(\n  cand.set = list(lm.0, lm.1, lm.2, lm.3, lm.4, lm.5, lm.6, lm.7, lm.8),\n  modnames = c(\"Model 0\", \"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\", \"Model 5\", \"Model 6\", \"Model 7\", \"Model 8\")\n  )\n\n\n# Create data frame to format into table\ntab_01 = model_evidence |&gt;\n  data.frame()\n\n# View table\ntab_01\n\n  Modnames K     AICc  Delta_AICc     ModelLik       AICcWt        LL    Cum.Wt\n8  Model 7 6 215.7859  0.00000000 1.000000e+00 4.480116e-01 -100.2776 0.4480116\n6  Model 5 5 215.8278  0.04193024 9.792531e-01 4.387168e-01 -101.8028 0.8867284\n9  Model 8 7 218.5893  2.80336833 2.461820e-01 1.102924e-01 -100.0546 0.9970208\n5  Model 4 4 227.0215 11.23555435 3.632707e-03 1.627495e-03 -108.7964 0.9986483\n3  Model 2 4 228.5452 12.75925104 1.695758e-03 7.597192e-04 -109.5583 0.9994080\n7  Model 6 5 229.2266 13.44069417 1.206120e-03 5.403555e-04 -108.5022 0.9999483\n2  Model 1 3 233.9220 18.13611066 1.152905e-04 5.165149e-05 -113.5472 1.0000000\n4  Model 3 3 279.7700 63.98406188 1.276549e-14 5.719088e-15 -136.4712 1.0000000\n1  Model 0 2 282.9998 67.21386820 2.539183e-15 1.137583e-15 -139.2999 1.0000000\n\n\nThen we can use the select() function to drop the LL and Cum.Wt columns from the data frame. The log-likelihood is redundant to the information in the AICc column, since AICc is a function of log-likelihood and the other information in the table. The cumulative weight can also easily be computed from the information in the AICcWt column.\n\n# Drop columns\ntab_01 = tab_01 |&gt;\n  select(-LL, -Cum.Wt)\n\n# View table\ntab_01\n\n  Modnames K     AICc  Delta_AICc     ModelLik       AICcWt\n8  Model 7 6 215.7859  0.00000000 1.000000e+00 4.480116e-01\n6  Model 5 5 215.8278  0.04193024 9.792531e-01 4.387168e-01\n9  Model 8 7 218.5893  2.80336833 2.461820e-01 1.102924e-01\n5  Model 4 4 227.0215 11.23555435 3.632707e-03 1.627495e-03\n3  Model 2 4 228.5452 12.75925104 1.695758e-03 7.597192e-04\n7  Model 6 5 229.2266 13.44069417 1.206120e-03 5.403555e-04\n2  Model 1 3 233.9220 18.13611066 1.152905e-04 5.165149e-05\n4  Model 3 3 279.7700 63.98406188 1.276549e-14 5.719088e-15\n1  Model 0 2 282.9998 67.21386820 2.539183e-15 1.137583e-15\n\n\nWe can then pipe the tab_01 data frame into the gt() function to format the table for pretty-printing in Quarto. For some column labels, I use the html() function in order to use HTML symbols to create the Greek letter Delta and the scripted “L”.\n\n# Create knitted table\ntab_01 |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(Modnames),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(K, AICc, Delta_AICc, ModelLik, AICcWt),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    Modnames = md(\"*Model*\"),\n    K = md(\"*k*\"),\n    AICc = md(\"*AICc*\"),\n    Delta_AICc = html(\"&#916;AICc\"),\n    ModelLik = html(\"Rel(&#8466;)\"),\n    AICcWt = md(\"*AICc Wt.*\")\n  ) |&gt;\n  tab_options(\n    table.width = pct(90)\n  ) |&gt;\n  tab_footnote(\n    footnote = html(\"Rel(&#8466;) = Relative likelihood. Values are displayed using scientific notation.\"),\n    locations = cells_column_labels(columns = ModelLik)\n  )\n\n\n\nTable 9.8: Models rank-ordered by the amount of empirical support as measured by the AICc after removing Model 1 from the candidate set. Other evidence includes the ΔAICc, relative likelihood, and model probability (AICc weight) for each model.\n\n\n\n\n\n\n\n\n\nModel\nk\nAICc\nΔAICc\nRel(ℒ)1\nAICc Wt.\n\n\n\n\nModel 7\n6\n215.7859\n0.00000000\n1.000000e+00\n4.480116e-01\n\n\nModel 5\n5\n215.8278\n0.04193024\n9.792531e-01\n4.387168e-01\n\n\nModel 8\n7\n218.5893\n2.80336833\n2.461820e-01\n1.102924e-01\n\n\nModel 4\n4\n227.0215\n11.23555435\n3.632707e-03\n1.627495e-03\n\n\nModel 2\n4\n228.5452\n12.75925104\n1.695758e-03\n7.597192e-04\n\n\nModel 6\n5\n229.2266\n13.44069417\n1.206120e-03\n5.403555e-04\n\n\nModel 1\n3\n233.9220\n18.13611066\n1.152905e-04\n5.165149e-05\n\n\nModel 3\n3\n279.7700\n63.98406188\n1.276549e-14\n5.719088e-15\n\n\nModel 0\n2\n282.9998\n67.21386820\n2.539183e-15\n1.137583e-15\n\n\n\n1 Rel(ℒ) = Relative likelihood. Values are displayed using scientific notation.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#references",
    "href": "03-02-information-criteria-and-model-selection.html#references",
    "title": "9  Information Criteria and Model Selection",
    "section": "9.14 References",
    "text": "9.14 References\n\n\n\n\n\n\nAnderson, D. R. (2008). Model based inference in the life sciences: A primer on evidence. Springer.\n\n\nBurnham, K. P., Anderson, D. R., & Huyvaert, K. P. (2011). AIC model selection and multimodel inference in behavioral ecology: Some background, observations, and comparisons. Behavioral Ecology and Sociobiology, 65(1), 23–35.\n\n\nHurvich, C. M., & Tsai, C.-L. (1989). Regression and time series model selection in small samples,. Biometrika, 76, 297–307.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-02-information-criteria-and-model-selection.html#footnotes",
    "href": "03-02-information-criteria-and-model-selection.html#footnotes",
    "title": "9  Information Criteria and Model Selection",
    "section": "",
    "text": "Analyses not shown.↩︎",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Information Criteria and Model Selection</span>"
    ]
  },
  {
    "objectID": "03-03-logarithmic-transformations-predictor.html",
    "href": "03-03-logarithmic-transformations-predictor.html",
    "title": "10  Log-Transforming the Predictor",
    "section": "",
    "text": "10.1 Relationship between Graduation Rate and SAT Scores\nIn this chapter, you will learn another method of dealing with nonlinearity. Specifically, we will look at log-transforming the predictor in a linear model. To do so, we will use the mn-schools.csv dataset. See data codbook for additional information.\nOur goal will be to re-visit if (and how) academic “quality” of the student-body (measured by SAT score) is related to institutional graduation rate. Within this work, we will use information theoretic approcaches (namely the AICc and related measures) to evaluate any fitted models.\nIn a previous set of notes, we determined that the relationship between SAT scores and graduation rates was curvilinear. We initially saw evidence of this curvilinearity in the scatterplot of graduation rates versus median SAT scores. After fitting a model assuming a linear relationship (syntax not shown), the plot of the standardized residuals versus the fitted values also suggests that the linearity assumption is not tenable.\nCode\n# Scatterplot\np1 = ggplot(data = mn, aes(x = sat, y = grad)) +\n    geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE) +\n    theme_light() +\n  xlab(\"Estimated median SAT score (in hundreds)\") +\n  ylab(\"Six-year graduation rate\")\n\n# Fit linear model\nlm.1 = lm(grad ~ 1 + sat, data = mn)\n\n# Check linearity assumptions\np2 = residual_plots(lm.1, type = \"s\") \n\n# Layout plot\np1 | p2\n\n\n\n\n\n\n\n\nFigure 10.1: LEFT: Six-year graduation rate as a function of median SAT score. The loess smoother (blue line) is also displayed. RIGHT: Scatterplot of the standardized residuals versus the fitted values for a model using median SAT scores to predict variation in six-year graduation rates. The reference line of Y=0 is also displayed along with the 95% confidence envelope (grey shaded area). The loess smoother (solid, blue line) shows the empirical relationship betwen the residuals and fitted values.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Log-Transforming the Predictor</span>"
    ]
  },
  {
    "objectID": "03-03-logarithmic-transformations-predictor.html#relationship-between-graduation-rate-and-sat-scores",
    "href": "03-03-logarithmic-transformations-predictor.html#relationship-between-graduation-rate-and-sat-scores",
    "title": "10  Log-Transforming the Predictor",
    "section": "",
    "text": "10.1.1 Logarithmic Function: Yet Another Non-Linear Function\nOne way to model this non-linearity is to fit a model that included a quadratic polynomial effect. Remember that the quadratic function is ‘U’-shaped. In our example, the quadratic term was negative, which creates an upside-down ‘U’-shape. This negative quadratic function is shown in the left-had side of the figure below.\nWe do not have to use a quadratic function to model curvilinear relationships. We can use any mathematical function that mimics the curvilinear pattern observed in the data.1 One mathematical function that is useful for modeling curvilinearity is the logarithmic function. The logarithmic function is shown in the right-hand side of the figure below.\n\n\nCode\nfig_01 = data.frame(\n  x = seq(from = -5, to = 3, by = 0.1)\n  ) |&gt;\n  mutate(y =  -(x^2))\n\np1 = ggplot(data = fig_01, aes(x = x, y = y)) +\n    geom_line() +\n    theme_light() +\n  scale_x_continuous(name = \"X\", breaks = NULL) +\n  scale_y_continuous(name = \"Y\", breaks = NULL) +\n  ggtitle(\"Quadratic Function\")\n\n\nfig_02 = data.frame(\n  x = seq(from = 0.1, to = 5, by = 0.1)\n  ) |&gt;\n  mutate(y =  log2(x))\n\np2 = ggplot(data = fig_02, aes(x = x, y = y)) +\n    geom_line() +\n    theme_light() +\n  scale_x_continuous(name = \"X\", breaks = NULL) +\n  scale_y_continuous(name = \"Y\", breaks = NULL) +\n  ggtitle(\"Logarithmic Function\")\n\n# Layout the plots\np1 | p2\n\n\n\n\n\n\n\n\nFigure 10.2: LEFT: Negative quadratic function of X. RIGHT: Logarithmic function of X.\n\n\n\n\n\nThese two functions have some similarities. They both model non-linear growth (continuous and diminishing growth). However, in the quadratic function, this growth eventually peaks and then is followed by continuous and increasing loss (parabola; the function changes direction). The logarithmic function, on the other hand, does not change direction—it continues to grow, albeit ever diminishing in the amount of growth.2\nAs you consider different mathematical functions, it is important to consider the substantive knowledge in an area. For example, while empirically both the quadratic polynomial model and the logarithmic model might both suggest reasonable fit to the data, in this context, the logarithmic model might be a more substantively sound model than the quadratic polynomial model. We would probably expect that the effect of SAT on graduation rate would diminish for schools with higher median SAT scores, but that it wouldn’t actually change direction. (This would mean that at some value, an increase in SAT would be associated with a lower graduation rate!)\n\n\n\n10.1.2 Fitting a Logarithmic Function Using lm()\nThe mathematical expression of the logarithmic function is:\n\\[\nY = \\log(X)\n\\]\nThis implies that, to fit a logarithmic function we need to log-transform the predictor and use that in the lm() function. (Note that we do not transform the Y-value.) The choice of base for the logarithm is irrelevant statistically, but does impact the interpretation of the slope. To illustrate, we will examine fitting models using base-2, base-10, and base-e. Rather than mutating the log-transformed variables into the data, we will include the log-transformation directly in the lm() function.\n\n\n\n10.1.3 Using Base-2 Log-Transformed SAT Scores in the Regression Model\nTo fit the model, we use the lm() function and input the log-transformed SAT scores as the predictor.\n\n# Fit regression model\nlm.log2 = lm(grad ~ 1 + log(sat, base = 2), data = mn)\n\n\nWe can now look at the regression output and interpret the results.\n\n# Model-level output\nglance(lm.log2)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.811         0.805  7.39      133. 9.30e-13     1  -112.  230.  234.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nExamining the model-level output, we see that differences in \\(\\log_2(\\mathrm{SAT})\\) explain 81.13% of the variation in graduation rates. Since differences in \\(\\log_2(\\mathrm{SAT})\\) imply that there are differences in the raw SAT scores, we would typically just say that “differences in SAT scores explain 81.13% of the variation in graduation rates.”\nMoving to the coefficient-level output,\n\n# Coefficient-level output\ntidy(lm.log2)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)           -307.     31.9      -9.62 7.94e-11\n2 log(sat, base = 2)     106.      9.22     11.5  9.30e-13\n\n\nWe can write the fitted equation as,\n\\[\n\\hat{\\mathrm{Graduation~Rate}_i} = -306.7 + 106.4\\bigg[\\log_2(\\mathrm{SAT}_i)\\bigg]\n\\]\nWe can interpret the coefficients as we always do, recognizing that these interpretation are based on the log-transformed predictor.\n\nThe intercept value of \\(-306.7\\) is the predicted average graduation rate for all colleges/universities with a \\(\\log_2(\\mathrm{SAT})\\) value of 0.\nThe slope value of 106.4 indicates that each one-unit difference in \\(\\log_2(\\mathrm{SAT})\\) is associated with a 106.4-unit difference in graduation rate, on average.\n\n\n\n\n10.1.4 Better Interpretations: Back-transforming to the Raw Metric\nWhile these interpretations are technically correct, it is more helpful to your readers (and more conventional) to interpret any regression results in the raw metric of the variable rather than log-transformed metric. This means we have to back-transform the interpretations. To back-transform a logarithm, we use its inverse function; exponentiation.\nWe interpreted the intercept as, “the predicted average graduation rate for all colleges/universities with a \\(\\log_2(\\mathrm{SAT})\\) value of 0”. To interpret this using the raw metric of our SAT attribute, we have to understand what \\(\\log_2(\\mathrm{SAT}) = 0\\) is equivalent to in the original SAT variable’s scale. Mathematically,\n\\[\n\\log_2 (\\mathrm{SAT}) = 0 \\quad \\mathrm{is~equivalent~to} \\quad 2^{0} = \\mathrm{SAT} \\\\\n\\]\nIn this computation, \\(\\mathrm{SAT}=1\\). Thus, rather than using the log-transformed interpretation, we can, instead, interpret the intercept as,\n\nThe predicted average graduation rate for all colleges/universities with a median SAT value of 1 (which since this is measures in hundreds corresponds to a median SAT of 100) is \\(-306.7\\). Since there are no colleges/universities in our data that have a median SAT value of 1, this is extrapolation.\n\nWhat about the slope? Our interpretation was that “each one-unit difference in \\(\\log_2(\\mathrm{SAT})\\) is associated with a 106.4-unit difference in graduation rate, on average.” Working with the same idea of back-transformation, we need to understand what a one-unit difference in \\(\\log_2(\\mathrm{SAT})\\) means. Consider four values of \\(\\log_2(\\mathrm{SAT})\\) that are each one-unit apart:\n\\[\n\\begin{split}\n\\log_2(\\mathrm{SAT}) &= 1\\\\\n\\log_2(\\mathrm{SAT}) &= 2\\\\\n\\log_2(\\mathrm{SAT}) &= 3\\\\\n\\log_2(\\mathrm{SAT}) &= 4\n\\end{split}\n\\]\nIf we back-transform each of these, then we can see how the four values of the raw SAT variable would differ.\n\\[\n\\begin{split}\n\\mathrm{SAT} &= 2^1 = 2\\\\\n\\mathrm{SAT} &= 2^2 = 4\\\\\n\\mathrm{SAT} &= 2^3 = 8\\\\\n\\mathrm{SAT} &= 2^4 = 16\n\\end{split}\n\\]\nWhen \\(\\log_2(\\mathrm{SAT})\\) is increased by one-unit, the raw SAT value is doubled. We can use this in our interpretation of slope:\n\nA doubling of the SAT value is associated with a 106.4 percentage point difference in graduation rate, on average.\n\nThe technical language for doubling is a “two-fold difference”. So we would conventionally interpret this as:\n\nEach two-fold difference in SAT value is associated with a 106.4 percentage point difference in graduation rate, on average.\n\nTo understand this further, consider a specific school, say Augsburg. Their measurement on the raw SAT variable is 10.3, and their log-transformed SAT score is 3.36. Using the fitted regression equation (which employs the log-transformed SAT),\n\n-306.7 + 106.4 * 3.36\n\n[1] 50.804\n\n\nAugsburg’s predicted graduation rate would be 50.8. If we increase the l2sat score by 1 to 4.36 (which is equivalent to a raw SAT measurement of 20.6; double 10.3), their predicted graduation rate would be,\n\n-306.7 + 106.4 * 4.36\n\n[1] 157.204\n\n\nThis is an increase of 106.4 percentage points from the predicted value of 50.8 when the l2sat value was 3.36.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Log-Transforming the Predictor</span>"
    ]
  },
  {
    "objectID": "03-03-logarithmic-transformations-predictor.html#using-base-10-log-transformed-sat-scores-in-the-regression-model",
    "href": "03-03-logarithmic-transformations-predictor.html#using-base-10-log-transformed-sat-scores-in-the-regression-model",
    "title": "10  Log-Transforming the Predictor",
    "section": "10.2 Using Base-10 Log-Transformed SAT Scores in the Regression Model",
    "text": "10.2 Using Base-10 Log-Transformed SAT Scores in the Regression Model\nHow do things change if we log-transfomr the SAT scores using base-10 rather than base-2?\n\n# Fit model\nlm.log10 = lm(grad ~ 1 + log(sat, base = 10), data = mn)\n\n# Model-level output\nglance(lm.log10)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.811         0.805  7.39      133. 9.30e-13     1  -112.  230.  234.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nExamining the model-level output, we see that differences in \\(\\log_{10}(\\mathrm{SAT})\\) explain 81.13% of the variation in graduation rates. Or simply, that differences in SAT scores explain 81.13% of the variation in graduation rates. These model-level results are the same as when we used the base-2 logarithm.\n\n# Coefficient-level output\ntidy(lm.log10)\n\n# A tibble: 2 × 5\n  term                estimate std.error statistic  p.value\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)            -307.      31.9     -9.62 7.94e-11\n2 log(sat, base = 10)     354.      30.6     11.5  9.30e-13\n\n\nThe linear form of the fitted equation is,\n\\[\n\\hat{\\mathrm{Graduation~Rate}_i} = -306.7 + 353.6\\bigg[\\log_{10}(\\mathrm{SAT}_i)\\bigg]\n\\]\n\n\n\n\nWe can interpret the coefficients using the base-10 logarithm of SAT scores as:\n\nThe intercept value of \\(-306.7\\) is the predicted average graduation rate for all colleges/universities with a \\(\\log_{10}(\\mathrm{SAT})\\) value of 0.\nThe slope value of 353.6 indicates that each one-unit difference in \\(\\log_{10}(\\mathrm{SAT})\\) is associated with a 353.6-unit difference in graduation rate, on average.\n\nComparing these results back to the base-2 results, we see that the interpretation of the intercept is identical to that from the base-2 model. The interpretation of the slope has changed. We can understand why by back-transforming the coefficient interpretations so that we are using raw median SAT scores rather than \\(\\log_{10}(\\mathrm{SAT})\\) scores.\n\nThe predicted average graduation rate for all colleges/universities with a SAT value of 1 (median SAT score = 100) is \\(-306.7\\).\nEach ten-fold difference in SAT is associated with a 353.6 percentage point difference in graduation rate, on average.\n\nTo further think about the effect of SAT, if Augsburg improved its median SAT score ten-fold (i.e., going from a SAT value of 10.3 to a value of 103) we would predict its graduation rate to go up by 353.6 percentage points!\nThe reason the slope value is different is because in this model we are expressing the change in the graduation rate for ten-fold difference in SAT scores, whereas in the previous model the slope value expressed the change in the graduation rate for two-fold difference in SAT scores.\n\n10.2.1 What About the Residual Fit?\nWhile most of the results didn’t change when we used base-10 rather than base-2, you might be asking whether one of these models fits the data better than the other. To determine that, we need to compare the residuals from these models.\n\n# Residual plots for base-2 model\np1 = residual_plots(lm.log2)\n\n# Residual plots for base-10 model\np2 = residual_plots(lm.log10)\n\n# Layout\np1 / p2\n\n\n\n\n\n\n\nFigure 10.3: Scatterplot of the standardized residuals versus the fitted values for the model fitted using the base-2 log-transformed median SAT values (LEFT) and the base-10 log-transformed median SAT values (RIGHT). In both plots the reference line of Y=0 is displayed along with the 95% confidence envelope (grey shaded area). The loess smoother (solid, blue line) showing the empirical relationship is also displayed.\n\n\n\n\n\nThe residuals fit EXACTLY the same. That suggests that changing the base does NOT improve the fit of the model. Why is this? Let’s again use Augsburg as an example. Using the fitted model that employed the base-2 logarithm, we found that Augsburg’s predicted graduation rate was,\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}} &= -306.7 + 106.4\\bigg[\\log_2(10.3)\\bigg] \\\\\n&= -306.7 + 106.4\\bigg[3.36\\bigg] \\\\\n&= 50.8\n\\end{split}\n\\]\nUsing the model that employed the base-10 logarithm, Augsburg’s predicted graduation rate would be\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}} &= -306.7 + 353.6\\bigg[\\log_{10}(10.3)\\bigg] \\\\\n&= -306.7 + 353.6\\bigg[1.01\\bigg] \\\\\n&= 50.8\n\\end{split}\n\\]\nAugsburg’s predicted graduation rate is exactly the same in the two models. This implies that Augsburg’s residual would also be the same in the two models. This is true for every college. Because of this, increasing (or decreasing) the base used in the logarithm does not help improve the fit of the model. The fit is exactly the same no matter which base you choose.\n\nPROTIP\nRegardless of base choice, the model-level results (e.g., \\(R^2\\), AICc) will the same. At the coefficient level, the intercept results (B, SE, p-value) will also be the same regardless of base. The only results that change are the results for the log-transformed effect (i.e., the estimate and SE for the slope). This reflects the change in scale and interpretation. However, the inferential results (t-value and p-value) are the same for both transformations. Because of this, the choice of base is solely based on interpretation of the slope. For example, does it make more sense to talk about a two-fold difference in the predictor? A five-fold difference in the predictor? A ten-fold difference in the predictor?\n\n\n\n\n10.2.2 Using Base-e Log-Transformed SAT Scores in the Regression Model\nIn our example, neither of the bases we examined is satisfactory in terms of interpreting the effect of median SAT score on graduation rate. Two-fold differences in median SAT scores are very unlikely, to say anything of ten-fold differences. One base that is commonly used for log-transformations because it offers a reasonable interpretation is base-e. Below we regress graduation rates on the log-transformed SAT scores, using the natural logarithm.\n\n# Fit model\nlm.ln = lm(grad ~ 1 + log(sat), data = mn)\n\n# Model-level output\nglance(lm.ln)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.811         0.805  7.39      133. 9.30e-13     1  -112.  230.  234.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# Coefficient-level output\ntidy(lm.ln)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -307.      31.9     -9.62 7.94e-11\n2 log(sat)        154.      13.3     11.5  9.30e-13\n\n\nAs with any base, using base-e results in the same model-level evidence (\\(R^2=.811\\), AIC values). The fitted equation is,\n\\[\n\\hat{\\mathrm{Graduation~Rate}_i} = -306.7 + 153.6\\bigg[\\ln(\\mathrm{SAT}_i)\\bigg]\n\\]\nThe intercept has the same coefficient (\\(\\hat\\beta_0=-306.7\\)), SE, t-value, and p-value as the intercept from the models using base-2 and base-10 log-transformations of SAT. (This is, again, because \\(2^0=10^0=e^0=1\\).) And, although the coefficient and SE for the effect of SAT is again different (a one-unit change in the three different log-scales does not correspond to the same amount of change in raw SAT for the three models).\nSo how can we interpret the model’s coefficients?\n\nThe intercept can be interpreted exactly the same as in the previous models in which we used base-2 or base-10; namely that the predicted average graduation rate for colleges/universities with a SAT value of one (median SAT score of 100) is \\(-306.7\\).\nInterpreting the slope, we could say that an e-fold difference in SAT value is associated with a 153.6-unit difference in graduation rates, on average.\n\nWe can aslo verify that this model’s residuals are identical to the other log-transformed models’ residuals.\n\n# Obtain residuals for base-2 log-transformed SAT\nresidual_plots(lm.ln)\n\n\n\n\n\n\n\nFigure 10.4: Plot of the standardized residuals versus the fitted values for a model fitted using the natural log-transformed median SAT scores. The reference line of Y=0 is displayed along with the 95% confidence envelope (grey shaded area). The loess smoother (solid, blue line) showing the empirical relationship is also displayed.\n\n\n\n\n\n\n\n\n10.2.3 Interpretation Using Percent Change\nConsider three schools, each having a median SAT values that differs by 1%; say these schools have median SAT values of 10, 10.1, 10.201. Using the fitted equation, we can compute the predicted graduation rate for each of these hypothetical schools:\n\\[\n\\hat{\\mathrm{Graduation~Rate}} = -306.7 + 153.6 \\bigg[\\ln (\\mathrm{SAT})\\bigg]\n\\]\nThe SAT values and predicted graduation rates for these schools are given below:\n\n\nCode\n# Create data\ntab_05 = data.frame(\n  sat = c(10, 10.1, 10.201)\n  ) %&gt;%\n  mutate(\n    grad = predict(lm.ln, newdata = .)\n  )\n\n# Create table\ntab_05 |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(sat, grad),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    sat = md(\"*SAT*\"),\n    grad = md(\"Predicted Graduation Rate\")\n  ) |&gt;\n  tab_options(\n    table.width = pct(40)\n  )\n\n\n\n\nTable 10.1: Median SAT Values and Graduation Rates for Three Hypothetical Schools that have Median SAT Values that Differ by One Percent.\n\n\n\n\n\n\n\n\n\nSAT\nPredicted Graduation Rate\n\n\n\n\n10.000\n46.87784\n\n\n10.100\n48.40581\n\n\n10.201\n49.93378\n\n\n\n\n\n\n\n\n\n\nThe difference between each subsequent predicted graduation rate is 1.53.\n\n# Difference between predicted value for 10.1 and 10\n48.4058 - 46.8778\n\n[1] 1.528\n\n# Difference between predicted value for 10.201 and 10.1\n49.9338 - 48.4058\n\n[1] 1.528\n\n\nIn other words, schools that have a SAT value that differ by 1%, have predicted graduation rates that differ by 1.53, on average.\n\nWARNING\nBe very careful when you are using “percent difference” and “percentage points difference”. These are two very different things. For example, increasing a graduation rate from 50% to 60% represents a difference of 10 percentage points, but it is a 20% increase!\n\n\n\n\n10.2.4 Mathematically Calculating the Size of the Effect\nTo understand how we can directly compute the effect for a 1% change in X, consider the predicted values for two x-values that differ by 1%, if we use symbolic notation:\n\\[\n\\begin{split}\n\\hat{y}_1 &= \\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(x)\\right] \\\\\n\\hat{y}_2 &= \\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(1.01x)\\right]\n\\end{split}\n\\]\nThe difference in their predicted values is:\n\\[\n\\begin{split}\n\\hat{y}_2 - \\hat{y}_1 &= \\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(1.01x)\\right] - \\left(\\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(x)\\right]\\right) \\\\[1ex]\n&=\\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(1.01x)\\right] - \\hat\\beta_0 - \\hat\\beta_1\\left[\\ln(x)\\right] \\\\[1ex]\n&=\\hat\\beta_1\\left[\\ln(1.01x)\\right] - \\hat\\beta_1\\left[\\ln(x)\\right] \\\\[1ex]\n&=\\hat\\beta_1\\left[\\ln(1.01x) - \\ln(x)\\right]\\\\[1ex]\n&=\\hat\\beta_1\\left[\\ln(\\frac{1.01x}{1x})\\right]\n\\end{split}\n\\]\nIf we substitute in any value for \\(x\\), we can now directly compute this constant difference. Note that a convenient value for X is 1. Then this reduces to:\n\\[\n\\hat\\beta_1\\left[\\ln(1.01)\\right]\n\\]\nSo now, we can interpret this as: a 1% difference in X is associated with a \\(\\hat\\beta_1\\left[\\ln(1.01)\\right]\\)-unit difference in Y, on average.\nIn our model, we can compute this difference using the fitted coefficient \\(\\hat\\beta_1=153.6\\) as\n\\[\n153.6\\left[\\ln(1.01)\\right] = 1.528371\n\\]\nThe same computation using R is\n\n153.6 * log(1.01)\n\n[1] 1.528371\n\n\nThis gives you the constant difference exactly. So you can interpret the effect of SAT as, each 1% difference in SAT score is associated with a difference in graduation rates of 1.53 percentage points, on average.\n\n\n\n10.2.5 Approximating the Size of the Effect\nWe can get an approximation for the size of the effect by using the mathematical shortcut of:\n\\[\n\\mathrm{Effect} \\approx \\frac{\\hat\\beta_1}{100}\n\\]\nUsing our fitted results, we could approximate the size of the effect as,\n\\[\n\\frac{153.6}{100} = 1.536\n\\]\nWe could then interpret the effect of SAT by saying a 1% difference in median SAT score is associated with a 1.53 percentage point difference in predicted graduation rate, on average.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Log-Transforming the Predictor</span>"
    ]
  },
  {
    "objectID": "03-03-logarithmic-transformations-predictor.html#plotting-the-fitted-curve",
    "href": "03-03-logarithmic-transformations-predictor.html#plotting-the-fitted-curve",
    "title": "10  Log-Transforming the Predictor",
    "section": "10.3 Plotting the Fitted Curve",
    "text": "10.3 Plotting the Fitted Curve\nTo aid interpretation of the effect of median SAT score on graduation rate, we can plot the fitted curve. Recall that our fitted equation was:\n\\[\n\\hat{\\mathrm{Graduation~Rate}} = -306.7 + 153.6 \\bigg[\\ln (\\mathrm{SAT})\\bigg]\n\\]\nWe can implement this into the geom_function() layer of our ggplot() syntax.\n\n# Plot\nggplot(data = mn, aes(x = sat, y = grad)) +\n    geom_point(alpha = 0.3) +\n    geom_function(\n      fun = function(x) {-306.7 + 153.6*log(x)}\n      ) +\n    theme_light() +\n  xlab(\"Estimated median SAT score (in hundreds)\") +\n  ylab(\"Six-year graduation rate\")\n\n\n\n\n\n\n\nFigure 10.5: Plot of the predicted graduation rate as a function of median SAT score. The non-linearity in the plot indicates that there is a diminishing positive effect of SAT on graduation rates.\n\n\n\n\n\n\n\n10.3.1 Alternative Form of the Fitted Equation\nThe fitted equation expresses graduation rates as a function of log-transformed median SAT scores. Using the natural logarithm, the fitted equation is:\n\\[\n\\hat{Y}_i = \\hat\\beta_0 + \\hat\\beta_1\\bigg[\\ln(X_i)\\bigg]\n\\]\nNow we will work through the algebra to relate \\(\\hat{Y}_i\\) to \\(X_i\\). Because there is a log of X we need to exponentiate both sides of the equation to get back to the raw X.\n\\[\ne^{\\hat{Y}_i} = e^{\\hat\\beta_0 + \\hat\\beta_1\\bigg[\\ln(X_i)\\bigg]}\n\\]\nThen we can use the algebraic rules of exponents and logarithms to re-express this equation.\n\\[\n\\begin{split}\ne^{\\hat{Y}_i} &= e^{\\hat\\beta_0} \\times e^{\\hat\\beta_1\\bigg[\\ln(X_i)\\bigg]} \\\\[1ex]\ne^{\\hat{Y}_i} &= e^{\\hat\\beta_0} \\times \\bigg[e^{\\ln(X_i)}\\bigg]^{\\hat\\beta_1} \\\\[1ex]\ne^{\\hat{Y}_i} &= e^{\\hat\\beta_0} \\times X_i^{\\hat\\beta_1}\n\\end{split}\n\\]\nNow the only problem is that the left-hand side is \\(2^{\\hat{Y}_i}\\) and not \\(Y_i\\). To fix this we take the natural logarithm of both sides of the equation.\n\\[\n\\ln\\bigg(e^{\\hat{Y}_i}\\bigg) = \\ln\\bigg(e^{\\hat\\beta_0} \\times X_i^{\\hat\\beta_1}\\bigg)\n\\]\nThis gives us an expression of the curve that relates the outcome to the predictor. Namely,\n\\[\n\\hat{Y}_i = \\ln\\bigg(e^{\\hat\\beta_0} \\times X_i^{\\hat\\beta_1}\\bigg)\n\\]\nNotice that the right-hand side of this expression is a log function; it has the general form of \\(Y=\\log(\\)🦒\\()\\).3 Remember that this is the continuous decay function we were interested in using to model the curvilinearity shown in the data. The important thing is that this non-linear decay function is mathematically the same as using a the log-transformed predictor to model the outcome with linear model.\n\\[\nY_i=\\ln\\bigg(e^{\\hat\\beta_0} \\times X_i^{\\hat\\beta_1}\\bigg) ~~\\equiv~~ Y_i =\\beta_0 + \\beta_1\\bigg[\\ln(X_i)\\bigg]\n\\]\nPractically speaking, this means we can model non-linearity using a linear model. Which means we can use the lm() function and all the other ideas you have learned about linear models (e.g., including covariates, adding interaction effects, assumption checking).",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Log-Transforming the Predictor</span>"
    ]
  },
  {
    "objectID": "03-03-logarithmic-transformations-predictor.html#polynomial-effects-vs.-log-transformations",
    "href": "03-03-logarithmic-transformations-predictor.html#polynomial-effects-vs.-log-transformations",
    "title": "10  Log-Transforming the Predictor",
    "section": "10.4 Polynomial Effects vs. Log-Transformations",
    "text": "10.4 Polynomial Effects vs. Log-Transformations\nThe inclusion of polynomial effects and the use of a log-transformation was to model the nonlinearity observed in the relationship between SAT scores and graduation rates. Both methods were successful in this endeavor. While either method could be used in practice to model nonlinearity, there are some considerations when making the choice of which may be more appropriate for a given modeling situation.\nThe first consideration is one of substantive theory. The plot below shows the mathematical function for a log-transformed X (solid, black line) and for a quadratic polynomial of X (dashed, red line).\n\n\nCode\nnew = data.frame(x = seq(from = 1, to = 15, by = 0.01))\nnew$y = log(new$x)\nnew$quad =  -0.21717 + 0.48782*new$x  - 0.02453*new$x^2\nplot(y ~ x, data = new, type = \"l\", lwd = 1.5, col = \"#ea5975\")\nlines(quad ~ x, data = new, col = \"#6d92ee\", lty = \"dashed\", lwd = 1.5)\n\n\n\n\n\n\n\n\nFigure 10.6: Comparison of the quadratic polynomial (blue, dashed line) and logarithmic (red, solid line) functions of X.\n\n\n\n\n\nBoth functions are nonlinear, however the polynomial function changes direction. For low x-values, the function has a large positive effect. This effect diminishes as the values of X gets bigger, and around \\(x=9\\) the effect is zero. For larger values of X, the effect is actually negative. For the logarithmic function, the effect is always positive, but it diminishes as X gets larger. (Functions that constantly increase, or constantly decrease, are referred to as monotonic functions.) Theoretically, these are very different ideas, and if substantive literature suggests one or the other, you should probably acknowledge that in the underlying statistical model that is fitted.\nEmpirically, the two functions are very similar especially within certain ranges of X. For example, although the predictions from these models would be quite different for really high values of X, if we only had data from the range of 2 to 8 (\\(2\\leq X \\leq 8\\)) both functions would produce similar residuals. In this case, the residuals would likely not suggest better fit for either of the two models. In this case, it might be prudent to think about Occam’s Razor—if two competing models produce similar predictions, adopt the simpler model. Between these two functions, the log-transformed model is simpler; it has one predictor compared to the two predictors in the quadratic model. The mathematical models make this clear:\n\\[\n\\begin{split}\n\\mathbf{Quadratic~Polynomial:~}Y_i &= \\beta_0 + \\beta_1(X_i) + \\beta_2(X_i^2) +\\epsilon_i \\\\\n\\mathbf{Log\\mbox{-}Transform:~}Y_i &= \\beta_0 + \\beta_1\\bigg[\\ln(X_i)\\bigg] + \\epsilon_i\n\\end{split}\n\\]\nIn the polynomial model, we need to estimate four parameters: an intercept, a slope for the linear term of X, a slope for the quadratic term of X, and a residual variance term. (Remember, the quadratic polynomial model is an interaction model.) While the log model only requires us to estimate three parameters (\\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), and \\(\\hat\\sigma^2_{\\epsilon}\\)). If there is no theory to guide your model’s functional form, and the residuals from the polynomial and log-transformed models seem to fit equally well, then the log-transformed model saves you a degree of freedom, and better achieves the goal of parsimony.\nFrom the two models, it is also possible to see that the these are not nested models; by setting one (or more) of the effects in either model to zero, we do not end up with the other model. This means we CANNOT use the likelihood ratio test to compare whether the quadratic model or the logarithmic model is more appropriate. However, we could use information criteria to choose from among these models since they are based on the same data and have the same outcome.\n\n# Fit models\nlm.quad = lm(grad ~ 1 + sat + I(sat ^ 2), data = mn)\nlm.log = lm(grad ~ 1 + log(sat), data = mn)\n\n# Table of model evidence\naictab(\n  cand.set = list(lm.quad, lm.log),\n  modnames = c(\"Quadratic polynomial\", \"Log-transformed SAT\")\n)\n\n\nModel selection based on AICc:\n\n                     K   AICc Delta_AICc AICcWt Cum.Wt      LL\nQuadratic polynomial 4 228.55       0.00   0.72   0.72 -109.56\nLog-transformed SAT  3 230.39       1.85   0.28   1.00 -111.78\n\n\nHere the quadratic model seems more empirically supported, but there is a fair amount of support for the log-model as well. Given this evidence, we probably cannot discern which model is better. (There is a fair amount of model selection uncertainty.4)\nAs with all decisions from statistical evidence, you must also rely on substantive knowledge and an evaluation of the residuals. Substantively, the quadratic model is less defensible, since it seems unlikely that we would see the graduation rate decreasing for institutions with high SAT scores. It is more likely that the effect of SAT score has less and less impact on graduation rates as it gets larger. In this scenario, the substantive knowledge points to the log model rather than the quadratic model.\nWe can also examine the residuals for the two models:\n\n\nCode\n# Evaluate residuals\np1 = residual_plots(lm.log)\np2 = residual_plots(lm.quad)\n\n# Layout\np1 / p2\n\n\n\n\n\n\n\n\nFigure 10.7: Residual plots for the log model (TOP) and quadratic model (BOTTOM).\n\n\n\n\n\nBoth models seem tenable based on the residual fit to the assumptions. Given this, the substantive argument, and the parsimony argument (the log model is simpler), I would likely adopt the log model over the quadratic model. You could also make this decision later, after adding other covariates and re-checking the model evidence and residuals for the updated models.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Log-Transforming the Predictor</span>"
    ]
  },
  {
    "objectID": "03-03-logarithmic-transformations-predictor.html#including-covariates",
    "href": "03-03-logarithmic-transformations-predictor.html#including-covariates",
    "title": "10  Log-Transforming the Predictor",
    "section": "10.5 Including Covariates",
    "text": "10.5 Including Covariates\nOnce we have settled on the functional form for the SAT predictor, we can also include covariates in the model. Below we examine the nonlinear effect of SAT on graduation controlling for differences in sector. To do this, we include a main effect of sector (public) in the model along with the natural log-transformed median SAT scores.\n\n# Fit model\nlm.2 = lm(grad ~ 1 + public + log(sat), data = mn)\n\n# Model-level output\nglance(lm.2)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.866         0.857  6.34      96.6 8.47e-14     2  -106.  220.  226.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe model explains 86.6% of the variation in graduation rates.\n\n# Coefficient-level output\ntidy(lm.2)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -286.       28.0     -10.2  2.73e-11\n2 public         -8.50      2.44     -3.48 1.56e- 3\n3 log(sat)      146.       11.6      12.6  1.74e-13\n\n\nThe linear form of the fitted equation is,\n\\[\n\\hat{\\mathrm{Graduation~Rate}_i} = -286.1 - 8.5(\\mathrm{Public}_i) + 146.0\\bigg[\\ln(\\mathrm{SAT}_i)\\bigg]\n\\]\nInterpreting each of the coefficients using the raw (back-transformed) metric of median SAT value:\n\nThe intercept value of \\(-286.1\\) is the predicted average graduation rate for all public colleges/universities with a SAT value of 1 (extrapolation).\nPublic schools have a predicted graduation rate that is 8.5 percentage points lower, on average, than private schools controlling for differences in median SAT scores.\nEach 1% difference in median SAT value is associated with a 1.46 percentage point difference in predicted graduation rate, on average, after controlling for differences in sector.\n\n\n\n10.5.1 Plot of the Fitted Equation\nTo further help interpret these effects, we can plot the curves resulting from this fitted equation. Now we will have two curves one for public schools and one for private schools. To determine the exact equations for these curves, we will substitute either 0 (for private schools) or 1 (for public schools) into the fitted equation for the public variable.\nPrivate\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} &= -286.1 - 8.5(0) + 146.0\\bigg[\\ln(\\mathrm{SAT}_i)\\bigg] \\\\\n&= -286.1 + 146.0\\bigg[\\ln(\\mathrm{SAT}_i)\\bigg] \\\\\n\\end{split}\n\\]\nPublic\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} &= -286.1 - 8.5(1) + 146.0\\bigg[\\ln(\\mathrm{SAT}_i)\\bigg] \\\\\n&= -294.6 + 146.0\\bigg[\\ln(\\mathrm{SAT}_i)\\bigg] \\\\\n\\end{split}\n\\]\nWe can input each of these fitted equations into a geom_function() layer of our ggplot() syntax.\n\n# Plot\nggplot(data = mn, aes(x = sat, y = grad)) +\n    geom_point(alpha = 0) +\n    geom_function( # Private schools\n      fun = function(x) {-286.1 + 146.0*log(x)},\n      color = \"#6d92ee\",\n      linetype = \"solid\"\n      ) +\n  geom_function( # Public schools\n    fun = function(x) {-294.6 + 146.0*log(x)},\n    color = \"#fe932d\",\n    linetype = \"dashed\"\n    ) +\n    theme_light() +\n  xlab(\"Estimated median SAT score (in hundreds)\") +\n  ylab(\"Six-year graduation rate\")\n\n\n\n\n\n\n\nFigure 10.8: Plot of predicted graduation rate as a function of median SAT score for public (blue, solid line) and private (orange, dashed line) institutions.\n\n\n\n\n\nThe plot shows the nonlinear, diminishing positive effect of median SAT score on graduation rate for both public and private schools. For schools with lower median SAT scores, there is a larger effect on graduation rates than for schools with higher median SAT scores (for both private and public schools). Note that the curves are parallel (the result of fitting a main effects model) suggesting that the effect of median SAT score on graduation rates is the same for both public and private institutions.\nThe plot also shows the controlled effect of sector. For schools with the same median SAT score, private schools have a higher predicted graduation rate than public schools, on average. This difference is the same, regardless of median SAT score; again a result of fitting the main effects model.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Log-Transforming the Predictor</span>"
    ]
  },
  {
    "objectID": "03-03-logarithmic-transformations-predictor.html#using-information-criteria-to-adopt-a-model",
    "href": "03-03-logarithmic-transformations-predictor.html#using-information-criteria-to-adopt-a-model",
    "title": "10  Log-Transforming the Predictor",
    "section": "10.6 Using Information Criteria to Adopt a Model",
    "text": "10.6 Using Information Criteria to Adopt a Model\nWe could use evidence from the information criteria to evaluate the evidence among a set of candidate models, so long as they were fitted with the same data and use the same outcome. For example, consider the following candidate models:\n\\[\n\\begin{split}\n\\mathbf{Model~1:~}\\mathrm{Graduation~Rate}_i &= \\beta_0 + \\epsilon_i \\\\[2ex]\n\\mathbf{Model~2:~}\\mathrm{Graduation~Rate}_i &= \\beta_0 + \\beta_1\\bigg[\\ln(\\mathrm{SAT}_i)\\bigg] + \\epsilon_i \\\\[2ex]\n\\mathbf{Model~3:~}\\mathrm{Graduation~Rate}_i &= \\beta_0 + \\beta_1\\bigg[\\ln(\\mathrm{SAT}_i)\\bigg] + \\beta_2(\\mathrm{Public}_i) + \\epsilon_i\n\\end{split}\n\\]\nwhere each model has the assumptions that \\(\\epsilon_i\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\).\n\n# Fit models (use natural log of SAT)\nlm.1 = lm(grad ~ 1, data = mn)\nlm.2 = lm(grad ~ 1 + log(sat), data = mn)\nlm.3 = lm(grad ~ 1 + log(sat) + public, data = mn)\n\n# Table of model evidence\naictab(\n  cand.set = list(lm.1, lm.2, lm.3),\n  modnames = c(\"Intercept-only\", \"Ln(SAT)\", \"Ln(SAT) and Sector\")\n)\n\n\nModel selection based on AICc:\n\n                   K   AICc Delta_AICc AICcWt Cum.Wt      LL\nLn(SAT) and Sector 4 221.81       0.00   0.99   0.99 -106.19\nLn(SAT)            3 230.39       8.58   0.01   1.00 -111.78\nIntercept-only     2 283.00      61.19   0.00   1.00 -139.30\n\n\nThe empirical evidence, given the models fitted and the data, overwhelmingly supports Model 3 (the model that includes the effects of median SAT and sector on graduation rates).\n\nFYI\nNote that this test can help us evaluate whether there is an effect, but CANNOT tell us whether that effect is nonlinear, nor whether a log function is appropriate to model the effect; for that you are reliant on the residuals!",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Log-Transforming the Predictor</span>"
    ]
  },
  {
    "objectID": "03-03-logarithmic-transformations-predictor.html#footnotes",
    "href": "03-03-logarithmic-transformations-predictor.html#footnotes",
    "title": "10  Log-Transforming the Predictor",
    "section": "",
    "text": "So long as we can ultimately write this function as a linear model, namely, \\(Y_i=\\beta_0+\\beta_1(x_i) + \\epsilon_i\\).↩︎\nMathematically, the instantaneous derivative of the logarithmic function is is always positive, but it approaches 0 for larger values of x, while that for the quadratic function eventually becomes negative.↩︎\nHere 🦒 is simply a placeholder for the expression \\(2^{\\hat\\beta_0} \\times X_i^{\\hat\\beta_1}\\).↩︎\nIn order to determine this we would need more data at higher values of SAT to determine if the graduation rate gets lower or essentially planes off. The lack of data in this range is why we have this model selection uncertainty.↩︎",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Log-Transforming the Predictor</span>"
    ]
  },
  {
    "objectID": "03-04-logarithmic-transformations-outcome.html",
    "href": "03-04-logarithmic-transformations-outcome.html",
    "title": "11  Log-Transforming the Outcome",
    "section": "",
    "text": "11.1 Relationship between Wealth and Carbon Emissions\nIn this set of notes, you will learn another method for dealing with nonlinearity. Specifically, we will look at log-transforming the outcome in a linear model. This transformation can also help rectify violations of the normality and homogeneity of variance assumptions. To do so, we will use the carbon.csv dataset. (See data codebook.)\nOur analytic goal will be to explain variation in worldwide carbon emissions. Specifically:\nWithin this work, we will use information theoretic approcaches (namely the AICc and related measures) to evaluate any fitted models.\nTo being the analysis, we will examine the marginal distributions of wealth (predictor) and carbon emissions (outcome), as well as a scatterplot between them for our sample data.\n# Marginal distribution of co2 (outcome)\np1 = ggplot(data = carbon, aes(x = co2)) +\n  geom_density() +\n  theme_bw() +\n  xlab(\"Carbon emissions (metric tons per person)\") +\n  ylab(\"Probability density\")\n\n# Marginal distribution of wealth (predictor)\np2 = ggplot(data = carbon, aes(x = wealth)) +\n  geom_density() +\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"Probability density\")\n\n# Scatterplot\np3 = ggplot(data = carbon, aes(x = wealth, y = co2)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"Carbon emissions (metric tons per person)\") +\n  annotate(geom = \"text\", x = 6.4, y = 38, label = \"Quatar\", size = 3, hjust = 1) +\n  annotate(geom = \"text\", x = 4.6, y = 31.3, label = \"Trinidad and Tobago\", size = 3, hjust = 1) \n\n# Place figures side-by-side\n(p1 / p2) |  p3\n\n\n\n\n\n\n\nFigure 11.1: Density plot of the distribution of carbon dioxide emissions (LEFT) and wealth (CENTER). The scatterplot (RIGHT) shows the relationship between wealth and carbon dioxide emissions. The loess smoother (blue line) is also displayed on the scatterplot. Two countries having extreme CO2 emissions are also identified.\nThe distribution of CO2 emissions is severely right-skewed. Although the median emissions is around 2.5 metric tons per person, the plot shows evidence that several countries have high emissions. The distribution of wealth is more symmetric (perhaps a little left-skewed), with most countries having a weath level around 3.5.\nThe scatterplot suggests a positive, nonlinear relationship between wealth and CO2 emissions; countries that are wealthier also tend to have higher CO2 emissions. It also suggests that there is more variation in CO2 emissions at higher levels of wealth than at lower levels of wealth—potential violation of the homoskedasticity assumption. The curvilinear relationship between wealth and carbon emissions suggests that we may want to include a quadratic effect of wealth in the model. We fit both a linear and quadratic model and show the residual plots for both models in Figure 11.2.\n#|fig-alt: \"Density plot of the standardized residuals (LEFT) and scatterplot of the standarized residuals versus the fitted values (RIGHT) from regressing CO2 emissions on wealth in a linear model (TOP) and in a model that includes a quadratic effect of wealth (BOTTOM). The reference line of Y=0 is also displayed in the scatterplot along with the 95% confidence envelope (grey shaded area). The loess smoother (solid, blue line) shows the empirical relationship betwen the residuals and fitted values.\"\n#| fig-width: 8\n#| fig-height: 4\n#| out-width: \"80%\"\n# Fit model\nlm.1 = lm(co2 ~ 1 + wealth, data = carbon)\nlm.2 = lm(co2 ~ 1 + wealth + I(wealth^2), data = carbon)\n\n# Obtain residual plots\nresidual_plots(lm.1) / residual_plots(lm.2)\n\n\n\n\n\n\n\nFigure 11.2: Density plot of the standardized residuals (LEFT) and scatterplot of the standarized residuals versus the fitted values (RIGHT) from regressing CO2 emissions on wealth in a linear model (TOP) and in a model that includes a quadratic effect of wealth (BOTTOM). The reference line of Y=0 is also displayed in the scatterplot along with the 95% confidence envelope (grey shaded area). The loess smoother (solid, blue line) shows the empirical relationship betwen the residuals and fitted values.\nThe plots from the linear model show clear violations of almost every assumption. By including a quadratic effect of wealth in the model, we have mitigated the “linearity” assumption; the average residual is 0 in this model. However, the residual plots still suggest violations of the normality assumption and of the assumption of homoskedasticity.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Log-Transforming the Outcome</span>"
    ]
  },
  {
    "objectID": "03-04-logarithmic-transformations-outcome.html#exponential-functions-another-non-linear-function",
    "href": "03-04-logarithmic-transformations-outcome.html#exponential-functions-another-non-linear-function",
    "title": "11  Log-Transforming the Outcome",
    "section": "11.2 Exponential Functions: Another Non-Linear Function",
    "text": "11.2 Exponential Functions: Another Non-Linear Function\nWe do not have to use a quadratic function to model curvilinear relationships. We can use any mathematical function that mimics the curvilinear pattern observed in the data.1 One mathematical function that is useful for modeling curvilinearity is the exponential function. The exponential function is shown in the right-hand side of the figure below.\n\n\nCode\nfig_01 = data.frame(\n  x = seq(from = -3, to = 5, by = 0.1)\n  ) |&gt;\n  mutate(y =  (x^2))\n\np1 = ggplot(data = fig_01, aes(x = x, y = y)) +\n    geom_line() +\n    theme_light() +\n  scale_x_continuous(name = \"X\", breaks = NULL) +\n  scale_y_continuous(name = \"Y\", breaks = NULL) +\n  ggtitle(\"Quadratic Function\")\n\n\nfig_02 = data.frame(\n  x = seq(from = 0.1, to = 5, by = 0.1)\n  ) |&gt;\n  mutate(y =  2^x)\n\np2 = ggplot(data = fig_02, aes(x = x, y = y)) +\n    geom_line() +\n    theme_light() +\n  scale_x_continuous(name = \"X\", breaks = NULL) +\n  scale_y_continuous(name = \"Y\", breaks = NULL) +\n  ggtitle(\"Exponential Function\")\n\n# Layout the plots\np1 | p2\n\n\n\n\n\n\n\n\nFigure 11.3: LEFT: Negative quadratic function of X. RIGHT: Logarithmic function of X.\n\n\n\n\n\nThese two functions have some similarities. They both model non-linear growth (continuous and increasing growth). However, in the quadratic function, the parabola changes direction (y-values decrease until some x-value, at which point the y-values increase). The exponential function, on the other hand, does not change direction—it continues to grow, at ever increasing amounts.2\nTo fit an exponential function we need to choose a base for the function and then we raise the right-hand side of our fitted equation using that base. For example, if we choose the base of 2, our fitted equation would be:\n\\[\n\\hat{Y}_i = 2^{\\hat\\beta_0 + \\hat\\beta_1(X_i)}\n\\]\nUnfortunately, the lm() function does not allow us to fit this directly. There is, however, a workaround that we can employ. But, to do so, requires us to understand the relationship between exponents and logarithms.\n\n\n\n11.2.1 Quick Refresher on Exponents and Logarithms\nThe logarithm is an inverse function of an exponent. Consider this example,\n\\[\n\\log_2 (32)\n\\]\nThe base-2 logarithm of 32 is the exponent to which the base, 2 in our example, must be raised to produce 32. Mathematically it is equivalent to solving the following equation for x:\n\\[\n\\begin{split}\n2^{x} &= 32 \\\\\nx &= 5\n\\end{split}\n\\]\nThus,\n\\[\n\\log_2 (32) = 5\n\\]\nTo compute a logarithm using R, we use the log() function. We also specify the argument base=, since logarithms are unique to a particular base. For example, to compute the mathematical expression \\(\\log_2 (32)\\), we use\n\nlog(32, base = 2)\n\n[1] 5\n\n\nThere is also a shortcut function to use base-2.\n\nlog2(32)\n\n[1] 5",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Log-Transforming the Outcome</span>"
    ]
  },
  {
    "objectID": "03-04-logarithmic-transformations-outcome.html#workaround-for-lm",
    "href": "03-04-logarithmic-transformations-outcome.html#workaround-for-lm",
    "title": "11  Log-Transforming the Outcome",
    "section": "11.3 Workaround for lm()",
    "text": "11.3 Workaround for lm()\nRecall that what we want to fit is the following exponential model:\n\\[\n\\hat{Y}_i = 2^{\\hat\\beta_0 + \\hat\\beta_1(X_i)}\n\\]\nWhat we will do is take the base-2 logarithm of both sides of the equation:\n\\[\n\\log_2(\\hat{Y}_i) = \\log_2\\bigg(2^{\\hat\\beta_0 + \\hat\\beta_1(X_i)}\\bigg)\n\\]\nSince logarithms and exponents are inverse functions, the log and power on right-hand side of the equation cancel out and we are left with:\n\\[\n\\log_2(\\hat{Y}_i) = \\hat\\beta_0 + \\hat\\beta_1(X_i)\n\\]\nThat is, we can regress the log-transformed values of Y onto X.\n\n\n11.3.1 Log-Transforming Variables\nFor our purposes, we need to log-transform each value of the Y attribute, namely the CO2 values in our data. Here, we will log-transform the CO2 values (using base-2). To do this we create a new column called l2co2 using the mutate() function.\n\n# Create base-2 log-transformed CO2 values\ncarbon = carbon |&gt;\n  mutate(\n    l2co2 = log(co2, base = 2)\n    )\n\n# View data\ncarbon\n\n# A tibble: 189 × 6\n   country             region      co2 wealth urbanization  l2co2\n   &lt;chr&gt;               &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1 Afghanistan         Asia      0.254   1.07        3.35  -1.98 \n 2 Albania             Europe    1.59    3.75        1.32   0.669\n 3 Algeria             Africa    3.69    3.54        2.81   1.88 \n 4 Angola              Africa    1.12    2.71        4.31   0.163\n 5 Antigua and Barbuda Americas  5.88    4.36        0.432  2.56 \n 6 Argentina           Americas  4.41    4.49        1.15   2.14 \n 7 Armenia             Europe    1.89    3.72        0.309  0.918\n 8 Australia           Oceania  16.9     5.60        1.66   4.08 \n 9 Austria             Europe    7.75    5.82        0.836  2.95 \n10 Azerbaijan          Europe    3.7     3.79        1.47   1.89 \n# ℹ 179 more rows\n\n\nHow does the distribution of the log-transformed CO2 values compare to the distribution of raw CO2 values? We can examine the density plot of both the original and log-transformed variables to answer this.\n\n\nCode\np1 = ggplot(data = carbon, aes(x = co2)) +\n  stat_density(geom = \"line\") +\n  theme_light() +\n  xlab(\"Carbon emissions (metric tons per person)\") +\n  ylab(\"Probability density\") +\n  ggtitle(\"Raw values of predictor\")\n\np2 = ggplot(data = carbon, aes(x = l2co2)) +\n  stat_density(geom = \"line\") +\n  theme_light() +\n  xlab(\"Log2 (Carbon emissions)\") +\n  ylab(\"Probability density\") +\n  ggtitle(\"Log-2 transformed carbon emissions\")\n\n# Layout the plots\np1 | p2\n\n\n\n\n\n\n\n\nFigure 11.4: Density plot of the raw CO2 values (LEFT) and the base-2 log-transformed CO2 (RIGHT).\n\n\n\n\n\nComparing the shapes of the two distributions, we see that:\n\nThe original CO2 variable was right-skewed. The log-transformed variable is more symmetric. (It is perhaps even a bit left-skewed, although certainly closer to symmetric than the original.)\nThe scale is quite different between the two variables (one is, after all, log-transformed). This has greatly affected the center (mean) and the variation. Importantly, after log-transforming, the variation in the distribution is much smaller.\n\n\nFYI\nLogarithmic transformations change the shape, center, and variation of a distribution! For modelers, they take (1) make a right-skewed variable more symmetric and (2) reduce the variation in the distribution.\n\n\n\n\n11.3.2 Relationship between Log-Transformed CO2 Emissions and Wealth\nLet’s examine the relationship between the log-transformed CO2 emissions and wealth.\n\n# Scatterplot\nggplot(data = carbon, aes(x = wealth, y = l2co2)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"Log2(Carbon emissions)\") +\n  annotate(geom = \"text\", x = 6.4, y = 5.25, label = \"Quatar\", size = 3, hjust = 1) +\n  annotate(geom = \"text\", x = 4.6, y = 4.97, label = \"Trinidad and Tobago\", size = 3, hjust = 1) \n\n\n\n\n\n\n\nFigure 11.5: Scatterplot between wealth and log base-2 transformed CO2 emissions. The loess smoother (blue, dashed line) is also displayed. The two countries with extreme carbon emissions are also identified on this plot.\n\n\n\n\n\nLog-transforming the outcome has drastically affected the scale for the outcome. The relationship between wealth and the log-transformed CO2 emissions is much more linear (although not perfect). The two countries which had extremely high CO2 emissions when we examined raw CO2, no longer seems like outliers in the transformed data. The differences in variation of log-carbon emissions between lower wealth and higher wealth countries also seems less severe.\nHas this helped us better meet the distributional assumptions for the regression model? To find out, we will fit the model using the log-transformed carbon emissions and examine the residual plots.\n\n# Fit model\nlm.log2 = lm(l2co2 ~ 1 + wealth, data = carbon)\n\n# Obtain residual plots\nresidual_plots(lm.log2)\n\n\n\n\n\n\n\nFigure 11.6: Density plot of the standardized residuals (LEFT) and scatterplot of the standarized residuals versus the fitted values (RIGHT) from regressing log-transformed CO2 emisssions on wealth. The reference line of Y=0 is also displayed along with the 95% confidence envelope (grey shaded area). The loess smoother (solid, blue line) shows the empirical relationship betwen the residuals and fitted values.\n\n\n\n\n\nThese plots suggest that after the transforming the outcome, there is a great deal of improvement in meeting the assumption of normality. The assumption of homoskedasticity also looks markedly improved. There still seems to be violations of the linearity assumption.\n\nFYI\nFor now, we will proceed so you understand how to interpret coefficients from models where the outcome has been log-transformed, but we will come back and fix this nonlinearity later.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Log-Transforming the Outcome</span>"
    ]
  },
  {
    "objectID": "03-04-logarithmic-transformations-outcome.html#interpreting-the-regression-output",
    "href": "03-04-logarithmic-transformations-outcome.html#interpreting-the-regression-output",
    "title": "11  Log-Transforming the Outcome",
    "section": "11.4 Interpreting the Regression Output",
    "text": "11.4 Interpreting the Regression Output\nWe will now examine the model- and coefficient-level output from the model.\n\n# Model-level output\nglance(lm.log2)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.838         0.837 0.866      955. 5.60e-75     1  -237.  481.  490.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe model-level summary information suggests that differences in wealth explains 83.8% of the variation in log-transformed carbon dioxide emissions. Since differences in \\(\\log_2(\\mathrm{CO2})\\) imply that there are differences in the raw CO2 values, we would typically just say that “differences in wealth explain 83.8% of the variation in carbon dioxide emissions.” That is, we interpret the \\(R^2\\) value as if we hadn’t used any log-transformation at all!\n\n# Coefficients\ncoef(lm.log2)\n\n(Intercept)      wealth \n  -3.168922    1.188289 \n\n\nFrom the coefficient-level output, the fitted equation is:\n\\[\n\\log_2\\left(\\widehat{\\mathrm{CO2}_i}\\right) = -3.17 + 1.19(\\mathrm{Wealth}_i)\n\\]\nWe can interpret the coefficients as we always do, recognizing that these interpretation are based on the log-transformed outcome.\n\nThe intercept value of \\(-3.17\\) is the predicted log-2 transformed carbon dioxide emissions for countries with a wealth level of 0 (extrapolation).\nThe slope, \\(\\hat{\\beta_1} = 1.19\\), indicates that each one-unit difference in the wealth measure is associated with a 1.19-unit change in the log-2 carbon dioxide emission, on average.\n\n\n\n11.4.1 Better Interpretations: Back-transforming to the Raw Metric\nWhile these interpretations are technically correct, it is more helpful to your readers (and more conventional) to interpret any regression results in the raw metric of the variable rather than log-transformed metric. This means we have to back-transform the interpretations. To think about how to do this, we first consider a more general expression of the fitted linear model:\n\\[\n\\log_2\\left(\\hat{Y}_i\\right) = \\hat\\beta_0 + \\hat\\beta_1(X_{i})\n\\]\nThe left-hand side of the equation is in the log-transformed metric, which drives our interpretations. If we want to instead, interpret using the raw metric of Y, we need to back-transform from \\(\\log_2(Y)\\) to Y. To back-transform, we use the inverse function, which is to exponentiate using the base of the logarithm, in our case, base-2.\n\\[\n2^{\\log_2(Y_i)} = Y_i\n\\]\nIf we exponentiate the left-hand side of the equation, to maintain the equality, we also need to exponentiate the right-hand side of the equation.\n\\[\n2^{\\log_2(Y_i)} = 2^{\\hat\\beta_0 + \\hat\\beta_1(X_{i})}\n\\]\nThen we use rules of exponents to simplify this.\n\\[\nY_i = 2^{\\hat\\beta_0} \\times 2^{\\hat\\beta_1(X_{i})}\n\\]\nFor our example, we exponentiate both sides of the fitted equation to get the following back-transformed fitted equation:\n\\[\n\\widehat{\\mathrm{CO2}_i} = 2^{-3.17} \\times 2^{1.19(\\mathrm{Wealth}_i)}\n\\]\n\n\n\n11.4.2 Substituting in Values for Wealth to Interpret Effects\nTo interpret the back-transformed effects, we can substitute in the different values for wealth and solve. For example when wealth = 0:\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= 2^{-3.17} \\times 2^{1.19(0)}\\\\\n&= 0.111 \\times 1 \\\\\n&= 0.111\n\\end{split}\n\\]\nIn R, we use the ^ operator to exponentiate. In our example, to compute the predicted caron dioxide emissions, we use the following:\n\n# Compute predicted CO2 when wealth=0\n2^(-3.17) * 2^(1.19 * 0)\n\n[1] 0.1111053\n\n\nThe predicted CO2 emissions for a countries that have a wealth level of 0 is 0.111 metric tons per person, on average (extrapolation). This is how we interpret the intercept! How about countries that have a wealth level of 1 (a one-unit difference in wealth from countries that have a wealth level of 0)?\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= 2^{-3.17} \\times 2^{1.19(1)}\\\\\n&= 0.111 \\times 2.28 \\\\\n&= 0.253\n\\end{split}\n\\]\nThe predicted CO2 emission for a countries that have a wealth level of 1 is 0.253 metric tons per person. This is 2.28 TIMES the emissions of countries that have a wealth level of 0. How about countries that have a wealth level of 2 (a one-unit difference in wealth from countries that have a wealth level of 1)?\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= 2^{-3.17} \\times 2^{1.19(2)}\\\\\n&= 0.111 \\times 5.21 \\\\\n&= 0.578\n\\end{split}\n\\]\nUsing another rule of exponents we could re-write this last fitted equation as:\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= 2^{-3.17} \\times (2^{1.19})^2\\\\\n&= 0.111 \\times 2.28^2 \\\\\n&= 0.111 \\times 2.28 \\times 2.28 \\\\\n&= 0.578\n\\end{split}\n\\]\nThis is 2.28 TIMES the emissions of countries that have a wealth level of 1. Each 1-unit increase in wealth is associated with an increase in CO2 emissions of a factor of 2.28 (i.e., 2.28 times higher CO2 emissions). The technical language we use to express “2.28 times” is a “2.28-fold difference”. So we would conventionally interpret this as:\n\nEach one-unit difference in wealth level is associated with a 2.28-fold difference in carbon emissions, on average.\n\nTo summarize, when we back-transform from interpretations of log-Y to Y the rate-of-change is multiplicative rather than additive. And to determine the multiplicative rate-of-change, we take the base we chose to the power of the estimated slope, that is:\n\\[\n\\text{Rate-of-Change} = \\text{base}^{\\hat\\beta_1}\n\\]\nA computational shortcut to obtain the back-transformed interpretations is to exponentiate the output of the coef() function (which returns the coefficients from the fitted model).\n\n# Obtain back-transformed interpretations\n2 ^ coef(lm.log2)\n\n(Intercept)      wealth \n  0.1111884   2.2788237 \n\n\n\n\n\n11.4.3 Interpretation of the Rate-of-Change as Percent Change\nThe results of log-transformed models are often interpreted as percent change. Rather than saying that each one-unit difference in wealth is associated with a 2.28-fold difference in carbon emissions, on average. We can also interpret this change as a percent change. To do this, we compute:\n\\[\n\\mathrm{Percent~Change} = (2^{\\hat{\\beta_1}} -1) \\times 100\n\\]\nIn our example,\n\\[\n\\begin{split}\n\\mathrm{Percent~Change} &= (2^{1.19} -1) \\times 100 \\\\[1ex]\n&= 128.15\n\\end{split}\n\\]\nThus the interpretation is:\n\nEach one-unit difference in wealth is associated with a 128% change in carbon emissions, on average.\n\n\nWARNING\nBe very careful when you are working with and interpreting using percents. The phrases “percent change” and “percentage point change” are two very different things. For example, increasing a graduation rate from 50% to 60% represents an increase of 10 percentage points, but it is a 20% increase or a 120% change!\n\n\n\n\n11.4.4 Plotting the Fitted Model\nAs always, we should plot the fitted model to aid in interpretation. To do this we will use the back-transformed expression of the fitted equation:\n\\[\n\\widehat{\\mathrm{CO2}_i} = 2^{-3.17} \\times 2^{1.19(\\mathrm{Wealth}_i)}\n\\]\nThis can be plotted by using the geom_function() layer of ggplot().\n\n# Plot\nggplot(data = carbon, aes(x = wealth, y = co2)) +\n  geom_point(alpha = 0) +\n  geom_function(fun = function(x) {2^(-3.17) * 2^(1.19*x)} ) +\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"Predicted CO2 emissions (in metric tones per person)\")\n\n\n\n\n\n\n\nFigure 11.7: Plot of predicted CO2 emisssions as a function of wealth.\n\n\n\n\n\nBased on this plot, we see a non-linear, positive effect of wealth on carbon emissions. Wealthier countries tend to have higher carbon emissions, on average, but the increase in carbon emissions as countries get wealthier is not constant; it is exponentially increasing.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Log-Transforming the Outcome</span>"
    ]
  },
  {
    "objectID": "03-04-logarithmic-transformations-outcome.html#using-a-different-base",
    "href": "03-04-logarithmic-transformations-outcome.html#using-a-different-base",
    "title": "11  Log-Transforming the Outcome",
    "section": "11.5 Using a Different Base",
    "text": "11.5 Using a Different Base\nThe choice of base-2 for our logarithm was arbitrary. We could have chosen any base. For example, another common base to use is base-10. Now the fitted equation will be represented as:\n\\[\n\\hat{Y}_i = 10^{\\hat\\beta_0 + \\hat\\beta_1(X_i)}\n\\]\nHere we take the base-10 logarithm of both sides of the equation:\n\\[\n\\begin{split}\n\\log_{10}(\\hat{Y}_i) &= \\log_{10}\\bigg(10^{\\hat\\beta_0 + \\hat\\beta_1(X_i)}\\bigg) \\\\[2ex]\n&= \\hat\\beta_0 + \\hat\\beta_1(X_i)\n\\end{split}\n\\]\nThat is, we can regress the base-10 log-transformed values of Y onto X. To do this, we initially create a new column in the data that includes the base-10 logs of the carbon dioxide emission values. We can then use those as the outcome in our lm().\n\n# Create base-10 log-transformed CO2 values\ncarbon = carbon |&gt;\n  mutate(\n    l10co2 = log(co2, base = 10)\n    )\n\n# View data\ncarbon\n\n# A tibble: 189 × 7\n   country             region      co2 wealth urbanization  l2co2  l10co2\n   &lt;chr&gt;               &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 Afghanistan         Asia      0.254   1.07        3.35  -1.98  -0.595 \n 2 Albania             Europe    1.59    3.75        1.32   0.669  0.201 \n 3 Algeria             Africa    3.69    3.54        2.81   1.88   0.567 \n 4 Angola              Africa    1.12    2.71        4.31   0.163  0.0492\n 5 Antigua and Barbuda Americas  5.88    4.36        0.432  2.56   0.769 \n 6 Argentina           Americas  4.41    4.49        1.15   2.14   0.644 \n 7 Armenia             Europe    1.89    3.72        0.309  0.918  0.276 \n 8 Australia           Oceania  16.9     5.60        1.66   4.08   1.23  \n 9 Austria             Europe    7.75    5.82        0.836  2.95   0.889 \n10 Azerbaijan          Europe    3.7     3.79        1.47   1.89   0.568 \n# ℹ 179 more rows\n\n# Fit model\nlm.log10 = lm(l10co2 ~ 1 + wealth, data = carbon)\n\n# Model-level output\nglance(lm.log10)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.838         0.837 0.261      955. 5.60e-75     1  -12.9  31.7  41.4\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe model-level output is identical to the model-level output from the base-2 model! That is differences in wealth explains 83.8% of the variation in CO2 emissions.\n\n# Coefficients\ncoef(lm.log10)\n\n(Intercept)      wealth \n -0.9539406   0.3577107 \n\n\nBecause we changed the base, the coefficient estimates for the fitted equation have changed. Writing the new fitted equation:\n\\[\n\\log_{10}(\\widehat{\\mathrm{CO2}_i}) = -0.954 + 0.358(\\mathrm{Wealth}_i)\n\\]\nOr, back-transforming the left-hand side:\n\\[\n\\widehat{\\mathrm{CO2}_i} = 10^{-0.954} \\times 10^{0.358(\\mathrm{Wealth}_i)}\n\\]\nTo interpret these, we exponentiate the coefficients using base-10.\n\n10 ^ coef(lm.log10)\n\n(Intercept)      wealth \n  0.1111884   2.2788237 \n\n\nWe get the exact same back-transformed coefficients as we did when we back-transformed the base-2 coefficients! Based on these, the interpretations are:\n\nIntercept: The predicted CO2 emissions for a countries that have a wealth level of 0 is 0.111 metric tons per person, on average (extrapolation).\nRate-of-Change: Each one-unit difference in wealth level is associated with a 2.28-fold difference in carbon emissions, on average. OR using the language of percent change, each one-unit difference in wealth level is associated with a 128% increase in carbon emissions, on average.\n\nLastly, we can plot the fitted curve.\n\n# Plot\nggplot(data = carbon, aes(x = wealth, y = co2)) +\n  geom_point(alpha = 0) +\n  geom_function(fun = function(x) {10^(-0.954) * 10^(0.358*x)} ) +\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"Predicted CO2 emissions (in metric tones per person)\")\n\n\n\n\n\n\n\nFigure 11.8: Plot of predicted CO2 emisssions as a function of wealth.\n\n\n\n\n\nThis is the exact same fitted curve we obtained from the base-2 model.\n\nFYI\nIt isn’t shown here, but because the fitted curve is exactly the same (i.e., the Y-hat values are the same), the residuals are exactly the same for the base-10 and base-2 models. This implies the residual plots will be identical as well. Changing the base does NOT change the fit of the model!\nChanging the base does not change any of the information we get from the model. The model-level output is the same regardless of base. While the fitted equation changes depending on the base chosen, the back-transformed interpretations of the coefficients and the plot of the fitted curve are the same. Moreover the residuals and therefore your evaluations of the assumptions will be the same regardless of base!",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Log-Transforming the Outcome</span>"
    ]
  },
  {
    "objectID": "03-04-logarithmic-transformations-outcome.html#one-more-base-base-e",
    "href": "03-04-logarithmic-transformations-outcome.html#one-more-base-base-e",
    "title": "11  Log-Transforming the Outcome",
    "section": "11.6 One More Base: Base-e",
    "text": "11.6 One More Base: Base-e\nOne base that is commonly used for log-transformations in the social sciences is base-e. e is a mathematical constant (Euler’s number) that is approximately equal to 2.71828.3 We can obtain this by using the exp() function in R. This function takes e to some exponent that is given as the argument. So to obtain the approximation of e we use\n\nexp(1)\n\n[1] 2.718282\n\n\nThe base-e logarithm for a number, referred to as the natural logarithm, can be obtained using the log() function with the argument base=exp(1). However, this base is so commonly used that it is the default value for the base= argument. So, if we use the log() function without defining the base= argument, it will automatically use base-e. For example, the natural logarithm of 10 can be computed as\n\nlog(10)\n\n[1] 2.302585\n\n\nIf we took \\(e^{2.303}\\) we would obtain 10. The natural logarithm even has its own mathematical notation; \\(\\ln\\). For example, we would mathematically express the natural logarithm of 10 as\n\\[\n\\ln (10) = 2.303\n\\]\n\n\n11.6.1 Fitting the Model Using Base-e Logarithms\nThe fitted equation will be represented as:\n\\[\n\\hat{Y}_i = e^{\\hat\\beta_0 + \\hat\\beta_1(X_i)}\n\\]\nHere we take the natural logarithm of both sides of the equation:\n\\[\n\\begin{split}\n\\ln(\\hat{Y}_i) &= \\ln\\bigg(e^{\\hat\\beta_0 + \\hat\\beta_1(X_i)}\\bigg) \\\\[2ex]\n&= \\hat\\beta_0 + \\hat\\beta_1(X_i)\n\\end{split}\n\\]\nThat is, we can regress the natural log-transformed values of Y onto X. To do this, we initially create a new column in the data that includes the natural logs of the carbon dioxide emission values. We can then use those as the outcome in our lm().\n\n# Create natural log-transformed CO2 values\ncarbon = carbon |&gt;\n  mutate(\n    lnco2 = log(co2)\n    )\n\n# View data\ncarbon\n\n# A tibble: 189 × 8\n   country             region      co2 wealth urbanization  l2co2  l10co2  lnco2\n   &lt;chr&gt;               &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 Afghanistan         Asia      0.254   1.07        3.35  -1.98  -0.595  -1.37 \n 2 Albania             Europe    1.59    3.75        1.32   0.669  0.201   0.464\n 3 Algeria             Africa    3.69    3.54        2.81   1.88   0.567   1.31 \n 4 Angola              Africa    1.12    2.71        4.31   0.163  0.0492  0.113\n 5 Antigua and Barbuda Americas  5.88    4.36        0.432  2.56   0.769   1.77 \n 6 Argentina           Americas  4.41    4.49        1.15   2.14   0.644   1.48 \n 7 Armenia             Europe    1.89    3.72        0.309  0.918  0.276   0.637\n 8 Australia           Oceania  16.9     5.60        1.66   4.08   1.23    2.83 \n 9 Austria             Europe    7.75    5.82        0.836  2.95   0.889   2.05 \n10 Azerbaijan          Europe    3.7     3.79        1.47   1.89   0.568   1.31 \n# ℹ 179 more rows\n\n# Fit model\nlm.ln = lm(lnco2 ~ 1 + wealth, data = carbon)\n\n# Model-level output\nglance(lm.ln)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.838         0.837 0.600      955. 5.60e-75     1  -169.  344.  353.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe model-level output is identical to the model-level output from the base-2 and base-10 model, which we expected. That is differences in wealth explains 83.8% of the variation in CO2 emissions.\n\n# Coefficients\ncoef(lm.ln)\n\n(Intercept)      wealth \n -2.1965295   0.8236594 \n\n\nBecause we changed the base, the coefficient estimates for the fitted equation have changed. Writing the new fitted equation:\n\\[\n\\ln(\\widehat{\\mathrm{CO2}_i}) = -2.20 + 0.824(\\mathrm{Wealth}_i)\n\\]\nOr, back-transforming the left-hand side:\n\\[\n\\widehat{\\mathrm{CO2}_i} = e^{2.20} \\times e^{0.824(\\mathrm{Wealth}_i)}\n\\]\nTo interpret these, we exponentiate the coefficients using base-e.\n\n# Exponentiate using base-e\nexp(coef(lm.ln))\n\n(Intercept)      wealth \n  0.1111884   2.2788237 \n\n\nWe get the exact same back-transformed coefficients as we did when we back-transformed the base-2 coefficients. Again, this is expected. Based on these, the interpretations are:\n\nIntercept: The predicted CO2 emissions for a countries that have a wealth level of 0 is 0.111 metric tons per person, on average (extrapolation).\nRate-of-Change: Each one-unit difference in wealth level is associated with a 2.28-fold difference in carbon emissions, on average. OR using the language of percent change, each one-unit difference in wealth level is associated with a 128% increase in carbon emissions, on average.\n\nLastly, we can plot the fitted curve.\n\n# Plot\nggplot(data = carbon, aes(x = wealth, y = co2)) +\n  geom_point(alpha = 0) +\n  geom_function(fun = function(x) {exp(-2.20) * exp(0.824*x)} ) +\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"Predicted CO2 emissions (in metric tones per person)\")\n\n\n\n\n\n\n\nFigure 11.9: Plot of predicted CO2 emisssions as a function of wealth.\n\n\n\n\n\nThis is the exact same fitted curve we obtained from the base-2 and base-10 models. Once again, this is completely expected.\n\n\n11.6.2 What is Special About Base-e\nOther than being a cool number, why do Social Scientists like using base-e? It turns out that if the slope estimate is small (e.g., \\(\\hat\\beta_k &lt; 0.20\\)), we can directly take the slope value from the coef() output, multiply it by 100, and interpret it as the percent change. That is because for small values of the slope,\n\\[\n(e^{\\hat{\\beta_1}} -1) \\approx \\hat{\\beta_1}\n\\]\nIn our example, the slope is not less than 0.20, so we have to do the math, \\((e^{0.824}-1)\\times 100=128\\), to compute the percent change.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Log-Transforming the Outcome</span>"
    ]
  },
  {
    "objectID": "03-04-logarithmic-transformations-outcome.html#modeling-the-remaining-non-linearity",
    "href": "03-04-logarithmic-transformations-outcome.html#modeling-the-remaining-non-linearity",
    "title": "11  Log-Transforming the Outcome",
    "section": "11.7 Modeling the Remaining Non-Linearity",
    "text": "11.7 Modeling the Remaining Non-Linearity\nRemember that the residual plots suggested that the problem of heterogeneity in the residuals was “fixed” by using the log-transformed outcome, but it did not completely fix the non-linearity. We can also see this by plotting the log-transformed CO2 emisssions versus wealth.\n\n# Scatterplot\nggplot(data = carbon, aes(x = wealth, y = log(co2))) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"ln(Carbon emissions)\") +\n  annotate(geom = \"text\", x = 6.4, y = 3.64, label = \"Quatar\", size = 3, hjust = 1) +\n  annotate(geom = \"text\", x = 4.6, y = 3.44, label = \"Trinidad and Tobago\", size = 3, hjust = 1)\n\n\n\n\n\n\n\nFigure 11.10: Scatterplot between wealth and log-transformed CO2 emissions. The loess smoother (blue, dashed line) is also displayed. The two countries with extreme carbon emissions are also identified on this plot.\n\n\n\n\n\nHow do we model the non-linearity we observe in the relationship even after we log-transformed the outcome? The pattern shown by the loess smoother in the plot, suggests a curvilinear pattern similar to what we observed in the college data in previous sets of notes. This means we can try to model the non-linearity by including a polynomial effect of wealth in the model.\nTo see if this improves our assumptions we will fit the polynomial model and examine the residuals. Here we fit the model by creating the natural logarithm of the CO2 values directly in the lm(). (If we do that, we don’t have to create a new column of log-transfomed values in the data!)\n\n# Fit polynomial model\nlm.poly = lm(log(co2) ~ 1 + wealth + I(wealth^2), data = carbon)\n\n# Residual plots\nresidual_plots(lm.poly)\n\n\n\n\n\n\n\nFigure 11.11: Density plots of the standardized residuals and scatterplot of the standardized residuals versus the fitted values for the model using the linear and quadratic effects of wealth to predict variation in log-transfomed carbon emissions. The reference line of Y=0 is also displayed along with the 95% confidence envelope (grey shaded area) in the scatterplot. The loess smoother (solid, blue line) shows the empirical relationship betwen the residuals and fitted values.\n\n\n\n\n\nBased on the residual plots, the polynomial model shows good fit to all the assumptions. Although this is reason enough to adopt the model that includes the quadratic term, we can also evaluate this by computing and comparing the AICc (and associated information theoretic metrics) for this model and the model that only includes the linear effect of wealth.\nBecause the aictab() function requires us to use the exact same outcome (computed in exactly the same way), we will refit the linear and quadratic models by computing the natural log of CO2 emissions directly in the lm().\n\n# Fit linear model\nlm.ln = lm(log(co2) ~ 1 + wealth, data = carbon)\n\n# Fit polynomial model\nlm.poly = lm(log(co2) ~ 1 + wealth + I(wealth^2), data = carbon)\n\n# Table of model evidence\naictab(\n  cand.set = list(lm.ln, lm.poly),\n  modnames = c(\"Linear effect\", \"Quadratic effect\")\n)\n\n\nModel selection based on AICc:\n\n                 K   AICc Delta_AICc AICcWt Cum.Wt      LL\nQuadratic effect 4 313.80       0.00      1      1 -152.79\nLinear effect    3 343.78      29.99      0      1 -168.83\n\n\nThe empirical evidence supports adopting the polynomial model that includes both the quadratic and linear effects of wealth.\n\nglance(lm.poly)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.863         0.862 0.552      581. 3.05e-80     2  -153.  314.  327.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nExamining the model-level summary information from the quadratic polynomial model, we find that the model explains 86.3% of the variation in CO2 emisssions.\n\n# Coefficients\ncoef(lm.poly)\n\n(Intercept)      wealth I(wealth^2) \n -2.8993795   1.3823852  -0.0836962 \n\n\nFrom the coefficient-level output, the fitted equation is:\n\\[\n\\ln\\left(\\widehat{\\mathrm{CO2}_i}\\right) = -2.90 + 1.38(\\mathrm{Wealth}_i) - 0.0837(\\mathrm{Wealth}_i^2)\n\\]\n\n\n11.7.1 Interpreting the Effect of Wealth\nSince the quadratic term is an interaction term, we interpret the effect of wealth generally as:\n\nINTERPRETATION\nThe effect of wealth on carbon dioxide emissions depends on the level of wealth.\n\nTo better understand the nature of this relationship we plot the fitted curve. To do so, we first back-transform the metric of log-carbon emission to the metric of raw carbon emissions. Remember, this creates a multiplicative relationship among the exponentiated coefficients. Exponentiating both sides of the fitted equation:\n\\[\n\\widehat{\\mathrm{CO2}_i} = e^{-2.90} \\times e^{1.38(\\mathrm{Wealth}_i)} \\times e^{-0.0837(\\mathrm{Wealth}_i^2)}\n\\]\nPlotting this function will allow us to understand the relationship between wealth and carbon emissions from the polynomial model. Inputting this into the geom_function() layer of our ggplot() syntax, we get:\n\n# Plot\nggplot(data = carbon, aes(x = wealth, y = co2)) +\n  geom_point(alpha = 0) +\n  geom_function(fun = function(x) {exp(-2.90) * exp(1.38*x) * exp(-0.0837*x^2)} ) +\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"Predicted CO2 (in metric tons per person)\")\n\n\n\n\n\n\n\nFigure 11.12: Plot of predicted carbon emisssions as a function of wealth for the quadratic polynomial model.\n\n\n\n\n\n\nAnswering the First Research Question\nThe relationship between wealth and carbon emisssions is complicated. It combines the positive exponential growth we saw earlier with the change in curvature of the negative quadratic effect. In general wealthier countries have exponentially increasing CO2 emissions that begins to diminish for countries with exceptionally high wealth levels.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Log-Transforming the Outcome</span>"
    ]
  },
  {
    "objectID": "03-04-logarithmic-transformations-outcome.html#rq-2-does-the-quadratic-effect-of-wealth-persist-after-controlling-for-urbanization",
    "href": "03-04-logarithmic-transformations-outcome.html#rq-2-does-the-quadratic-effect-of-wealth-persist-after-controlling-for-urbanization",
    "title": "11  Log-Transforming the Outcome",
    "section": "11.8 RQ 2: Does the quadratic effect of wealth persist after controlling for urbanization?",
    "text": "11.8 RQ 2: Does the quadratic effect of wealth persist after controlling for urbanization?\nTo answer the second research we will include urbanization in the model as a covariate. That is, we will fit the model:\n\\[\n\\ln\\left(\\mathrm{CO2}_i\\right) = \\beta_0 + \\beta_1(\\mathrm{Wealth}_i) + \\beta_2(\\mathrm{Wealth}_i^2) + \\beta_3(\\mathrm{Urbanization_i}) + \\epsilon_i\n\\]\nwhere \\(\\epsilon_i\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\).\n\n# Fit model\nlm.3 = lm(log(co2) ~ 1 + wealth + I(wealth^2) + urbanization, data = carbon)\n\n# Model-level output\nglance(lm.3)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.861         0.859 0.555      375. 2.30e-77     3  -152.  313.  329.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThis model explains 86.1% of the variation in carbon emissions. We also look at the model evidence.\n\n# Table of model evidence\naictab(\n  cand.set = list(lm.ln, lm.poly, lm.3),\n  modnames = c(\"Wealth\", \"Wealth, Wealth^2\", \"Wealth, Wealth^2, Urbanization\")\n)\n\n\nModel selection based on AICc:\n\n                               K   AICc Delta_AICc AICcWt Cum.Wt      LL\nWealth, Wealth^2, Urbanization 5 313.70       0.00   0.51   0.51 -151.68\nWealth, Wealth^2               4 313.80       0.10   0.49   1.00 -152.79\nWealth                         3 343.78      30.09   0.00   1.00 -168.83\n\n\nThe model that included urbanization has more empirical evidence than the model that doesn’t, but the \\(\\Delta\\)AICc values and model probabilities suggest that both models are reasonable candidates.\n\n# Coefficients\ncoef(lm.3)\n\n (Intercept)       wealth  I(wealth^2) urbanization \n -2.73862600   1.34091626  -0.08032407  -0.03308582 \n\n\nFrom the coefficient-level output, the fitted equation is:\n\\[\n\\ln\\left(\\widehat{\\mathrm{CO2}_i}\\right) = -2.74 + 1.34(\\mathrm{Wealth}_i) -0.08(\\mathrm{Wealth}_i^2) - 0.03(\\mathrm{Urbanization}_i)\n\\]\nInterpreting these in the log-metric:\n\nThe intercept, \\(\\hat{\\beta_0} = -2.74\\), is the average predicted natural log of CO2 emissions for countries with a wealth level of 0 and an urbanization of 0 (extrapolation).\nThe linear slope of wealth would not be interpreted because it is part of an interaction term.\nThe effect of wealth on carbon emissions depends on the level of wealth, after controlling for differences in urbanization.\nThe urbanization effect indicates that each one-point difference in the urbanization measure is associated with lower natural log values of CO2 emissions by \\(0.03\\), on average, after controlling for differences in wealth.\n\nBack-transforming the fitted equation, we get:\n\\[\n\\widehat{\\mathrm{CO2}_i} = e^{-2.74} \\times e^{1.34(\\mathrm{Wealth}_i)} \\times e^{-0.08(\\mathrm{Wealth}_i^2)} \\times e^{- 0.03(\\mathrm{Urbanization}_i)}\n\\]\nIf we exponentiate the coefficients:\n\nexp(coef(lm.3))\n\n (Intercept)       wealth  I(wealth^2) urbanization \n  0.06465913   3.82254435   0.92281724   0.96745553 \n\n\nInterpreting these :\n\nThe average predicted CO2 emissions for countries with a wealth level of 0 and an urbanization of 0 is .064 tonnes (extrapolation).\nThe linear slope of wealth would not be interpreted because it is part of an interaction term.\nThe effect of wealth on carbon emissions depends on the level of wealth, after controlling for differences in urbanization.\nEach one-point difference in the urbanization measure is associated with a 0.96-fold change in CO2 emissions, on average, after controlling for differences in wealth. OR Each one-point difference in the urbanization measure is associated with a 3.25% decrease in CO2 emissions, on average, after controlling for differences in wealth.\n\nPlotting this function will allow us to understand the relationship between wealth, urbanization, and carbon emissions from the polynomial model. Here I choose a low level of urbanization (0.8) and a high level of urbanization (6.2) based on the data. We substitute these into the fitted equation to get an equation expressing carbon dioxide emissions as a function of wealth which we will then input into a geom_function() layer of our ggplot() syntax.\nLow Urbanization\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= e^{-2.74} \\times e^{1.34(\\mathrm{Wealth}_i)} \\times e^{-0.08(\\mathrm{Wealth}_i^2)} \\times e^{- 0.03(0.8)} \\\\[2ex]\n&= .065 \\times e^{1.34(\\mathrm{Wealth}_i)} \\times e^{-0.08(\\mathrm{Wealth}_i^2)} \\times .976 \\\\[2ex]\n&= .0634 \\times e^{1.34(\\mathrm{Wealth}_i)} \\times e^{-0.08(\\mathrm{Wealth}_i^2)}\n\\end{split}\n\\]\nHigh Urbanization\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= e^{-2.74} \\times e^{1.34(\\mathrm{Wealth}_i)} \\times e^{-0.08(\\mathrm{Wealth}_i^2)} \\times e^{- 0.03(6.2)} \\\\[2ex]\n&= .065 \\times e^{1.34(\\mathrm{Wealth}_i)} \\times e^{-0.08(\\mathrm{Wealth}_i^2)} \\times .830 \\\\[2ex]\n&= .054 \\times e^{1.34(\\mathrm{Wealth}_i)} \\times e^{-0.08(\\mathrm{Wealth}_i^2)}\n\\end{split}\n\\]\n\n# Plot\nggplot(data = carbon, aes(x = wealth, y = co2)) +\n  geom_point(alpha = 0) +\n  geom_function(fun = function(x) {.0634 * exp(1.34*x) * exp(-0.08*x^2)}, color = \"#56b4e9\") +\n  geom_function(fun = function(x) {.054 * exp(1.34*x) * exp(-0.08*x^2)}, color = \"#0072b2\") +\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"Predicted CO2 (in metric tons per person)\")\n\n\n\n\n\n\n\nFigure 11.13: Plot of predicted carbon emisssions as a function of wealth for the quadratic polynomial model shown for countries with low (light blue) and high (dark blue) urbanization.\n\n\n\n\n\n\nAnswering the Second Research Question\nHere we see that in general wealthier countries have exponentially increasing CO2 emissions that begins to diminish for countries with exceptionally high wealth levels. This pattern is similar for countries with both low and high urbanization, although the countries with higher urbanization have lower carbon dioxide emissions.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Log-Transforming the Outcome</span>"
    ]
  },
  {
    "objectID": "03-04-logarithmic-transformations-outcome.html#rq-3-does-the-effect-of-wealth-vary-between-world-regions",
    "href": "03-04-logarithmic-transformations-outcome.html#rq-3-does-the-effect-of-wealth-vary-between-world-regions",
    "title": "11  Log-Transforming the Outcome",
    "section": "11.9 RQ 3: Does the Effect of Wealth Vary Between World Regions?",
    "text": "11.9 RQ 3: Does the Effect of Wealth Vary Between World Regions?\nTo answer the third research question, we will include world region and urbanization into the model along with our linear and quadratic effects of wealth. To determine if the effects of wealth are different across the different regions we have to include interaction effects between wealth and world region. Since our model includes a quadratic effect of wealth we need to include both:\n\n\\(\\mathrm{Wealth} \\times \\mathrm{World~Region}\\), and\n\\(\\mathrm{Wealth}^2 \\times \\mathrm{World~Region}\\)\n\nSince world region is categorical, we will need to create a set of dummy variables to represent it in the models.\n\n# Create dummy variables for world region\ncarbon = carbon |&gt;\n  mutate(\n    africa = if_else(region == \"Africa\", 1, 0),\n    asia = if_else(region == \"Asia\", 1, 0),\n    americas = if_else(region == \"Americas\", 1, 0),\n    europe = if_else(region == \"Europe\", 1, 0),\n    oceania = if_else(region == \"Oceania\", 1, 0),\n  )\n\n# View data\ncarbon\n\n# A tibble: 189 × 13\n   country  region    co2 wealth urbanization  l2co2  l10co2  lnco2 africa  asia\n   &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 Afghani… Asia    0.254   1.07        3.35  -1.98  -0.595  -1.37       0     1\n 2 Albania  Europe  1.59    3.75        1.32   0.669  0.201   0.464      0     0\n 3 Algeria  Africa  3.69    3.54        2.81   1.88   0.567   1.31       1     0\n 4 Angola   Africa  1.12    2.71        4.31   0.163  0.0492  0.113      1     0\n 5 Antigua… Ameri…  5.88    4.36        0.432  2.56   0.769   1.77       0     0\n 6 Argenti… Ameri…  4.41    4.49        1.15   2.14   0.644   1.48       0     0\n 7 Armenia  Europe  1.89    3.72        0.309  0.918  0.276   0.637      0     0\n 8 Austral… Ocean… 16.9     5.60        1.66   4.08   1.23    2.83       0     0\n 9 Austria  Europe  7.75    5.82        0.836  2.95   0.889   2.05       0     0\n10 Azerbai… Europe  3.7     3.79        1.47   1.89   0.568   1.31       0     0\n# ℹ 179 more rows\n# ℹ 3 more variables: americas &lt;dbl&gt;, europe &lt;dbl&gt;, oceania &lt;dbl&gt;\n\n\nWe will start by fitting a model that includes the main-effect of world region in addition to our effects of wealth and urbanization. As always, when we have multiple dummy variables representing a predictor, we include all but one of the dummies in the model. Here we will omit Oceania (our reference group).\n\n# Fit model\nlm.4 = lm(log(co2) ~ 1 + wealth + I(wealth^2) + urbanization + asia + africa + americas + europe, data = carbon)\n\nWe also want to include our interactions between world region and wealth. To do this we have to include multiple interaction terms because we have multiple dummy variables. For example, here is the model that includes interactions between region and the lineqar effect of wealth and region and the quadratic effects of wealth where, once again, Oceania is our references group.\n\n# Fit model\nlm.5 = lm(log(co2) ~ 1 + wealth + I(wealth^2) + urbanization +\n            asia + africa + americas + europe +\n            asia:wealth + africa:wealth + americas:wealth + europe:wealth +\n            asia:I(wealth^2) + africa:I(wealth^2) + americas:I(wealth^2) + europe:I(wealth^2),\n          data = carbon)\n\nEvaluating these models:\n\naictab(\n  cand.set = list(lm.3, lm.4, lm.5),\n  modnames = c(\"No world region\", \"Main effect of region\", \"Interaction between region and wealth\")\n)\n\n\nModel selection based on AICc:\n\n                                       K   AICc Delta_AICc AICcWt Cum.Wt\nMain effect of region                  9 296.21       0.00      1      1\nInteraction between region and wealth 17 308.87      12.67      0      1\nNo world region                        5 313.70      17.49      0      1\n                                           LL\nMain effect of region                 -138.59\nInteraction between region and wealth -135.60\nNo world region                       -151.68\n\n\nHere, the empirical evidence favors the model that includes the main effect of world region indicating that there is not evidence of an interaction between world region and wealth. Below we look at the coefficients and write the fitted equation.\n\n# Coefficients\ncoef(lm.4)\n\n (Intercept)       wealth  I(wealth^2) urbanization         asia       africa \n  -2.1432012    1.2799732   -0.0693486   -0.0977616   -0.1547836   -0.3577002 \n    americas       europe \n  -0.5942931   -0.7134999 \n\n\n\\[\n\\begin{split}\n\\ln\\left(\\widehat{\\mathrm{CO2}_i}\\right) = &-2.14 + 1.28(\\mathrm{Wealth}_i) -0.07(\\mathrm{Wealth}_i^2) - 0.10(\\mathrm{Urbanization}_i) \\\\\n&-0.16(\\mathrm{Asia}_i) - .36(\\mathrm{Africa}_i) - .59(\\mathrm{Americas}_i) - 0.71(\\mathrm{Europe}_i)\n\\end{split}\n\\]\nRecall that the coefficients associated with the different regions are comparisons to the reference group. For example, the Asia coefficient indicates that Asia has lower log-CO2 emissions than Oceania by 0.16, on average, controlling for differences in wealth and urbanization. We can also back transform this and interpret it as percent change:\n\\[\ne^{-.16} - 1 = -.147\n\\]\n\nAsia’s carbon emissions are 14.7% lower than Oceania’s carbon emissions, on average, controlling for differences in wealth and urbanization.\n\nIn this analysis, we are most interested in the effect of wealth on CO2 emissions. So I will again create a plot of this model’s results. After creating this plot we can describe any regional differences, but we will not make inferences about regional differences. To create this plot I will set urbanization to its mean level (2.00) and then produce a fitted equation for each world region. I will do this with the back-transformed outcome.\n\nPROTIP\nIf the goal of the analysis was to compare different regions, you would have to fit other models to make the different comparisons. If we were using p-values for variable selection, you would also have to adjust based on the total number of pairwise (region-to-region) comparisons using something like the Boferroni or Benjamini-Hochberg adjustments. However, it is less clear how to view multiple comparisons when using information criteria. Dayton (1998) laid out a framework for using measures such as AIC for the multiple comparisons problem.\n\nAsia\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= e^{-2.14} \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)} \\times e^{-.10(2.00)} \\times e^{-.16(1)} \\times e^{-.36(0)} \\times e^{-.59(0)} \\times e^{-.71(0)}\\\\[2ex]\n&= .117 \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)} \\times .819 \\times .852  \\times 1 \\times 1 \\times 1\\\\[2ex]\n&= .082 \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)}\n\\end{split}\n\\]\nFrom this we note that the effects of the regions that were not Asia became 1 and dropped out of the model. We can use this in our other world regions to shorten the fitted equation.\nAfrica\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= e^{-2.14} \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)} \\times e^{-.10(2.00)}  \\times e^{-.36(1)} \\\\[2ex]\n&= .117 \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)} \\times .819 \\times .698 \\\\[2ex]\n&= .067 \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)}\n\\end{split}\n\\]\nAmericas\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= e^{-2.14} \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)} \\times e^{-.10(2.00)}  \\times e^{-.59(1)} \\\\[2ex]\n&= .117 \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)} \\times .819 \\times .54 \\\\[2ex]\n&= .053 \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)}\n\\end{split}\n\\]\nEurope\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= e^{-2.14} \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)} \\times e^{-.10(2.00)}  \\times e^{-.71(1)} \\\\[2ex]\n&= .117 \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)} \\times .819 \\times .491 \\\\[2ex]\n&= .047 \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)}\n\\end{split}\n\\]\nOceania\n\\[\n\\begin{split}\n\\widehat{\\mathrm{CO2}_i} &= e^{-2.14} \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)} \\times e^{-.10(2.00)} \\\\[2ex]\n&= .117 \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)} \\times .819 \\\\[2ex]\n&= .096 \\times e^{1.28(\\mathrm{Wealth}_i)} \\times e^{-.07(\\mathrm{Wealth}_i^2)}\n\\end{split}\n\\]\nThen we will create our plot by adding one geom_function() layer for each region.\n\n# Load library for color in caption\nlibrary(ggtext)\n\n# Plot\nggplot(data = carbon, aes(x = wealth, y = co2)) +\n  geom_point(alpha = 0) +\n  geom_function(fun = function(x) {.082 * exp(1.28*x) * exp(-0.07*x^2)}, color = \"#1b2a41\") + #Asia\n  geom_function(fun = function(x) {.067 * exp(1.28*x) * exp(-0.07*x^2)}, color = \"#99c24d\") + #Africa\n  geom_function(fun = function(x) {.053 * exp(1.28*x) * exp(-0.07*x^2)}, color = \"#d30c7b\") + #Americas\n  geom_function(fun = function(x) {.047 * exp(1.28*x) * exp(-0.07*x^2)}, color = \"#adcad6\") + #Europe\n  geom_function(fun = function(x) {.096 * exp(1.28*x) * exp(-0.07*x^2)}, color = \"#bc69aa\") + #Oceania\n  theme_bw() +\n  xlab(\"Wealth\") +\n  ylab(\"Predicted CO2 (in metric tons per person)\") +\n  labs(\n    title = \"Region: &lt;b style = 'color:#1b2a41;'&gt;Asia&lt;/b&gt;, &lt;b style = 'color:#99c24d;'&gt;Africa&lt;/b&gt;, &lt;b style = 'color:#d30c7b;'&gt;Americas&lt;/b&gt;, &lt;b style = 'color:#adcad6;'&gt;Europe&lt;/b&gt;, and &lt;b style = 'color:#bc69aa;'&gt;Oceania&lt;/b&gt;. \"\n  ) +\n  theme(\n    plot.title = element_textbox_simple()\n  )\n\n\n\n\n\n\n\nFigure 11.14: Plot of predicted carbon emisssions as a function of wealth for the quadratic polynomial model shown for countries from different regions of the world. Urbanization has been adjusted for by setting it to its mean value.\n\n\n\n\n\n\nAnswering the Third Research Question\nHere we see that in general wealthier countries have exponentially increasing CO2 emissions that begins to diminish for countries with exceptionally high wealth levels. This pattern is true for countries with both low and high urbanization, although the countries with higher urbanization have lower exponentially increasing carbon emissions.\n\n\n\n\n\n\n\n\nDayton, C. M. (1998). Information criteria for the paired-comparisons problem. The American Statistician, 52(2), 144–151.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Log-Transforming the Outcome</span>"
    ]
  },
  {
    "objectID": "03-04-logarithmic-transformations-outcome.html#footnotes",
    "href": "03-04-logarithmic-transformations-outcome.html#footnotes",
    "title": "11  Log-Transforming the Outcome",
    "section": "",
    "text": "So long as we can ultimately write this function as a linear model, namely, \\(Y_i=\\beta_0+\\beta_1(x_i) + \\epsilon_i\\).↩︎\nMathematically, the instantaneous derivative of the exponential function is is always positive, and it approaches \\(+\\infty\\) for larger values of x, while that for the quadratic function changes in sign from negative to positive.↩︎\nFun Fact: e also happens to be Andy’s favorite number. Google founders might also enjoy e, as their IPO indicated they were going to raise e-billion dollars (see this Forbes article).↩︎",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Log-Transforming the Outcome</span>"
    ]
  },
  {
    "objectID": "03-05-rule-of-the-bulge.html",
    "href": "03-05-rule-of-the-bulge.html",
    "title": "12  Rule of the Bulge—An Example”",
    "section": "",
    "text": "12.1 Power Transformations\nIn this set of notes you will learn about power transformations, how to use power transformations to re-express data so that the re-expressed data meet the assumption of “linearity” (i.e., straighten” curvilinear data), and see this in an empirical example.\nAll of the transformations, or re-expressions, of data we have seen in this course are power transformations. Power transformations essentially transform some variable X using the function:\n\\[\nX \\rightarrow X^{(p)}\n\\]\nwhere p is some power. Here are some important ideas about power transformations:\nThis is called the ladder of transformations, since we can think about these different power transformations as a ladders going up or down from the \\(p=1\\) (no transformation) starting point.\nFigure 12.1: Ladder of transformations indicating upward (up-the-ladder) power transformations and downward (down-the-ladder) power transformations.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Rule of the Bulge---An Example\"</span>"
    ]
  },
  {
    "objectID": "03-05-rule-of-the-bulge.html#power-transformations",
    "href": "03-05-rule-of-the-bulge.html#power-transformations",
    "title": "12  Rule of the Bulge—An Example”",
    "section": "",
    "text": "If we let \\(p=1\\), this transformation maps X to X (no transformation).’\nWe refer to transformations where \\(p&gt;1\\) as upward transformations.\nWe refer to transformations where \\(p&lt;1\\) as downward transformations.\nSince the transformation using \\(p=0\\) would be pointless (it would transform all the X-values to 1), we attribute \\(p=0\\) as the log-transformation.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Rule of the Bulge---An Example\"</span>"
    ]
  },
  {
    "objectID": "03-05-rule-of-the-bulge.html#rule-of-the-bulge",
    "href": "03-05-rule-of-the-bulge.html#rule-of-the-bulge",
    "title": "12  Rule of the Bulge—An Example”",
    "section": "12.2 Rule of the Bulge",
    "text": "12.2 Rule of the Bulge\nThe Rule of the Bulge is a technique introduced by John Tukey and Frederick Mosteller for “straightening” data to better meet the assumption of linearity. Note that the following figure shows the four monotonic curves; one in each of the four quadrants:\n\n\n\n\n\n\n\n\nFigure 12.2: Rule of the Bulge: Visual mneumonic for choosing power transformations to ‘straighten’ each of the four monotonic curves.\n\n\n\n\n\nThe Rule of the Bulge tells us:\n\nIf the data have a shape similar to that shown in the Quadrant 1, then the data analyst can try to re-express the variables by using an upward transformation of X (up-the-ladder), an upward transformation of Y, or both.\nIf the data have a shape similar to that shown in the Quadrant 2, then the data analyst can try to re-express the variables by using a downward transformation of X (down-the-ladder), an upward transformation of Y (up-the-ladder), or both.\nIf the data have a shape similar to that shown in the Quadrant 3, then the data analyst can try to re-express the variables by using a downward transformation of X (down-the-ladder), a downward transformation of Y, or both.\nIf the data have a shape similar to that shown in the Quadrant 4, then the data analyst can try to re-express the variables by using an upward transformation of X (up-the-ladder), a downward transformation of Y (down-the-ladder), or both.\n\n\nFYI\nThe relationships in Quadrants 1 and 4 depict exponential growth. These relationships both have a growing rate-of-change for larger x-values, albeit in different directions. The model in Quadrant 1 is a negative exponential growth model while that in Quadrant 4 is a positive exponential growth model.\nThe relationships in Quadrants 1 and 4 are referred to as exponential decay models, since they both have a decaying rate-of-change for larger x-values (again in different directions). The model in Quadrant 2 is a positive exponential decay model while that in Quadrant 3 is a negative exponential decay model.\n\nTo illustrate, consider the the non-linear relationships depicted in the following two scatterplots.\n\n\nCode\n# Load libraries\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Import data\nmn = read_csv(file = \"https://raw.githubusercontent.com/zief0002/fluffy-ants/main/data/mn-schools.csv\")\nfert = read_csv(file = \"https://raw.githubusercontent.com/zief0002/fluffy-ants/main/data/fertility.csv\")\n\n# Create scatterplot of graduation data\np1 = ggplot(data = mn, aes(x = sat, y = grad)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  theme_light() +\n  xlab(\"Estimated median SAT score (in hundreds)\") +\n  ylab(\"Six-year graduation rate\")\n\n# Create scatterplot of fertility data\np2 = ggplot(data = fert, aes(x = educ_female, y = infant_mortality)) +\n    geom_point() +\n    geom_smooth(se = FALSE) +\n    xlab(\"Average female education level\") +\n    ylab(\"Infant mortality rate (per 1,000 live births\") +\n    theme_light()\n\n# Layout plots\np1 | p2\n\n\n\n\n\n\n\n\nFigure 12.3: LEFT: The relationship between graduation rates and SAT scores is depicted by the monotonic curve in Quadrant 2 (positive, decreasing change). RIGHT: The relationship between infant mortality rates and female education level is depicted by the monotonic curve shown in Quadrant 3 (negative, decreasing change).\n\n\n\n\n\nThe scatterplot of the relationship between median SAT scores and graduation rate indicates a relationship similar to that in Quadrant 2 (positive exponential decay). This suggests that we could try to: (1) Re-express X using a downward transformation; (2) re-express Y using an upward transformation, or (3) both. In the notes, we fitted a model in which we log-transformed the median SAT scores (downward transformation of X) to “straighten” the relationship,\nY ~ 1 + ln(X)\nThe scatterplot of the relationship between female education level and infant mortality rate indicates a relationship similar to that in Quadrant 3 (negative exponential decay). This suggests that we could try to: (1) Re-express X using a downward transformation; (2) re-express Y using a downward transformation, or (3) both. Below is the plot in which we re-expressed infant mortality rate using a log-transform (downward transformation.)\nln(Y) ~ 1 + X\n\n\n12.2.1 Caution ⚠️\nSometimes these re-expressions will not be adequate. In some cases, you might not be able to “straighten” the data enough to meet the assumption. This is because these transformations “deteriorate” or “spuriously increase” the information contained in the data. As you use re-expressions further down-the-ladder, the variation in the re-expressed data decreases (Less variation = less information). Eventually, the variation in the re-expressed data will be so small that the values become indistinguishable (no information).\nIn the other direction, as you use re-expressions further up-the-ladder, the variation in the re-expressed data increases (more variation = more information), albeit spuriously. Essentially, we are adding information that is not truly in the data. This might lead us to finding results that aren’t really there, or over-emphasizing relationships.\n\nFYI\nRe-expressions that only go a little way up- or down-the-ladder are fine. Just beware if you need to go too far up- or down-the-ladder to straighten your data. In those cases you may want to use a different method of estimating the model than OLS (e.g., non-linear least squares).",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Rule of the Bulge---An Example\"</span>"
    ]
  },
  {
    "objectID": "03-05-rule-of-the-bulge.html#empirical-example",
    "href": "03-05-rule-of-the-bulge.html#empirical-example",
    "title": "12  Rule of the Bulge—An Example”",
    "section": "12.3 Empirical Example",
    "text": "12.3 Empirical Example\nWe will use the mammals.csv data to predict variation in body weight for mammals using their brain weight as a predictor. See the data codebook for additional information.\n\n# Load libraries\nlibrary(broom)\nlibrary(tidyverse)\n\n# Import data\nmammal = read_csv(\"https://raw.githubusercontent.com/zief0002/fluffy-ants/main/data/mammals.csv\")\n\n# View data\nmammal\n\n# A tibble: 62 × 11\n   species       body_weight brain_weight slow_wave paradox total_sleep lifespan\n   &lt;chr&gt;               &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1 African elep…    6654           5712        NA      NA           3.3     38.6\n 2 African gian…       1              6.6       6.3     2           8.3      4.5\n 3 Arctic fox          3.38          44.5      NA      NA          12.5     14  \n 4 Arctic groun…       0.92           5.7      NA      NA          16.5     NA  \n 5 Asian elepha…    2547           4603         2.1     1.8         3.9     69  \n 6 Baboon             10.6          180.        9.1     0.7         9.8     27  \n 7 Big brown bat       0.023          0.3      15.8     3.9        19.7     19  \n 8 Brazilian ta…     160            169         5.2     1           6.2     30.4\n 9 Cat                 3.3           25.6      10.9     3.6        14.5     28  \n10 Chimpanzee         52.2          440         8.3     1.4         9.7     50  \n# ℹ 52 more rows\n# ℹ 4 more variables: gestation &lt;dbl&gt;, predation &lt;dbl&gt;, exposure &lt;dbl&gt;,\n#   danger &lt;dbl&gt;\n\n# Examine relationship\nggplot(data = mammal, aes(x = brain_weight, y = body_weight)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  theme_light() +\n  xlab(\"Brain weight (in g)\") +\n  ylab(\"Body weight (in kg)\")\n\n\n\n\n\n\n\nFigure 12.4: Scatterplot showing the relationship between body weight and brain weight for 62 mammals.\n\n\n\n\n\nThe relationship is non-linear, and shows a positive exponential growth curve. Using the Rule of the Bulge mnemonic, we identify this curve in the lower right-hand quadrant. To help straighten this curve we can either:\n\nTransform X using an upward transformation; or\nTransform Y using a downward transformation\n\nSince there is only a single predictor, transforming Y is low-cost (it doesn’t affect the relationship between Y and other predictors), whereas transforming X with an upward transformation means we would have to include more than one effect in the model (e.g., \\(X\\) and \\(X^2\\)).\nBecause of this I will transform Y using the log-transformation. Looking at the relationship between ln(body weight) and brain weight, we will see if this “straightened” the relationship.\n\nggplot(data = mammal, aes(x = brain_weight, y = log(body_weight))) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  theme_light() +\n  xlab(\"Brain weight (in g)\") +\n  ylab(\"ln(Body weight)\")\n\n\n\n\n\n\n\nFigure 12.5: Scatterplot showing the relationship between the natural logarithm of body weight and brain weight for 62 mammals.\n\n\n\n\n\nThe transformed relationship is still non-linear, and shows positive exponential decay. Again, using the Rule of the Bulge mnemonic, we identify this curve in the upper left-hand quadrant. To help straighten this curve we can either:\n\nTransform X using an downward transformation; or\nTransform Y using an upward transformation\n\nSince we just used a downward transformation on Y to fix the last relationship, using an upward transformation now would just re-introduce the initial problem. Because of this I will transform X using the log-transformation. Looking at the relationship between ln(body weight) and ln(brain weight), we will see if this “straightened” the relationship.\n\nggplot(data = mammal, aes(x = log(brain_weight), y = log(body_weight))) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  theme_light() +\n  xlab(\"ln(Brain weight)\") +\n  ylab(\"ln(Body weight)\")\n\n\n\n\n\n\n\nFigure 12.6: Scatterplot showing the relationship between the natural logarithm of body weight and the natural logarithm of brain weight for 62 mammals.\n\n\n\n\n\nThis relationship looks linear! So we can fit a linear model that uses ln(brain weight) to predict variation in ln(body weight). We can then use back-transformations and a plot of the fitted equation to interpret the coefficients in the model.\n\n\n12.3.1 Fit Linear Model\nFitting the linear model and looking at it’s output:\n\n# Fit model\nlm.1 = lm(log(body_weight) ~ 1 + log(brain_weight), data = mammal)\n\n# Model-level output\nglance(lm.1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.921         0.919 0.886      697. 9.84e-35     1  -79.5  165.  171.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# Coefficient-level output\ntidy(lm.1)\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          -2.51    0.184      -13.6 4.98e-20\n2 log(brain_weight)     1.22    0.0464      26.4 9.84e-35\n\n\nInterpreting this output:\n\nDifferences in mammals’ brain weight explain 92.1% of the variation in body weight.\n\nThe fitted equation is:\n\\[\n\\hat{\\ln(\\mathrm{Body~Weight})}_i = -2.51 + 1.22\\bigg[\\ln(\\mathrm{Brain~Weight}_i)\\bigg]\n\\]\n\nMammals with a log(brain weight) of 0 have a predicted log(body weight) of \\(-2.51\\), on average.\nEach one-unit change in log(brain weight) is associated with a change in log(body weight) of 1.22-units, on average.\n\nWe can also back-transform these log entities to get a better interpretation of the coefficients. For the intercept, when log(brain weight) is 0, actual brain weight = 1. Thus, mammals with a 1-gram brain weight have a predicted log(body weight) of \\(-2.51\\), on average. Exponentiating this (\\(e^{-2.51}=0.081\\)), so we can interpret the intercept as:\n\nMammals with a brain weight of 1 gram have a predicted body weight of 0.081 kg, on average.\n\nTo consider the interpretation of the slope, we utilize the fact that log-transforming X (using the natural logarithm) results in an interpretation that can be interpreted as a 1% change in X. As such, we choose a series of brain weights that differ by 1% and plug them into our fitted equation to get predicted log(body weights):\n\n# Choose brain weights that differ by 1%\nbody = c(100, 101, 102.01)\n\n# Get predicted ln(body weight) values\n-2.51 + 1.22 * log(body)\n\n[1] 3.108308 3.120447 3.132586\n\n\nThe interpretation is:\n\nEach 1% difference in brain weight is associated with a difference of 0.012 in log(body weight), on average.\n\nHere 0.012 is the slope coefficient divided by 100. Now let’s transform the log(body weight) values to raw body weights. To do this, we exponentiate these predicted values:\n\n# Exponentiate the predicted values\nexp(-2.51 + 1.22 * log(body))\n\n[1] 22.38313 22.65651 22.93322\n\n\nThis results in a constant multiplicative difference of 1.0122, Namely,\n\nEach 1% difference in brain weight is associated with a 1.012-fold difference in body weight, on average.\n\nOr, interpreting this as a percent change:\n\nEach 1% difference in brain weight is associated with a 1.22% difference in body weight, on average.\n\nThis 1.22% change is essentially the slope coefficient from the fitted equation. Thus when we log-transform both X and Y using the natural logarithm, we can interpret both the change in X and change in Y as a percent change. In general:\n\nEach 1% difference in X is associated with a \\(\\hat\\beta_1\\)% difference in Y, on average.\n\nWe can also plot the fitted curve to facilitate a graphical interpretation.\n\n# Plot fitted curve\nggplot(data = mammal, aes(x = brain_weight, y = body_weight)) +\n  geom_point(alpha = 0.2) +\n  geom_function(fun = function(x){exp(-2.509 + 1.225*log(x))}) +\n  theme_light() +\n  xlab(\"Brain weight (in g)\") +\n  ylab(\"Body weight (in kg)\")\n\n\n\n\n\n\n\nFigure 12.7: Fitted regression curve showing the relationship between body weight and brain weight for 62 mammals.\n\n\n\n\n\n\nFYI\nNote that the relationship between brain weight and body weight is referred to as a proportional growth model. (If the relationship was negative we would call it a proportional decay model.) To “linearize” a proportional growth model, we fit a log–log model.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Rule of the Bulge---An Example\"</span>"
    ]
  },
  {
    "objectID": "03-05-rule-of-the-bulge.html#interpreting-log-transformed-models",
    "href": "03-05-rule-of-the-bulge.html#interpreting-log-transformed-models",
    "title": "12  Rule of the Bulge—An Example”",
    "section": "12.4 Interpreting Log-Transformed Models",
    "text": "12.4 Interpreting Log-Transformed Models\n\n\n\n\n\n\n\n\nFigure 12.8: Log-transformations for different curvilinear relationship and interpretation of the slope coefficients from these models.",
    "crumbs": [
      "Modeling Nonlinearity",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Rule of the Bulge---An Example\"</span>"
    ]
  },
  {
    "objectID": "05-00-modeling-nonindependence.html",
    "href": "05-00-modeling-nonindependence.html",
    "title": "Modeling Nonindependence",
    "section": "",
    "text": "The assumption of independence is important for using the general linear model (i.e., regression). When that assumption is not met, we need to use a different analytic method. In this section you will learn about linear mixed-effects models and how to fit, interpret, and evaluate these models using R.",
    "crumbs": [
      "Modeling Nonindependence"
    ]
  },
  {
    "objectID": "05-01-introduction-to-mixed-effects-models.html",
    "href": "05-01-introduction-to-mixed-effects-models.html",
    "title": "13  Introduction to Mixed-Effects Models",
    "section": "",
    "text": "13.1 Fixed-Effects Regression Model\nIn this set of notes, you will learn the conceptual ideas behind linear mixed-effects models, also called multilevel models or hierarchical linear models. To do this, we will use data from the file minneapolis.csv (see codebook).\nThese data include repeated measurements of vertically scaled reading achievement scores for \\(n=22\\) students. We will use these data to explore the question of whether students’ reading achievement is changing over time.\nTo examine the research question of whether students’ reading achivement scores are changing over time, we might try regressing reading achievement scores on grade-level predict using the lm() function. The lm() function fits a fixed-effects regression model.\n# Fit fixed-effects model\nlm.1 = lm(reading_score ~ 1 + grade, data = mpls)\n\n# Model-level output\nglance(lm.1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0608        0.0499  19.4      5.57  0.0205     1  -385.  776.  783.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# Coefficient-level output\ntidy(lm.1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   184.       12.2      15.1  6.24e-26\n2 grade           4.36      1.85      2.36 2.05e- 2\nThe model-level summary information suggests that differences in grade-level explains 6.08% of the variation in reading achievement scores. The fitted equation based on the coefficient-level output is:\n\\[\n\\hat{\\mathrm{Reading~Score}}_i = 184 + 4.36(\\mathrm{Grade}_i)\n\\]\nThe intercept suggests that the reading achievement score for students in the 0th grade (grade=0) is 184, on average. The slope indicates that each one-grade difference is associated with a change in reading achievement of 4.36 points, on average. To have faith in the analytic results from this model, we need to evaluate whether the assumptions are satisfied.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Mixed-Effects Models</span>"
    ]
  },
  {
    "objectID": "05-01-introduction-to-mixed-effects-models.html#fixed-effects-regression-model",
    "href": "05-01-introduction-to-mixed-effects-models.html#fixed-effects-regression-model",
    "title": "13  Introduction to Mixed-Effects Models",
    "section": "",
    "text": "13.1.1 Residual Analysis\n\n# Augment the model\nout = augment(lm.1)\n\n# Density plot\np1 = ggplot(data = out, aes(x = .std.resid)) +\n  educate::stat_density_confidence(model = \"normal\") +\n  geom_density() +\n  theme_minimal() +\n  xlab(\"Standardized residuals\") +\n  ylab(\"Probability density\")\n\n# Scatterplot\np2 = ggplot(data = out, aes(x = .fitted, y = .std.resid)) +\n  geom_smooth(method = \"lm\", color = \"lightgrey\") + #Create conf envelope for where the average residual should be\n  geom_hline(yintercept = 0) +\n  geom_point() +\n  theme_minimal() +\n  xlab(\"Fitted values\") +\n  ylab(\"Standardized residuals\")\n\n# Output plots\np1 | p2\n\n\n\n\n\n\n\nFigure 13.1: Residual plots for the fixed-effects regression model.\n\n\n\n\n\n\nBecause the loess smoother completely changes the scale, the residual_plots() function does not work here, so I created the residual plots by hand.\n\nThe assumption that the mean residual is 0 seems reasonably satisfied, as does the tenability of the normality and homoscedasticity assumptions. However, the assumption of independence (which we don’t evaluate from the common residual plots) is probably not tenable. Recall that the data includes multiple reading achievment scores for the same student. These scores (and thus the residuals) are probably correlated—this is a violation of independence which assumes that the correlation between each set of residuals is 0.\nBecause we have a variable that identifies each student, we can actually examine this by plotting the residuals separately for each student. To do so we need to include the student_id variable as a new column in the augmented data (using mutate()) so we can show the residuals versus the fitted values by student.\n\n# Augment the model and mutate on student ID\nout = augment(lm.1) |&gt;\n  mutate(student_id = mpls$student_id)\n\n### Show residuals by student\nggplot(data = out, aes(x = .fitted, y = .std.resid)) +\n    geom_point() +\n    geom_hline(yintercept = 0) +\n    theme_bw() +\n  xlab(\"Fitted values\") +\n  ylab(\"Studentized residuals\") +\n    facet_wrap(~student_id)\n\n\n\n\n\n\n\nFigure 13.2: Scatterplots of the standardized residuals versus the fitted values for 22 students.\n\n\n\n\n\nThe residuals for several of the students show a systematic trends of being primarily positive or negative within students. For example, the residuals for several students (e.g., Sudents 1, 3, 4, 6, 8, 9) are primarily negative. Knowing that a residual is negative gives us information that other residuals for that student are also negative. There are similar trends for students that have positive residuals. This is a sign of non-independence of the residuals. If we hadn’t had the student ID variable we could have still made a logical argument about this non-independence via substantive knowledge. For example, students who perform above average in Grade 5 will likely tend to perform well in subsequent grades (i.e., have positive residuals) relative to the population. Similarly, students who perform below average in 5th grade will tend to perform below average, relative to the population, in other grades. That is, a student’s reading achievement scores across grade levels tend to be correlated.\nTo account for this within-student correlation we need to use a statistical model that accounts for the correlation among the residuals within student. This is what mixed-effects models bring to the table. By correctly modeling the non-independence, we get more accurate standard errors and p-values.\nAnother benefit of using mixed-effects models is that we also get estimates of the variation accounted for at both the between- and within-student levels. This disaggregation of the variation allows us to determine which level is explaining more variation and to study predictors appropriate to explaining that variation. For example, suppose that you disaggregated the variation in reading achievement scores and found that:\n\n82% of the variation in these scores was at the within-student level, and\n18% of the variation in these scores was at the between-student level.\n\nWe could conclude that most of the variation in reading achievement scores is within-student, which means that we would be better off thinking about within-student predictors. (These are predictors that may vary across grade-levels for a given student.) The only within-student predictor in our example data is grade-level. Both special education status, and attendance are between-student predictors—they vary for different students, but have the same value for any one student. By including between-student predictors in the model, you would only be “chipping away” at that 18% of the variation that is between-student variation. This type of decompostion of the unexplained variation helps focus your attention and resources on the levels of variation that matter!\n\nWithin- and between-student predictors go by many other names. Other names for these predictors are time-varying predictors (within-student) and non-time-varying (between-students) predictors. Also level-1 (within-student) and level-2 (between-student) predictors.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Mixed-Effects Models</span>"
    ]
  },
  {
    "objectID": "05-01-introduction-to-mixed-effects-models.html#conceptual-idea-of-mixed-effects-models",
    "href": "05-01-introduction-to-mixed-effects-models.html#conceptual-idea-of-mixed-effects-models",
    "title": "13  Introduction to Mixed-Effects Models",
    "section": "13.2 Conceptual Idea of Mixed-Effects Models",
    "text": "13.2 Conceptual Idea of Mixed-Effects Models\nIn this section we will outline the conceptual ideas behind mixed-effects models by linking the ideas behind these models to the conventional, fixed-effects regression model. It is important to realize that this is just conceptual in nature. Its purpose is only to help you understand the output you get from a mixed-effects model analysis. It is NOT how we carry out a mixed-effects analysis.\nTo begin, we remind you of the fitted equation we obtained earlier from the fixed-effects regression:\n\\[\n\\hat{\\mathrm{Reading~Score}}_i = 184 + 4.36(\\mathrm{Grade}_i)\n\\]\nThis is the fitted equation from the global model, so called because it was fitted with the data from all time points across all students. As such, the intercept and slope estimates from this equation are averaged across time grade levels and students.1\nMixed-effects regression conceptually fits a global model (like the one above) AND a set of student-specific models, one for each student. The student-specific model is akin to fitting a regression model for each student separately. Below I show the results (for five of the students) of fitting a different regression model to each student, but keep in mind that this is not actually what happens; it is only to help you understand.\n\n# Fit student models\nstudent_models = mpls |&gt;\n  group_by(student_id) |&gt;\n  summarize(\n    tidy(lm(reading_score  ~ 1 + grade))\n    ) |&gt;\n  ungroup()\n\n# View coefficients from fitted models\nstudent_models\n\n# A tibble: 44 × 6\n   student_id term        estimate std.error statistic  p.value\n        &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1          1 (Intercept)   144.      18.8        7.64 0.0167  \n 2          1 grade           6        2.85       2.11 0.170   \n 3          2 (Intercept)   188.      11.2       16.9  0.00350 \n 4          2 grade           2.90     1.69       1.71 0.229   \n 5          3 (Intercept)   153.       6.98      21.9  0.00209 \n 6          3 grade           7.60     1.06       7.18 0.0188  \n 7          4 (Intercept)   206.       6.33      32.6  0.000938\n 8          4 grade          -1.60     0.959     -1.67 0.237   \n 9          5 (Intercept)   200.       6.69      29.9  0.00112 \n10          5 grade           1.70     1.01       1.68 0.236   \n# ℹ 34 more rows\n\n\nAs an example, let’s focus on the fitted model for Student 1.\n\\[\n\\mathbf{Student~1:}~~\\hat{\\mathrm{Reading~Score}_i} = 144 + 6.00(\\mathrm{Grade}_i)\n\\]\nComparing this student-specific model to the global model, we find that Student 1’s intercept is lower than the intercept from the global model (by 40 points) and Student 1’s slope is higher than the slope from the global model (by 1.64). We can actually re-write the Student 1’s student-specific model using these ideas:\n\\[\n\\mathbf{Student~1:}~~\\hat{\\mathrm{Reading~Score}_i} = \\bigg[184 - 40\\bigg] + \\bigg[4.36 + 1.64\\bigg](\\mathrm{Grade}_i)\n\\]\nIn the language of mixed-effects modeling:\n\nThe global intercept and slope are referred to as fixed-effects. (These are also sometimes referred to as between-groups effects.)\n\nThe fixed-effect of intercept is 184; and\nThe fixed effect of the slope is 4.36.\n\nThe student-specific deviations from the fixed-effect values are referred to as random-effects. (These are also sometimes referred to as within-groups effects.)\n\nThe random-effect of the intercept for Student 1 is \\(-40\\); and\nThe random-effect of the slope for Student 1 is 1.64.\n\n\nWriting the student-specific fitted equation for Student 2 in this manner:\n\\[\n\\begin{split}\n\\mathbf{Student~2:}~~\\hat{\\mathrm{Reading~Score}_i} &= 188 + 2.90(\\mathrm{Grade}_i)\\\\\n&= \\bigg[184 + 4\\bigg] + \\bigg[4.36 - 1.46\\bigg](\\mathrm{Grade}_i)\\\\\n\\end{split}\n\\]\nIn this model:\n\nThe fixed-effects (global effects) are the same as they were for Student 1.\n\nThe fixed-effect of intercept is 184; and\nThe fixed effect of the slope is 4.36.\n\nThe random-effect of intercept for Student 2 is 4.\nThe random-effect of slope for Student 2 is \\(-1.46\\).\n\nThe fixed-effects (effects from the global model) are the same across all students. The random-effects, which represent the deviation between the global effects and the effects in a particular student’s equation, vary across students. That is, each student could potentially have a different random-effect for intercept and slope.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Mixed-Effects Models</span>"
    ]
  },
  {
    "objectID": "05-01-introduction-to-mixed-effects-models.html#fitting-the-mixed-effects-regression-model-in-practice",
    "href": "05-01-introduction-to-mixed-effects-models.html#fitting-the-mixed-effects-regression-model-in-practice",
    "title": "13  Introduction to Mixed-Effects Models",
    "section": "13.3 Fitting the Mixed-Effects Regression Model in Practice",
    "text": "13.3 Fitting the Mixed-Effects Regression Model in Practice\nIn practice, we use the lmer() function from the {lme4} library to fit mixed-effect regression models. This function will essentially do what we did in the previous section, but rather than independently fitting the student-specific models, it will fit all these models simultaneously and make use of the information in all the clusters (students) to do this. This will result in better estimates for both the fixed- and random-effects.\nThe syntax looks similar to the syntax we use in lm() except now we split it into two parts. The first part of the syntax gives a model formula to specify the outcome and fixed-effects included in the model. This is identical to the syntax we used in the lm() function. In our example: reading_score ~ 1 + grade indicating that we want to fit a model that includes fixed-effects for both the intercept and the effect of grade level.\nWe also have to declare that we want to fit a model for each student. To do this, we will include a random-effect for intercept. (We could also include a random-effect of grade, but to keep it simpler right now, we only include the RE of intercept.) The second part of the syntax declares this: (1 | student_id). This says to fit student-specific models that vary in their intercepts. This is literally added to the fixed-effects formula using +. The complete syntax is:\n\n# Fit mixed-effects regression model\nlmer.1 = lmer(reading_score ~ 1 + grade + (1 | student_id), data = mpls)\n\nTo view the fixed-effects, we use the tidy() function from the {broom.mixed} package with the argument effects=\"fixed\".\n\ntidy(lmer.1, effects = \"fixed\")\n\n# A tibble: 2 × 5\n  effect term        estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)   184.       5.30      34.8 \n2 fixed  grade           4.36     0.525      8.31\n\n\nThis gives the coefficients for the fixed-effects part of the model (i.e., the global model),\n\\[\n\\mathbf{Fixed\\mbox{-}Effects~Equation:}~~\\hat{\\mathrm{Reading~Score}_{ij}} = 184.27 + 4.36(\\mathrm{Grade}_{ij})\n\\]\nNote that the notation now includes two subscripts. The i subscript now indicates the ith time point (grade level), and the new j subscript indicates Student J. We interpret these coefficients from the fixed-effects equation exactly like lm() coefficients. Here,\n\nThe reading achievement score for students in Grade 0 is 184.27, on average.\nEach one-grade difference is associated with a 4.36-point difference in reading achievement score, on average.\n\nTo view the student-specific random-effects, we again use the tidy() function from the {broom.mixed} package, but this time with the argument effects=\"ran_vals\".\n\ntidy(lmer.1, effects = \"ran_vals\")\n\n# A tibble: 22 × 6\n   effect   group      level term        estimate std.error\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 ran_vals student_id 1     (Intercept)   -29.5       2.73\n 2 ran_vals student_id 2     (Intercept)    -5.27      2.73\n 3 ran_vals student_id 3     (Intercept)   -10.4       2.73\n 4 ran_vals student_id 4     (Intercept)   -16.3       2.73\n 5 ran_vals student_id 5     (Intercept)    -1.36      2.73\n 6 ran_vals student_id 6     (Intercept)   -17.0       2.73\n 7 ran_vals student_id 7     (Intercept)    -3.07      2.73\n 8 ran_vals student_id 8     (Intercept)   -19.2       2.73\n 9 ran_vals student_id 9     (Intercept)   -48.1       2.73\n10 ran_vals student_id 10    (Intercept)    -2.58      2.73\n# ℹ 12 more rows\n\n\nThe random-effects indicate how the student-specific intercept differs from the intercept in the fixed-effects equation (which is the average intercept across students). From the estimated fixed- and random-effects, we can re-construct each student-specific fitted equation. For example, to construct the student-specific fitted equation for Student 1, we combine the estimated coefficients for the fixed-effects and the estimated random-effect for Student 1:\n\\[\n\\begin{split}\n\\mathbf{Student~1:}~~\\hat{\\mathrm{Reading~Score}_{i1}} &= \\bigg[184.27 - 29.50 \\bigg]+ 4.36(\\mathrm{Grade}_{i1}) \\\\[1ex]\n&= 154.77 + 4.36(\\mathrm{Grade}_{i1})\n\\end{split}\n\\]\nIn this notation, the j part of the subscript is now set to 1 since it refers to Student 1. (If the student is identified as “Student 1” j part of the subscript could also be dropped entirely.) We can also interpret the coefficients from the fitted student-specific equation:\n\nThe reading achievement score for Student 1 in Grade 0 is 154.77, on average.\nFor Student 1, each one-grade difference is associated with a 4.36-point difference in reading achievement score, on average.\n\nThese interpretations are similar to those for the fixed-effects, but they are in refrence to a specific student, namely Student 1. We can also interpret the difference between the coefficients in the global equation and the student-specific equation:\n\nThe reading achievement score for Student 1 in Grade 0 is 29.50 points lower than the average reading score of students in Grade 0.\nStudent 1’s rate-of-growth (4.36 points per grade level) is exactly the same as the average rate-of-growth. (In fact all students would have the same average predicted rate-of-growth in this model.)\n\nThese interpretations are the interpretations of the random-effects.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Mixed-Effects Models</span>"
    ]
  },
  {
    "objectID": "05-01-introduction-to-mixed-effects-models.html#random-effects-for-intercept-and-grade-level",
    "href": "05-01-introduction-to-mixed-effects-models.html#random-effects-for-intercept-and-grade-level",
    "title": "13  Introduction to Mixed-Effects Models",
    "section": "13.4 Random Effects for Intercept and Grade Level",
    "text": "13.4 Random Effects for Intercept and Grade Level\nIf we want to include both a random effect for intercept and grade level (i.e., allow the student-specific equations to have different intercept and slopes), we need to specify this in the random-effects part of the syntax: (1 + grade | student_id). This says to fit student-specific models that vary in their intercepts and in their effects for grade level. The complete syntax is:\n\n# Fit mixed-effects regression model\nlmer.2 = lmer(reading_score ~ 1 + grade + (1 + grade | student_id), data = mpls)\n\nTo view the fixed-effects, we again use the tidy() function with effects=\"fixed\".\n\ntidy(lmer.2, effects = \"fixed\")\n\n# A tibble: 2 × 5\n  effect term        estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)   184.       6.58      28.0 \n2 fixed  grade           4.36     0.720      6.06\n\n\nThis fitted equation for the fixed-effects part of the model (i.e., the global model) is:\n\\[\n\\mathbf{Fixed\\mbox{-}Effects~Equation:}~~\\hat{\\mathrm{Reading~Score}_{ij}} = 184.27 + 4.36(\\mathrm{Grade}_{ij})\n\\]\nThe fixed-effects did not change much from our initial model (sometimes they do). They have the same interpretations as in lmer.1. To view the student-specific random-effects, we use the tidy() function with effects=\"ran_vals\". We pipe this output into arrange() to order the output by student ID values.\n\ntidy(lmer.2, effects = \"ran_vals\") |&gt;\n  arrange(level)\n\n# A tibble: 44 × 6\n   effect   group      level term        estimate std.error\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 ran_vals student_id 1     (Intercept)  -38.5       10.3 \n 2 ran_vals student_id 1     grade          1.34       1.55\n 3 ran_vals student_id 10    (Intercept)   -3.79      10.3 \n 4 ran_vals student_id 10    grade          0.182      1.55\n 5 ran_vals student_id 11    (Intercept)    8.68      10.3 \n 6 ran_vals student_id 11    grade          1.34       1.55\n 7 ran_vals student_id 12    (Intercept)   15.0       10.3 \n 8 ran_vals student_id 12    grade          1.62       1.55\n 9 ran_vals student_id 13    (Intercept)   29.2       10.3 \n10 ran_vals student_id 13    grade         -1.43       1.55\n# ℹ 34 more rows\n\n\nNow there are two columns of output in the random-effects. The value in the (Intercept) column indicates how the student-specific intercept differs from the intercept in the fixed-effects equation. The value in the grade column indicates how the student-specific grade level effect differs from the grade level in the fixed-effects equation.\nFor example, based on the random-effects for Student 1:\n\nStudent 1’s intercept is \\(38.51\\)-points lower than the average intercept of 184.27.\nStudent 1’s grade level effect is \\(1.34\\)-points higher than the average grade level effect of 4.36.\n\nThis implies that, on average, Student 1’s reading achievement score in Grade 0, is 38.51 points lower than their peers (extrapolation). But, their rate-of-growth is 1.34 points-per-grade level higher than their peers.\nWriting the student-specific fitted equation for Student 1:\n\\[\n\\begin{split}\n\\mathbf{Student~1:}~~\\hat{\\mathrm{Reading~Score}_{i}} &= \\bigg[184.27 - 38.51 \\bigg] + \\bigg[4.36 + 1.34\\bigg](\\mathrm{Grade}_{i}) \\\\[1ex]\n&= 145.76 + 5.70(\\mathrm{Grade}_{i})\n\\end{split}\n\\]",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Mixed-Effects Models</span>"
    ]
  },
  {
    "objectID": "05-01-introduction-to-mixed-effects-models.html#looking-forward",
    "href": "05-01-introduction-to-mixed-effects-models.html#looking-forward",
    "title": "13  Introduction to Mixed-Effects Models",
    "section": "13.5 Looking Forward",
    "text": "13.5 Looking Forward\nThe mixed-effects model accounts for non-independence by including one or more random effects into the regression model. Over the next several sets of notes, we will learn how to determine whether we should include only a random effect of intercept, or whether we should also allow other effects in the model (e.g., grade level?) to also have a random effect. You will also learn how to do the decomposition of variation into within- and between-subjects components.\nAs with any other statistical model, the mixed-effects model has a set of assumptions that we need to evaluate. You will learn what those assumptions are, and how to evaluate them. We will also learn about different ways of mathematically expressing this model, and how to present the results from our fitted equations.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Mixed-Effects Models</span>"
    ]
  },
  {
    "objectID": "05-01-introduction-to-mixed-effects-models.html#footnotes",
    "href": "05-01-introduction-to-mixed-effects-models.html#footnotes",
    "title": "13  Introduction to Mixed-Effects Models",
    "section": "",
    "text": "Some authors refer to this as a completely pooled model or the mean model.↩︎",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Mixed-Effects Models</span>"
    ]
  },
  {
    "objectID": "05-02-lmer-average-change-over-time.html",
    "href": "05-02-lmer-average-change-over-time.html",
    "title": "14  LMER: Average Change Over Time",
    "section": "",
    "text": "14.1 Data Structure: Tidy/Long Data vs. Wide Data\nIn this set of notes, you will learn how to use the linear mixed-effects model to examine the mean change over time in a set of longitudinal/repeated measurements. To do this, we will use data from the file vocabulary.csv (see data codebook).\nThese data include repeated measurements of scaled vocabulary scores for \\(n=64\\) students.\nWe will use these data to explore the change in vocabulary over time (longitudinal variation in the vocabulary scores). In most longitudinal analyses, the primary goal is to examine the mean change of some outcome over time. To do this we will focus on the following research question: (1) What is the growth pattern in the average vocabulary score over time?\nBefore doing any analysis of the data, it is worth understanding the structure of the data. There are two common structures for repeated measures data: tidy/long structured data and wide structured data.\nThe vocabulary data is currently structured as wide data; the vocabulary scores are organized into four separate columns and the information about grade-level (the time predictor) is embedded in the variable names (e.g., vocab_08 indicates 8th-grade). The same data are presented below in the tidy/long structure.\n# A tibble: 256 × 4\n      id female grade    vocab_score\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1     1      1 vocab_08        1.75\n 2     1      1 vocab_09        2.6 \n 3     1      1 vocab_10        3.76\n 4     1      1 vocab_11        3.68\n 5     2      0 vocab_08        0.9 \n 6     2      0 vocab_09        2.47\n 7     2      0 vocab_10        2.44\n 8     2      0 vocab_11        3.43\n 9     3      0 vocab_08        0.8 \n10     3      0 vocab_09        0.93\n# ℹ 246 more rows\nNotice that in the tidy/long structured data that the vocabulary scores (outcome) are now organized into a single column. Grade-level (the time predictor) is also now explicitly included in the data and is also organized as a single column in the data. Note that in the long structure, each row now represents a particular student at a particular grade-level, and that each student’s data now encompasses several rows.\nThere are advantages to each of the structures. For example the wide structure has the advantage of being a better structure for data entry. Since each row corresponds to a different student, there are fewer rows and therefore less redundancy in the data entry process. Compare this to the tidy/long data where each student’s data encompasses four rows. If you were doing data entry in this structure you would need to record the student’s sex four times rather than once in the wide structure.\nThe tidy/long structure is the structure that is needed for modeling. Thus, if one of the analytic goals is to fit a linear mixed-effects model to explain variation or examine predictor effects, the tidy/long data structure is key. Note that the wide structured data is also used in some analyses (e.g., computing correlations).",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>LMER: Average Change Over Time</span>"
    ]
  },
  {
    "objectID": "05-02-lmer-average-change-over-time.html#data-structure-tidylong-data-vs.-wide-data",
    "href": "05-02-lmer-average-change-over-time.html#data-structure-tidylong-data-vs.-wide-data",
    "title": "14  LMER: Average Change Over Time",
    "section": "",
    "text": "In tidy/long structured data, there is a single column per variable. For example, the outcome variable (vocabulary scores) would be organized into a single column. Similarly, the predictor that designates time (grade-level in our example) would also be organized into a single column.\nIn wide structured data, the outcome variable (or predictor variables) is typically spread out over multiple columns. Often there are not columns that include data on the time predictor; instead this information is typically embedded in the column name.\n\n\n\n\n\n\n\n\n14.1.1 Switching between the Two Data Structures\nThe library {tidyr} (loaded as part of the {tidyverse} metapackage) has two functions that convert data between these two structures:\n\npivot_longer() (wide \\(\\rightarrow\\) tidy/long), and\npivot_wider() (tidy/long \\(\\rightarrow\\) wide).\n\nBelow, I show the code for going from the wide structured data (vocabulary) to the tidy/long structure.\n\n# Convert from wide to long structured data\nvocabulary_long = vocabulary |&gt;\n  pivot_longer(cols = vocab_08:vocab_11, names_to = \"grade\", values_to = \"vocab_score\") |&gt;\n  arrange(id, grade)\n\n# View data\nvocabulary_long\n\n# A tibble: 256 × 4\n      id female grade    vocab_score\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1     1      1 vocab_08        1.75\n 2     1      1 vocab_09        2.6 \n 3     1      1 vocab_10        3.76\n 4     1      1 vocab_11        3.68\n 5     2      0 vocab_08        0.9 \n 6     2      0 vocab_09        2.47\n 7     2      0 vocab_10        2.44\n 8     2      0 vocab_11        3.43\n 9     3      0 vocab_08        0.8 \n10     3      0 vocab_09        0.93\n# ℹ 246 more rows\n\n\nHere is an animated tutorial for understanding this syntax.\n\nFor more information about using these functions, Google “tidyr pivot” and read through any number of great tutorials or vignettes; for example here. You can also read Hadley Wickham’s (2014) original paper on tidy data.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>LMER: Average Change Over Time</span>"
    ]
  },
  {
    "objectID": "05-02-lmer-average-change-over-time.html#exploration-plot-of-the-mean-and-individual-profiles",
    "href": "05-02-lmer-average-change-over-time.html#exploration-plot-of-the-mean-and-individual-profiles",
    "title": "14  LMER: Average Change Over Time",
    "section": "14.2 Exploration: Plot of the Mean and Individual Profiles",
    "text": "14.2 Exploration: Plot of the Mean and Individual Profiles\nThere are two plots that are particularly useful in exploring longitudinal data. The first is a plot of the mean value of the outcome at each time point (mean profile plot). This shows the average growth profile and is useful for determining the functional form of the fixed-effects part of the model; is the mean change over time linear? Quadratic? Log-linear?\nAnother plot that is often examined is a plot of the individual patterns or profiles, referred to as a spaghetti plot. A spaghetti plot is useful for determining whether there is variation from the average profile. This helps us to consider the set of random-effects to include in the model. Below we examine both the mean profile and individual profiles simultaneously.\n\nggplot(data = vocabulary_long, aes(x = grade, y = vocab_score)) +\n  geom_line(aes(group = id), alpha = 0.3) +                      #Add individual profiles\n  stat_summary(fun = mean, geom = \"line\", size = 2, group = 1) + #Add mean profile line\n  stat_summary(fun = mean, geom = \"point\", size = 3) +           #Add mean profile points\n  theme_light() +\n  scale_x_discrete(\n    name = \"Grade-level\",\n    labels = c(\"8th-grade\", \"9th-grade\", \"10th-grade\", \"11th-grade\")\n    ) +\n  ylab(\"Vocabulary score\")\n\n\n\n\n\n\n\nFigure 14.1: Plot showing the change in vocabulary score over time for 64 students. The average growth profile is displayed as a thicker line.\n\n\n\n\n\nBecause our research question is focused on examining the average change of vocabulary scores over time, we will focus on the mean profile. Based on this plot:\n\nThe average profile displays change over time that is positive (growth) and linear (or perhaps log-linear).",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>LMER: Average Change Over Time</span>"
    ]
  },
  {
    "objectID": "05-02-lmer-average-change-over-time.html#unconditional-random-intercepts-model",
    "href": "05-02-lmer-average-change-over-time.html#unconditional-random-intercepts-model",
    "title": "14  LMER: Average Change Over Time",
    "section": "14.3 Unconditional Random Intercepts Model",
    "text": "14.3 Unconditional Random Intercepts Model\nAs in a conventional fixed-effects regression analysis we begin a mixed-effects analysis by fitting the intercept-only model. This model is referred to as the unconditional random intercepts model or the unconditional means model. The model posits that there is no change over time in the average vocabulary score.\nThis model includes a fixed-effect of intercept and a random-effect of intercept, and no other predictors. This is the simplest model we can fit while still accounting for the dependence in the data (e.g., including a random-effect). The statistical model in this example can be expressed as:\n\\[\n\\mathrm{Vocabulary~Score}_{ij} = \\big[\\beta_0 + b_{0j}\\big] + \\epsilon_{ij}\n\\]\nwhere,\n\n\\(\\mathrm{Vocabulary~Score}_{ij}\\) is the vocabulary score for Student \\(j\\) at time point \\(i\\);\n\\(\\beta_0\\) is the fixed-effect of intercept;\n\\(b_{0j}\\) is the random-effect of intercept for Student \\(j\\); and\n\\(\\epsilon_{ij}\\) is the error for Student \\(j\\) at time point \\(i\\).\n\n\n\n14.3.1 Fitting and Interpreting the Model\nWe fit the model and display the output below. We include the argument REML=FALSE to force the lmer() function to produce maximum likelihood estimates (rather than restricted maximum likelihood estimates). In practice, we will generally want to fit these models using ML estimation.\n\n# Fit model\nlmer.0 = lmer(vocab_score ~ 1 + (1 | id), data = vocabulary_long, REML = FALSE)\n\nRecall that there are two types of fitted equations we can produce from this model: (1) the global fitted equation, and (2) student-specific fitted equation. The global fitted equation is based on only the fixed effects. We obtain the fixed-effects estimates using the tidy() function from the {broom.mixed} package with the argument effects=\"fixed\".\n\n# Fit model\nlmer.0 = lmer(vocab_score ~ 1 + (1 | id), data = vocabulary_long, REML = FALSE)\n\n# Coefficient-level output\ntidy(lmer.0, effects = \"fixed\")\n\n# A tibble: 1 × 5\n  effect term        estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)     2.53     0.231      11.0\n\n\nUsing the fixed-effects estimates, the global fitted equation based on the fixed-effects model is:\n\\[\n\\hat{\\mathrm{Vocabulary~Score}_{ij}} = 2.53\n\\]\nWe interpret coefficients from the fixed-effects model the same way we interpret coefficients produced from the lm() output. For example,\n\nThe predicted average vocabulary language score for all students at all time points is 2.53.\n\nWe can also write the student specific fitted equations. Each student specific fitted equation is based on the fixed-effects AND the student-specific random-effect. To obtain the student-specific random-effects we use the tidy() function from the {broom.mixed} package with the argument effects=\"ran_vals\".\n\n# Obtain random effects\ntidy(lmer.0, effects = \"ran_vals\")\n\n# A tibble: 64 × 6\n   effect   group level term        estimate std.error\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 ran_vals id    1     (Intercept)    0.359     0.629\n 2 ran_vals id    2     (Intercept)   -0.193     0.629\n 3 ran_vals id    3     (Intercept)   -1.24      0.629\n 4 ran_vals id    4     (Intercept)    1.13      0.629\n 5 ran_vals id    5     (Intercept)   -3.38      0.629\n 6 ran_vals id    6     (Intercept)   -1.63      0.629\n 7 ran_vals id    7     (Intercept)   -1.02      0.629\n 8 ran_vals id    8     (Intercept)   -1.62      0.629\n 9 ran_vals id    9     (Intercept)   -1.62      0.629\n10 ran_vals id    10    (Intercept)    0.915     0.629\n# ℹ 54 more rows\n\n\nFor example, Student 1’s fitted equation is:\n\\[\n\\begin{split}\n\\hat{\\mathrm{Vocabulary~Score}_{i1}} &= 2.53 + 0.359 \\\\[1ex]\\hat{\\mathrm{Vocabulary~Score}_{i1}} &= 2.53 + 0.359 \\\\[1ex]\n&= 2.89\n\\end{split}\n\\]\n\nThe predicted average vocabulary language score for Student 1 at all time points is 2.889.\n\nThe argument effects=\"ran_coefs\" in the tidy() function gives the student-specific intercepts directly.\n\n# Obtain student-specific coefficients\ntidy(lmer.0, effects = \"ran_coefs\")\n\n# A tibble: 64 × 5\n   effect    group level term        estimate\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1 ran_coefs id    1     (Intercept)    2.89 \n 2 ran_coefs id    2     (Intercept)    2.34 \n 3 ran_coefs id    3     (Intercept)    1.29 \n 4 ran_coefs id    4     (Intercept)    3.66 \n 5 ran_coefs id    5     (Intercept)   -0.852\n 6 ran_coefs id    6     (Intercept)    0.907\n 7 ran_coefs id    7     (Intercept)    1.52 \n 8 ran_coefs id    8     (Intercept)    0.913\n 9 ran_coefs id    9     (Intercept)    0.913\n10 ran_coefs id    10    (Intercept)    3.45 \n# ℹ 54 more rows\n\n\nFrom here, we see that Student 2’s fitted equation is:\n\\[\n\\hat{\\mathrm{Vocabulary~Score}_{i2}} = 2.34\n\\]\nThe predicted average vocabulary language score for Student 2 (at all time points) is 2.34. From the two students’ fitted equations we see that Student 1’s “growth” curve is higher than the average “growth” curve and Student 2’s “growth” curve is lower than the average “growth” curve. Here is a plot of those three “growth” curves.\n\n\nCode\nggplot(data = vocabulary_long, aes(x = grade, y = vocab_score)) +\n  geom_point(alpha = 0) +\n  geom_hline(yintercept = 2.53) + #Global 'growth' curve\n  geom_hline(yintercept = 2.89, linetype = \"dashed\", color = \"#0072B2\") + #Student 1 'growth' curve\n  geom_hline(yintercept = 2.34, linetype = \"dashed\", color = \"#E69F00\") + #Student 1 'growth' curve\n  theme_light() +\n  scale_x_discrete(\n    name = \"Grade-level\",\n    labels = c(\"8th-grade\", \"9th-grade\", \"10th-grade\", \"11th-grade\")\n    ) +\n  ylab(\"Vocabulary score\") +\n  ylim(0, 4)\n\n\n\n\n\n\n\n\nFigure 14.2: Predicted change in vocabulary score over time based on the unconditional random intercepts model for all students (solid, black line), Student 1 (blue, dashed line), and Student 2 (orange, dashed line).\n\n\n\n\n\n\n\n\n14.3.2 Partitioning Unexplained Variation\nThis model is not very informative for applied researchers in terms of characterizing change over time, but it is essential to decompose the variation into within- and between-subjects variation. In our model, there are two sources of variation. The first source of variation measures the between-student variation (how different are students’ individual “growth” curves). The second source of variation measures the within-student variation (how different are an individual’s outcome values from their individual “growth” curve).\nTo see this graphically, we show Student 1’s data along with the global “growth” curve and Student 1’s specific “growth” curves. The between-student variation is akin to the average of the squared deviations between each student-specific “growth” curve and the global “growth” curve. The left-hand plot below shows these residuals for Student 1, referred to as Level-2 residuals. The within-student variation is akin to the average of the squared deviations between each student’s outcome (vocabulary scores) and that student’s specific “growth” curve. The right-hand plot below shows these residuals for Student 1, referred to as Level-1 residuals.\n\n\n\n\n\n\n\n\nFigure 14.3: Student 1’s vocabulary scores at each time point along with the predicted change in vocabulary score over time based on the unconditional random intercepts model for all students (solid, black line) and Student 1 (blue, dashed line). The between-student variation (LEFT) and within-student variation (RIGHT) based on the residuals for Student 1 are also displayed.\n\n\n\n\n\nWe quantify the amount of between- and within-student variation using an estimate of variance. The between-student variance estimate is denoted \\(\\sigma^2_{b_0}\\) (or simply \\(\\sigma^20\\)), and the within-student variance estimate is denoted \\(\\sigma^2_{\\epsilon}\\). We can access these paramater estimates using the argument effects=\"ran_pars\" in the tidy() function.\n\n# Obtain variance estimates\ntidy(lmer.0, effects = \"ran_pars\")\n\n# A tibble: 2 × 4\n  effect   group    term            estimate\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 ran_pars id       sd__(Intercept)     1.72\n2 ran_pars Residual sd__Observation     1.35\n\n\nNote that the output is in the standard deviation metric. We need to square these values to get the actual variance estimates.\nBetween-Student Variance:\n\\[\n\\hat\\sigma^2_0 = 1.72^2 = 2.96\n\\]\nWithin-Student Variance:\n\\[\n\\hat\\sigma^2_{\\epsilon} = 1.35^2 = 1.82\n\\]\nThese values represent the unexplained variation at each level. The total unexplained variation is the sum of these two terms.\n\n# Total unexplained variation\n2.96 + 1.82\n\n[1] 4.78\n\n\nWe can compute the proportion of unexplained variation at each level by dividing the appropriate variance estimate by this total.\n\n# Proportion of unexplained variation that is between-subjects\n2.96 / (2.96 + 1.82)\n\n[1] 0.6192469\n\n# Proportion of unexplained variation that is within-subjects\n1.82 / (2.96 + 1.82)\n\n[1] 0.3807531\n\n\nInterpreting these values:\n\nRoughly 62% of the unexplained variation in vocabulary scores is between-student variation.\nRoughly 38% of the unexplained variation in vocabulary scores is within-student variation.\n\nBased on this partitioning from the unconditional random intercepts model we have evidence that it may be fortuitous to include both between-student and within-student predictors; there is unaccounted for variation at both levels. To explain the unaccounted for between-student variation, include between-student-level predictors (e.g., female) in the model. To explain the unaccounted for within-student variation, include within-student predictors (e.g., grade) in the model.\nSince the larger proportion of unexplained variation is between-student variation, we might ultimately focus on between-student predictors more than within-student predictors. However, because this is a longitudinal analysis, our primary focus is on the grade predictor, which is a within-subjects predictor. It is here that we turn next.\n\nFYI\nThis partitioning of variation should be done in every analysis, and ALWAYS is done using the unconditional random intercepts model. The unconditional random intercepts model will serve as our baseline model. As we add predictors, we can compare the unexplained variation at each level in the predictor models to the baseline unaccounted for variation in the unconditional means model. This is one way of measuring how effective predictors are at further explaining variation in the model.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>LMER: Average Change Over Time</span>"
    ]
  },
  {
    "objectID": "05-02-lmer-average-change-over-time.html#modeling-unconditional-growth-model",
    "href": "05-02-lmer-average-change-over-time.html#modeling-unconditional-growth-model",
    "title": "14  LMER: Average Change Over Time",
    "section": "14.4 Modeling: Unconditional Growth Model",
    "text": "14.4 Modeling: Unconditional Growth Model\nTo model change in the outcome over time, we include time as a fixed-effect in the LMER model. In this data set, the time predictor is grade, which is a categorical predictor. We could create dummy variables, or simply add grade into the model and let R choose the reference group alphabetically (vocab_08 in this example). The statistical model in this example can be expressed as:\n\\[\n\\mathrm{Vocabulary~Score}_{ij} = \\big[\\beta_0 + b_{0j}\\big] + \\beta_1(\\mathrm{Vocab\\_09}_{ij}) + \\beta_2(\\mathrm{Vocab\\_10}_{ij}) + \\beta_3(\\mathrm{Vocab\\_11}_{ij}) + \\epsilon_{ij}\n\\]\nwhere,\n\n\\(\\mathrm{Vocabulary~Score}_{ij}\\) is the vocabulary score at time point \\(i\\) for student \\(j\\);\n\\(\\beta_0\\) is the fixed-effect of intercept;\n\\(b_{0j}\\) is the random-effect of intercept for student \\(j\\);\n\\(\\mathrm{Vocab\\_09}_{ij}\\), \\(\\mathrm{Vocab\\_10}_{ij}\\), and \\(\\mathrm{Vocab\\_11}_{ij}\\) are dummy coded variable indicating grade-level,\n\\(\\beta_1\\) is the effect of 9th-grade (i.e., mean vocabulary score difference between 8th- and 9th-grade),\n\\(\\beta_2\\) is the effect of 10th-grade (i.e., mean vocabulary score difference between 8th- and 10th-grade),\n\\(\\beta_3\\) is the effect of 11th-grade (i.e., mean vocabulary score difference between 8th- and 11th-grade), and\n\\(\\epsilon_{ij}\\) is the error at time point \\(i\\) for student \\(j\\).\n\nFitting the model:\n\n# Fit unconditional growth model\nlmer.1 = lmer(vocab_score ~ 1 + grade + (1|id), data = vocabulary_long, REML = FALSE)\n\n# Coefficient-level output\ntidy(lmer.1, effects = \"fixed\")\n\n# A tibble: 4 × 5\n  effect term          estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)       1.13     0.250      4.52\n2 fixed  gradevocab_09     1.41     0.159      8.87\n3 fixed  gradevocab_10     1.86     0.159     11.7 \n4 fixed  gradevocab_11     2.34     0.159     14.7 \n\n\nThe fitted equation is:\n\\[\n\\hat{\\mathrm{Vocabulary~Score}_{ij}} = 1.13 + 1.41(\\mathrm{Vocab\\_09}_{ij}) + 1.86(\\mathrm{Vocab\\_10}_{ij}) + 2.34(\\mathrm{Vocab\\_11}_{ij})\n\\]\nInterpreting the coefficients,\n\nThe predicted average vocabulary score for 8th-grade students (intercept) is 1.13.\nOn average, 9th-grade students have a vocabulary score that is 1.41-points higher than 8th-grade students.\nOn average, 10th-grade students have a vocabulary score that is 1.86-points higher than 8th-grade students.\nOn average, 11th-grade students have a vocabulary score that is 2.34-points higher than 8th-grade students.\n\nLooking at the variance components we find:\n\n# SD estimates\ntidy(lmer.1, effects = \"ran_pars\")\n\n# A tibble: 2 × 4\n  effect   group    term            estimate\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 ran_pars id       sd__(Intercept)    1.79 \n2 ran_pars Residual sd__Observation    0.899\n\n# Variance estimates\n1.791 ^ 2 #Between-student variance\n\n[1] 3.207681\n\n0.899 ^ 2 #Within-Student variance\n\n[1] 0.808201\n\n\nComparing the variances to those from the unconditional random intercepts model:\nWithin-Student Variance (\\(\\hat\\sigma^2_{\\epsilon}\\))\nThe unexplained variance that is within-students has decreased from 1.82 to 0.81. Including grade-level in the model has explained 55.8% of the within-student variation. This is because grade is a within-student predictor (it has values that vary within each student).\nBetween-Student Variance (\\(\\hat\\sigma^2_{0}\\))\nThe unexplained variance that is between-students has increased from 2.96 to 3.21. This means that by including grade in the model we have increased the amount of unexplained variation that is between students! (The model explained \\(-8.7\\)% of the between-student variance.) This is a mathematical artifact of the estimation process.\n\n\n14.4.1 Repeated-Measures ANOVA (RM-ANOVA)\nOne historic method of analyzing longitudinal data is Repeated Measures Analysis of Variance (RM-ANOVA).1 The linear mixed-effects model that includes time as one or more categorical predictors and a random-effect of intercept produces the same results as the RM-ANOVA.\n\nWARNING\nYou should not use RM-ANOVA to analyze longitudinal data. It requires a condition called sphericity that makes some stringent assumptions about the variances and correlations between repeated measures. One requirement of sphericity is that the correlation between any two time points are exactly the same. Consider the correlations of our data:\n\n# Compute correlations between repeated measures\nvocabulary |&gt;\n  select(vocab_08:vocab_11) |&gt;\n  correlate()\n\n# A tibble: 4 × 5\n  term     vocab_08 vocab_09 vocab_10 vocab_11\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 vocab_08   NA        0.810    0.866    0.782\n2 vocab_09    0.810   NA        0.785    0.757\n3 vocab_10    0.866    0.785   NA        0.811\n4 vocab_11    0.782    0.757    0.811   NA    \n\n\nThe correlations are not equal, and are likely not to be equal in the population! In repeated measures data, observations that are more closely spaced in time tend to be more correlated. This violates the sphericity assumption needed for RM-ANOVA. It is rare that sphericity is ever tenablein practice with longitudinal data. Because of this it is preferable to use LMER to carry out these analyses.\n\n\n\n\n14.4.2 Quantitative Time Predictor: A More Flexible Model for Repeated Measures Data\nOne advantage to using the linear mixed-effects model to analyze repeated measures data over traditional methods (e.g., RM-ANOVA or MANOVA) is that the regression model allows for both categorical and quantitative variables. For example, rather than code our grade-levels categorically (as vocab_08, vocab_09, vocab_10 and vocab_11), which was a necessity in days of yore, we could have simply coded them as 8, 9, 10, and 11. Then we could have fitted the LMER model using this quantitative predictor. The statistical model when time is quantitative would be:\n\\[\n\\mathrm{Vocabulary~Score}_{ij} = \\big[\\beta_0 + b_{0j}\\big] + \\beta_1(\\mathrm{Grade}_{ij}) + \\epsilon_{ij}\n\\]\nwhere,\n\n\\(\\mathrm{Vocabulary~Score}_{ij}\\) is the vocabulary score at time point \\(i\\) for student \\(j\\);\n\\(\\beta_0\\) is the fixed-effect of intercept;\n\\(b_{0j}\\) is the random-effect of intercept for student \\(j\\);\n\\(\\mathrm{Grade}_{ij}\\) is a quantitative variable indicating grade-level,\n\\(\\beta_1\\) is the effect of a one-unit change in grade, and\n\\(\\epsilon_{ij}\\) is the error at time point \\(i\\) for student \\(j\\).\n\nThis is still referred to as the unconditional growth model since the only predictor is a fixed-effect of time.\n\n\n\n14.4.3 Lookup Table: Mapping Categories to Quantities\nOne method to convert grade to a quantitative variable is to create a lookup table. A lookup table maps the levels of the categorical time predictor to the values we want to use in our new quantitative predictor. Below I create a lookup table to map the categorical time predictor to the relevant grade-level (grade_quant).\n\n# Create lookup table\nlookup_table = data.frame(\n  grade = c(\"vocab_08\", \"vocab_09\", \"vocab_10\", \"vocab_11\"),\n  grade_quant = c(8, 9, 10, 11)\n)\n\n# View lookup table\nlookup_table\n\n     grade grade_quant\n1 vocab_08           8\n2 vocab_09           9\n3 vocab_10          10\n4 vocab_11          11\n\n\nThen, we join (or merge) the tidy/long data with the lookup table. This adds the quantitative variables (with the correct mapping) to our tidy data. There are, of course, other ways to accomplish the same thing. For example a mutate() using the case_when() function could also be used to create this mapping.\n\n# Join the data with the lookup table\nvocabulary_long = vocabulary_long |&gt;\n  left_join(lookup_table, by = \"grade\")\n\n# View data\nvocabulary_long\n\n# A tibble: 256 × 5\n      id female grade    vocab_score grade_quant\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n 1     1      1 vocab_08        1.75           8\n 2     1      1 vocab_09        2.6            9\n 3     1      1 vocab_10        3.76          10\n 4     1      1 vocab_11        3.68          11\n 5     2      0 vocab_08        0.9            8\n 6     2      0 vocab_09        2.47           9\n 7     2      0 vocab_10        2.44          10\n 8     2      0 vocab_11        3.43          11\n 9     3      0 vocab_08        0.8            8\n10     3      0 vocab_09        0.93           9\n# ℹ 246 more rows\n\n\nIn this join, we are adding records from the lookup_table data (right data) to the vocabulary_long data (left data).2 When we perform this left join, the final joined data will include all records from vcabulary_long (left dataset) but only those records from the lookup_table dataset whose key (grade value) is contained in the student data. For example, if the lookup_table data included information about a grade level that was not included in the vocabulary_long data, it would not appear in the joined data.\nThe {tidyverse} package includes six different join functions. You can read about four common join functions and see an animation illustrating them here.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>LMER: Average Change Over Time</span>"
    ]
  },
  {
    "objectID": "05-02-lmer-average-change-over-time.html#fitting-the-unconditional-growth-model-with-a-quantitative-time-predictor",
    "href": "05-02-lmer-average-change-over-time.html#fitting-the-unconditional-growth-model-with-a-quantitative-time-predictor",
    "title": "14  LMER: Average Change Over Time",
    "section": "14.5 Fitting the Unconditional Growth Model with a Quantitative Time Predictor",
    "text": "14.5 Fitting the Unconditional Growth Model with a Quantitative Time Predictor\nBelow we fit the linear mixed-effects model using the grade_quant predictor.\n\n# Fit unconditional growth model\nlmer.2 = lmer(vocab_score ~ 1 + grade_quant + (1|id), data = vocabulary_long, REML = FALSE)\n\n# Coefficient-level output\ntidy(lmer.2, effects = \"fixed\")\n\n# A tibble: 2 × 5\n  effect term        estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)   -4.56     0.553      -8.24\n2 fixed  grade_quant    0.747    0.0529     14.1 \n\n\nThe fitted equation is:\n\\[\n\\hat{\\mathrm{Vocabulary~Score}_{ij}} = -4.56 + 0.75(\\mathrm{Grade\\mbox{-}level}_{ij})\n\\]\nThe model using the quantitative predictor of grade-level is simpler than the model using the categorical version of grade-level since it has two fewer fixed-effects to estimate (fewer degrees-of-freedom = more parsimonious).\nInterpreting the coefficients,\n\nThe predicted average vocabulary score for 0th-grade students (intercept) is \\(-4.55\\) (extrapolation).\nEach one-unit difference in grade-level is associated with a 0.75-point difference in vocabulary score, on average.\n\nLooking at the variance components:\n\n# SD\ntidy(lmer.2, effects = \"ran_pars\")\n\n# A tibble: 2 × 4\n  effect   group    term            estimate\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 ran_pars id       sd__(Intercept)    1.78 \n2 ran_pars Residual sd__Observation    0.947\n\n# Compute variance components\n1.784 ^ 2 #Between-student\n\n[1] 3.182656\n\n0.947 ^ 2 #Within-student\n\n[1] 0.896809\n\n\nComparing these values to the unconditional random intercepts model:\nWithin-Student Variance (\\(\\hat\\sigma^2_{\\epsilon}\\))\nThe unexplained variance that is within-students has decreased from 1.82 to 0.90. Including grade-level in the model has explained 50.8% of the within-student variation. This is because grade is a within-student predictor (it has values that vary within each student).\nBetween-Student Variance (\\(\\hat\\sigma^2_{0}\\))\nThe unexplained variance that is between-students has increased from 2.96 to 3.18. This means that by including grade in the model we have increased the amount of unexplained variation that is between students by 7.8%; a mathematical artifact of the estimation process.\nThese are similar to the variance components obtained from the model using the categorical predictors of grade level.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>LMER: Average Change Over Time</span>"
    ]
  },
  {
    "objectID": "05-02-lmer-average-change-over-time.html#centering-the-time-predictor-better-interpretations-of-the-intercept",
    "href": "05-02-lmer-average-change-over-time.html#centering-the-time-predictor-better-interpretations-of-the-intercept",
    "title": "14  LMER: Average Change Over Time",
    "section": "14.6 Centering the Time Predictor: Better Interpretations of the Intercept",
    "text": "14.6 Centering the Time Predictor: Better Interpretations of the Intercept\nOne method of improving the interpretations of the intercept is to center the time predictor. Recall that centering a variable means to add/subtract a constant value from each case. In a longitudinal analysis, we typically center the time predictor by subtracting the value of the time predictor at baseline (i.e., the first time point). In our example:\n\\[\n\\mathrm{Centered~Grade}_{ij} = \\mathrm{Grade}_{ij} - 8\n\\]\nThis essentially maps the values of {8, 9, 10, 11} in the quant_grade variable to {0, 1, 2, 3}. Then we include this centered predictor as fixed-effect in the model. Mathematically, the model is:\n\\[\n\\hat{\\mathrm{Vocabulary~Score}_{ij}} = \\bigg[\\beta_0 + b_0\\bigg]  + \\beta_1(\\mathrm{Grade\\mbox{-}level}_{ij} - 8) + \\epsilon_{ij}\n\\]\nFitting this model:\n\n# Fit unconditional growth model with centered grade\nlmer.3 = lmer(vocab_score ~ 1 + I(grade_quant-8) + (1|id), data = vocabulary_long, REML = FALSE)\n\n# Coefficient-level output\ntidy(lmer.3, effects = \"fixed\")\n\n# A tibble: 2 × 5\n  effect term               estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)           1.41     0.244       5.79\n2 fixed  I(grade_quant - 8)    0.747    0.0529     14.1 \n\n\nThe fitted equation is:\n\\[\n\\hat{\\mathrm{Vocabulary~Score}_{ij}} = 1.41 + 0.75(\\mathrm{Grade\\mbox{-}level}_{ij}-8)\n\\]\nInterpreting the coefficients,\n\nThe predicted average vocabulary score for 8th-grade students is 1.41. Centering removes the problem of extrapolation in the interpretation because we have now made 0 a legitimate value in the predictor.\nEach one-unit difference in grade-level is associated with a 0.75-point difference in vocabulary score, on average. This is identical to the previous model since we have not changed what a one-unit difference in the predictor represents.\n\nWe can see why the intercepts are different and the slopes are the same by comparing the plots of the individual growth profiles and the fitted fixed-effects models for the uncentered and centered predictors.\n\n\nCode\np1 = ggplot(data = vocabulary_long, aes(x = grade_quant, y = vocab_score)) +\n  geom_line(aes(group = id), alpha = 0.3) +\n  geom_abline(intercept = -4.56, slope = 0.75, color = \"blue\") +\n  geom_point(x = 0, y = -4.56, size = 1.5, color = \"blue\") +\n  theme_light() +\n  scale_x_continuous(\n    name = \"Grade-level\",\n    limits = c(0, 11),\n    breaks = c(0, 2, 4, 6, 8, 10)\n    ) +\n  scale_y_continuous(\n    name = \"Vocabulary score\",\n    limits = c(-5, 10)\n    )\n\np2 = ggplot(data = vocabulary_long, aes(x = I(grade_quant-8), y = vocab_score)) +\n  geom_line(aes(group = id), alpha = 0.3) +\n  geom_abline(intercept = 1.41, slope = 0.75, color = \"blue\") +\n  geom_point(x = 0, y = 1.41, size = 1.5, color = \"blue\") +\n  theme_light() +\n  scale_x_continuous(\n    name = \"Centered grade-level (0 = 8th grade)\",\n    limits = c(0, 11),\n    breaks = c(0, 2, 4, 6, 8, 10)\n    ) +\n  scale_y_continuous(\n    name = \"Vocabulary score\",\n    limits = c(-5, 10)\n    )\n\n# Display plots\np1 | p2\n\n\n\n\n\n\n\n\nFigure 14.4: Plot showing the change in vocabulary score over time for 64 students. The average growth profile is also displayed. This is shown for the non-centered (left) and 8th-grade centered (right) grade-level. A large blue point is shown at the intercept value in both plots.\n\n\n\n\n\nLooking at the variance components:\n\n# SD\ntidy(lmer.3, effects = \"ran_pars\")\n\n# A tibble: 2 × 4\n  effect   group    term            estimate\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 ran_pars id       sd__(Intercept)    1.78 \n2 ran_pars Residual sd__Observation    0.947\n\n# Compute variance components\n1.784 ^ 2 #Between-students\n\n[1] 3.182656\n\n0.947 ^ 2 #Within-students\n\n[1] 0.896809\n\n\nComparing them to the unconditional random intercepts model:\nWithin-Student Variance (\\(\\hat\\sigma^2_{\\epsilon}\\))\nThe unexplained variance that is within-students has decreased from 1.82 to 0.90. Including grade-level in the model has explained 50.8% of the within-student variation. This is because grade is a within-student predictor (it has values that vary within each student).\nBetween-Student Variance (\\(\\hat\\sigma^2_{0}\\))\nThe unexplained variance that is between-students has increased from 2.96 to 3.18. This means that by including grade in the model we have increased the amount of unexplained variation that is between students by 7.8%; a mathematical artifact of the estimation process.\nThese values are identical to the variance components obtained from the previous model. That is because the model we fitted is mostly identical to the uncentered quantitative model. The only difference is in the intercept value, which now is interpretable since a grade of 0 on the centered scale corresponds to the initial time point in the data; it is no longer extrapolation.\n\n\n14.6.1 Student-Specific Profiles\nObtaining the coefficients for the student specific growth curves we get:\n\n# Get student estimated parameters\ntidy(lmer.3, effects = \"ran_coefs\") |&gt;\n  arrange(as.numeric(level))\n\n# A tibble: 128 × 5\n   effect    group level term               estimate\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1 ran_coefs id    1     (Intercept)          1.80  \n 2 ran_coefs id    1     I(grade_quant - 8)   0.747 \n 3 ran_coefs id    2     (Intercept)          1.20  \n 4 ran_coefs id    2     I(grade_quant - 8)   0.747 \n 5 ran_coefs id    3     (Intercept)          0.0743\n 6 ran_coefs id    3     I(grade_quant - 8)   0.747 \n 7 ran_coefs id    4     (Intercept)          2.63  \n 8 ran_coefs id    4     I(grade_quant - 8)   0.747 \n 9 ran_coefs id    5     (Intercept)         -2.24  \n10 ran_coefs id    5     I(grade_quant - 8)   0.747 \n# ℹ 118 more rows\n\n\nThe fitted equations for the student-specific growth curves for Student 1 and Student 2 are:\n\\[\n\\begin{split}\n\\mathbf{Student~1:~} \\hat{\\mathrm{Vocabulary~Score}_{ij}} &= 1.80 + 0.75(\\mathrm{Grade\\mbox{-}level}_{ij}-8)\\\\[1ex]\n\\mathbf{Student~2:~} \\hat{\\mathrm{Vocabulary~Score}_{ij}} &= 1.20 + 0.75(\\mathrm{Grade\\mbox{-}level}_{ij}-8)\n\\end{split}\n\\]\nThe student-specific growth rates are identical for all students (i.e., the slope value is the same in all the equations), and also is the same as the average global growth rate. Including a random-effect of intercept, although accounting for the within-student correlation among vocabulary scores, does not allow students to have different rates-of-growth. The difference is in the intercept values, which indicates where students start (i.e., students’ initial vocabulary score in the 8th grade).\nWe can see these same things by plotting these student specific growth curves along with the global average growth curve.\n\n# Create plot\nggplot(data = vocabulary_long, aes(x = I(grade_quant-8), y = vocab_score)) +\n  geom_point(alpha = 0) +\n  geom_abline(intercept = 1.41, slope = 0.75, color = \"black\") +     # Average growth curve\n  geom_abline(intercept = 1.80, slope = 0.75, color = \"#0072B2\") +   # Student 1\n  geom_abline(intercept = 1.20, slope = 0.75, color = \"#E69F00\") +   # Student 2\n  theme_light() +\n  scale_x_continuous(\n    name = \"Grade-level\",\n    breaks = c(0, 1, 2, 3),\n    labels = c(\"0\\n(8th)\", \"1\\n(9th)\", \"2\\n(10th)\", \"3\\n(11th)\")\n    ) +\n  ylab(\"Vocabulary score\")\n\n\n\n\n\n\n\nFigure 14.5: Predicted change in vocabulary score over time based on the linear (centered) unconditional growth model for all students (solid, black line), Student 1 (blue, dashed line), and Student 2 (orange, dashed line).\n\n\n\n\n\nThe parallel lines suggest the same rate-of-growth across students.\n\n\n\n14.6.2 Comparing the Unconditional Growth Models\nThe unconditional growth model was fitted using three different methods.\n\nThe first model treated time (grade) as categorical.\nThe second two models treated time as a continuous variable.\n\nTreating time continuously rather than as a categorical predictor has many advantages:\n\nIn the real-world time is continuous.\nThe model is simpler. In the model where time was treated categorically, we had to estimate four regression coefficients and two variance components. In the continuous models, we only had to estimate two regression coefficients and two variance components. If there were additional time points, we would still only need to estimate four parameters for the continuous model, but the number of estimates would increase for the categorical model.\nIt allows us to include participants being measured at different times.\nIt allows us to model nonlinear relationships more easily.\n\n\nFYI\nIn general, you should always treat time continuously when you have a longitudinal study! This guidance also implies that you should use linear mixed-effects models rather than RM-ANOVA for the analysis of longitudinal data.\nMoreover, because of the interpretive value of the intercept when we center the grade-level predictor at the first time point, it is often a good idea to center your time predictor.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>LMER: Average Change Over Time</span>"
    ]
  },
  {
    "objectID": "05-02-lmer-average-change-over-time.html#re-visiting-the-functional-form-of-the-growth-model",
    "href": "05-02-lmer-average-change-over-time.html#re-visiting-the-functional-form-of-the-growth-model",
    "title": "14  LMER: Average Change Over Time",
    "section": "14.7 Re-Visiting the Functional Form of the Growth Model",
    "text": "14.7 Re-Visiting the Functional Form of the Growth Model\nAs in any regression analysis, we need to specify the functional form of the growth model. Since the spaghetti plot suggested that the relationship between grade-level and vocabulary score may be nonlinear, below we consider three potential functional forms for our growth model (using centered grade level):\n\nA linear relationship (already fitted: lmer.3);\nA quadratic relationship; and\nA log-linear relationship (based on log-transforming grade-level).\n\nNote: Since the centered grade predictor includes values of zero, we need to add one to each value prior to log-transforming this predictor\n\n# Quadratic model\nlmer.4 = lmer(vocab_score ~ 1 + I(grade_quant-8) + I((grade_quant-8)^2) + (1|id),\n              data = vocabulary_long, REML = FALSE)\n\n# Log-linear model\nlmer.5 = lmer(vocab_score ~ 1 + log((grade_quant-8) + 1) + (1|id),\n              data = vocabulary_long, REML = FALSE)\n\n\n\n14.7.1 Evaluating the Three Functional Forms\nAs with any model, we want to examine the evidence to determine the correct functional form. In practice, you would also want to evaluate the residuals to see which of the potential candidate models meets the assumptions. (We will look at residuals in an upcoming set of notes.) Suffice it to say, the residual plots look similar across all three models indicating that all the models seem to meet the assumptions equally well. Since we are using the same outcome and data in all three models, we can also evaluate the models using information criteria and their related metrics.\n\n# Model-evidence\naictab(\n  cand.set = list(lmer.0, lmer.3, lmer.4, lmer.5),\n  modnames = c(\"No change\", \"Linear growth\", \"Quadratic growth\", \"Log-linear growth\")\n)\n\n\nModel selection based on AICc:\n\n                  K    AICc Delta_AICc AICcWt Cum.Wt      LL\nLog-linear growth 4  864.30       0.00   0.79   0.79 -428.07\nQuadratic growth  5  866.96       2.67   0.21   1.00 -428.36\nLinear growth     4  880.86      16.57   0.00   1.00 -436.35\nNo change         3 1015.35     151.06   0.00   1.00 -504.63\n\n\nGiven the data and candidate models, the evidence primarily supports the log-linear model. There is also some evidence for the quadratic model and almost no evidence for the linear model. This is consistent with the nonlinearity we observed in the mean profile in the spaghetti plot. Given this, the higher evidence for the log-linear model, and the simplicity of the log-linear model relative to the quadratic model, we will adopt the log-linear functional form for our unconditional growth model.\n\n\n\n14.7.2 Examining the Output for the Adopted Log-Linear Fitted Model\n\n# Coefficient-level output\ntidy(lmer.5, effects = \"fixed\")\n\n# A tibble: 2 × 5\n  effect term                       estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;                         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)                    1.21     0.246      4.91\n2 fixed  log((grade_quant - 8) + 1)     1.67     0.109     15.3 \n\n\nThe fitted equation is:\n\\[\n\\hat{\\mathrm{Vocabulary~Score}_{ij}} = 1.21 + 1.67\\bigg[\\ln(\\mathrm{Grade\\mbox{-}level}_{ij}-7)\\bigg]\n\\]\nInterpreting the coefficients,\n\nThe predicted average vocabulary score for 8th-grade students is 1.21. Remember that the centered value for 8th-grade is 0, which results in \\(1.21 + 1.67\\bigg[\\ln(1)\\bigg] = 1.21 + 1.67(0) = 1.21\\).\nSince we used the natural log, we can interpret the change in X as a percent change and the change in Y as \\(\\hat{\\beta_1}/100\\); Each one-percent difference in grade-level is associated with a 0.0166-point difference in vocabulary score, on average.\n\nLooking at the variance components:\n\n# SD\ntidy(lmer.5, effects = \"ran_pars\")\n\n# A tibble: 2 × 4\n  effect   group    term            estimate\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 ran_pars id       sd__(Intercept)    1.79 \n2 ran_pars Residual sd__Observation    0.907\n\n# Compute variance components\n1.790 ^ 2 #Between-student\n\n[1] 3.2041\n\n0.907 ^ 2 #Within-student\n\n[1] 0.822649\n\n\nComparing them to the unconditional random intercepts model:\nWithin-Student Variance (\\(\\hat\\sigma^2_{\\epsilon}\\))\nThe unexplained variance that is within-students has decreased from 1.82 to 0.82, a 54.9% explanation of the within-student variation.\nBetween-Student Variance (\\(\\hat\\sigma^2_{0}\\))\nThe unexplained variance that is between-students has increased from 2.96 to 3.20, an increase of 8.1% in the unexplained between-students variation.\nThese values are quite similar to the variance components obtained from the other unconditional growth models since time (grade level) is the only effect included in the model.\n\n\n\n14.7.3 Plot of the Unconditional Growth Model\nTo better understand the relationship between grade-level and vocabulary score represented in the adopted unconditional growth model, we can plot the predicted mean profile based on the model’s fixed-effects.\n\n# Create plot\nggplot(data = vocabulary_long, aes(x = I(grade_quant-8), y = vocab_score)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {1.21 + 1.67 * log(x + 1)},\n    color = \"blue\"\n    ) +\n  theme_light() +\n  scale_x_continuous(\n    name = \"Grade-level\",\n    breaks = c(0, 1, 2, 3),\n    labels = c(\"0\\n(8th)\", \"1\\n(9th)\", \"2\\n(10th)\", \"3\\n(11th)\")\n    ) +\n  ylab(\"Vocabulary score\")\n\n\n\n\n\n\n\nFigure 14.6: Predicted change in vocabulary score as a function of grade-level.\n\n\n\n\n\n\n\n\n14.7.4 Student-Specific Profiles\nObtaining the coefficients for the student specific growth curves we get:\n\n# Get student estimated parameters\ntidy(lmer.5, effects = \"ran_coefs\") |&gt;\n  arrange(as.numeric(level))\n\n# A tibble: 128 × 5\n   effect    group level term                       estimate\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                         &lt;dbl&gt;\n 1 ran_coefs id    1     (Intercept)                   1.60 \n 2 ran_coefs id    1     log((grade_quant - 8) + 1)    1.67 \n 3 ran_coefs id    2     (Intercept)                   1.00 \n 4 ran_coefs id    2     log((grade_quant - 8) + 1)    1.67 \n 5 ran_coefs id    3     (Intercept)                  -0.137\n 6 ran_coefs id    3     log((grade_quant - 8) + 1)    1.67 \n 7 ran_coefs id    4     (Intercept)                   2.43 \n 8 ran_coefs id    4     log((grade_quant - 8) + 1)    1.67 \n 9 ran_coefs id    5     (Intercept)                  -2.46 \n10 ran_coefs id    5     log((grade_quant - 8) + 1)    1.67 \n# ℹ 118 more rows\n\n\nThe fitted equations for the student-specific growth curves for Student 1 and Student 2 are:\n\\[\n\\begin{split}\n\\mathbf{Student~1:~} \\hat{\\mathrm{Vocabulary~Score}_{ij}} &= 1.60 + 1.67\\bigg[\\ln(\\mathrm{Grade\\mbox{-}level}_{ij}-7)\\bigg]\\\\[1ex]\n\\mathbf{Student~2:~} \\hat{\\mathrm{Vocabulary~Score}_{ij}} &= 1.00 + 1.67\\bigg[\\ln(\\mathrm{Grade\\mbox{-}level}_{ij}-7)\\bigg]\n\\end{split}\n\\]\nAgain, because we only included a random-effect of intercept the student-specific growth rates are identical for all students (i.e., the slope value is the same in all the equations), and also is the same as the average global growth rate. The random-effect of intercept does allow students to have different intercept values (i.e., students’ initial vocabulary score in the 8th grade can differ).\nWe can see these same things by plotting these student specific growth curves along with the global average growth curve.\n\n# Create plot\nggplot(data = vocabulary_long, aes(x = I(grade_quant-8), y = vocab_score)) +\n  geom_point(alpha = 0) +\n  # Average growth curve\n  geom_function(\n    fun = function(x) {1.21 + 1.67 * log(x + 1)},\n    color = \"black\"\n    ) +\n  # Student 1\n  geom_function(\n    fun = function(x) {1.60 + 1.67 * log(x + 1)},\n    color = \"#0072B2\"\n    ) +\n  # Student 2\n  geom_function(\n    fun = function(x) {1.00 + 1.67 * log(x + 1)},\n    color = \"#E69F00\"\n    ) +\n  theme_light() +\n  scale_x_continuous(\n    name = \"Grade-level\",\n    breaks = c(0, 1, 2, 3),\n    labels = c(\"0\\n(8th)\", \"1\\n(9th)\", \"2\\n(10th)\", \"3\\n(11th)\")\n    ) +\n  ylab(\"Vocabulary score\")\n\n\n\n\n\n\n\nFigure 14.7: Predicted change in vocabulary score over time based on the log-linear unconditional growth model for all students (solid, black line), Student 1 (blue, dashed line), and Student 2 (orange, dashed line).\n\n\n\n\n\nThe parallel curves suggest the same rate-of-growth across students.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>LMER: Average Change Over Time</span>"
    ]
  },
  {
    "objectID": "05-02-lmer-average-change-over-time.html#answering-the-research-question",
    "href": "05-02-lmer-average-change-over-time.html#answering-the-research-question",
    "title": "14  LMER: Average Change Over Time",
    "section": "14.8 Answering the Research Question",
    "text": "14.8 Answering the Research Question\nHere we present the rersults of our analysis in prose. We also presented a table of results from the fitted candidate models. For mixed-effects models, it is important to present coefficients and standard errors, estimated variance components, and any model-level summaries.\n\nINTERPRETATION\nWe set out to answer the following research question: What is the growth pattern in the average vocabulary score over time? We fitted four potential models corresponding to different growth patterns: no change over time, linear growth, quadratic growth, and log-linear growth. These models were evaluated using the AICc.\nThe evidence indicated that there was no empirical support for either the no change over time model. This indicates that students’ vocabulary scores are changing over time, on average. There was also no empirical support for the linear growth model indicating that the change over time is non-linear in nature. The log-linear growth model had the most empirical support, although there was also some support for the quadratic growth model. These models both suggest that students’ vocabulary scores, on average, are growing over time, but that growth diminishes over time.\n\nWe can also provide a table of the fitted model results. This table is similar to the table we create for fixed-effects regression models. However, we now need to include multiple variance components.\n\nCode\nhtmlreg(\n  l = list(lmer.0, lmer.3, lmer.4, lmer.5),\n  stars = numeric(0),    #No p-value stars\n  digits = 2,\n  padding = 20,          #Add space around columns (you may need to adjust this via trial-and-error)\n  custom.model.names = c(\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"),\n  custom.coef.names = c(\"Intercept\", \"Grade\", \"Grade$^2$\", \"ln(Grade)\"),\n  reorder.coef = c(2:4, 1), #Put intercept at bottom of table\n  include.loglik = FALSE,   #Omit log-likelihood\n  include.aic = FALSE, #Omit AIC\n  include.bic = FALSE, #Omit BIC\n  include.nobs = FALSE,  #Omit sample size\n  include.groups = FALSE, #Omit group size\n  custom.gof.names = c(\"$\\\\hat\\\\sigma^2_{0}$\", \"$\\\\hat\\\\sigma^2_{\\\\epsilon}$\"), # Rename variance component rows\n  custom.gof.rows = list(\n    AICc = c(AICc(lmer.0), AICc(lmer.3), AICc(lmer.4), AICc(lmer.5))  # Add AICc values\n    #R2 = (2722.5 - c(NA, 2662.09, 2605.00, 2540.35, 2536.67)) /  2722.5\n  ),\n  reorder.gof = c(2, 3, 1),\n  caption = NULL,\n  caption.above = TRUE, #Move caption above table\n  inner.rules = 1, #Include line rule before model-level output\n  outer.rules = 1 , #Include line rules around table\n  custom.note = \"The grade predictor was centered at 8th-grade for Models 2 and 3. In Model 4, it was centered at 7th grade to ensure all values were greater than 0.\"\n)\n\n\n\n\nTable 14.1: Four candidate models predicting longitudinal variation in students’ vocabulary scores.\n\n\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\nModel 4\n\n\n\n\n\n\nGrade\n\n\n \n\n\n0.75\n\n\n1.44\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.05)\n\n\n(0.18)\n\n\n \n\n\n\n\nGrade\\(^2\\)\n\n\n \n\n\n \n\n\n-0.23\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.06)\n\n\n \n\n\n\n\nln(Grade)\n\n\n \n\n\n \n\n\n \n\n\n1.67\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.11)\n\n\n\n\nIntercept\n\n\n2.53\n\n\n1.41\n\n\n1.18\n\n\n1.21\n\n\n\n\n \n\n\n(0.23)\n\n\n(0.24)\n\n\n(0.25)\n\n\n(0.25)\n\n\n\n\n\\(\\hat\\sigma^2_{0}\\)\n\n\n2.95\n\n\n3.18\n\n\n3.20\n\n\n3.20\n\n\n\n\n\\(\\hat\\sigma^2_{\\epsilon}\\)\n\n\n1.83\n\n\n0.90\n\n\n0.82\n\n\n0.82\n\n\n\n\nAICc\n\n\n1015.35\n\n\n880.86\n\n\n866.96\n\n\n864.30\n\n\n\n\n\n\nThe grade predictor was centered at 8th-grade for Models 2 and 3. In Model 4, it was centered at 7th grade to ensure all values were greater than 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWickham, H. (2014). Tidy data. The Journal of Statistical Software, 59(10). http://www.jstatsoft.org/v59/i10/",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>LMER: Average Change Over Time</span>"
    ]
  },
  {
    "objectID": "05-02-lmer-average-change-over-time.html#footnotes",
    "href": "05-02-lmer-average-change-over-time.html#footnotes",
    "title": "14  LMER: Average Change Over Time",
    "section": "",
    "text": "Unfortunately RM-ANOVA is still used, despite many known methodological limitations of the methodology, and the availability of better analytic options (e.g., LMER).↩︎\nThey are referred to as ‘left’ and ‘right’ because of where they appear in the syntax. If we wrote the syntax without the pipe it would be left_join(vocabulary_long, lookup_table, by = \"grade\"). In the syntax, vocabulary_long is to the left of lookup_table.↩︎",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>LMER: Average Change Over Time</span>"
    ]
  },
  {
    "objectID": "05-03-lmer-other-random-effects-and-covariates.html",
    "href": "05-03-lmer-other-random-effects-and-covariates.html",
    "title": "15  LMER: Other Random-Effects and Covariates",
    "section": "",
    "text": "15.1 Plots of the Individual Student Profiles\nIn this set of notes, you will continue to learn how to use the linear mixed-effects model to examine the mean change over time in a set of longitudinal/repeated measurements. You will also learn how to expand the model to allow cases to have different growth rates. Lastly, you will learn how to include covariates to evaluate whether the average growth rates vary for different groups. To do so, we will use data from the file minneapolis.csv (see data codebook).\nWe will use these data to explore the change in students’ reading scores over time (i.e., is there longitudinal variation in students’ reading scores). As in most longitudinal analyses, our primary goal is to examine the mean change (i.e., pattern of growth) of the outcome over time.\nThe data are already in the long/tidy format, so we do not need to re-structure them. We will start the analysis by examining the average and individual growth profiles.\nFocusing on the average growth profile, it appears that the students’ average reading score gets higher over time, and that this change is fairly linear. We will next fit the unconditional random intercepts model (to get a baseline measure of the unaccounted for variation) and the unconditional linear growth model. In the latter model we center grade on the initial measurement occasion. We display the results from these fitted models in a table.\nFrom Model A we find that most of the unaccounted for variation in reading scores is between-student variation (84%). There is also unaccounted for variation at the within-student level. Model B includes the time-varying predictor of grade to explain within-student variation. This model explains 51.4% of the within-student variation, and \\(-2.2\\)% of the between-student variation (mathematical artifact). Below we write the global fitted equation and Student 1 and 2’s fitted equation based on the results of Model B.\n\\[\n\\begin{split}\n\\mathbf{Global:~} \\hat{\\mathrm{Reading~Score}}_{ij} &= 206 + 4.36(\\mathrm{Grade}_{ij} - 5) \\\\[1ex]\n\\mathbf{Student~1:~} \\hat{\\mathrm{Reading~Score}}_{i1} &= 177 + 4.36(\\mathrm{Grade}_{i1} - 5) \\\\[1ex]\n\\mathbf{Student~2:~} \\hat{\\mathrm{Reading~Score}}_{i2} &= 201 + 4.36(\\mathrm{Grade}_{i2}-5 ) \\\\[1ex]\n\\end{split}\n\\]\nThe coefficient interpretations from the global fitted equation are:\nThe unconditional growth model we fitted included a random-effect of intercept. This term accounted for the non-independence of reading scores in the data. It also allowed the student-specific equations to differ from the global equation in the intercept. That is, it allowed students to have different reading scores at the initial measurement occasion. Substantively, this implies that the reason an individual student has a higher (or lower) reading score than average is because their reading score at the initial measurement occasion was higher (or lower) than the average student.\nAnother reason that students might have a higher (or lower) reading score than average is because their growth rate is different than the average growth rate. The model we fitted did not allow students to have different growth rates than the average student. To evaluate whether we should allow this kind of flexibility in the model, we will focus on a plot of the individual student profiles to see whether they appear to have different rates-of-growth than the average profile.\nAlthough we included the student profiles in the spaghetti plot we created earlier, in cases with a larger number of students it would be difficult to see the individual profiles in the spaghetti plot. To remedy this, you could facet on student so that each individual profile appears in a separate panel of the plot. (Note: If you have a lot of cases, rather than looking at all profiles, you could select a random sample of the cases and view those profiles.)\n# Get the mean reading score at each grade\n# Need to use this to add the average profile in each facet\nglobal = mpls |&gt;\n  group_by(grade) |&gt;\n  summarize(\n    reading_score = mean(reading_score)\n    ) |&gt;\n  ungroup()\n\n# Plot individual profiles in different panels\n# Add average profile to each panel\nggplot(data = mpls, aes(x = grade, y = reading_score)) +\n  geom_line(aes(group = student_id), alpha = 0.3) + #Individual profiles\n  geom_line(data = global, size = 2, group = 1, color = \"#FF2D21\") + #Mean profile\n  theme_light() +\n  xlab(\"Grade\") +\n  ylab(\"Reading Score\")  +\n  facet_wrap(~student_id)\n\n\n\n\n\n\n\nFigure 15.2: Plot showing the change in reading score over time for 22 students. The average growth profile is displayed as a thicker, red line.\nThe different slopes seen in the individual profiles indicate that students have different growth rates in their reading score over time, which also differ from the average growth rate. For example, Students 14 and 22 have higher growth rates than the average student. Student 16, on the other hand, has a smaller growth rate than the average student. This suggests that we also need to include a random-effect of grade (time) in our model.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>LMER: Other Random-Effects and Covariates</span>"
    ]
  },
  {
    "objectID": "05-03-lmer-other-random-effects-and-covariates.html#unconditional-growth-model-with-random-effects-of-intercept-and-grade-level",
    "href": "05-03-lmer-other-random-effects-and-covariates.html#unconditional-growth-model-with-random-effects-of-intercept-and-grade-level",
    "title": "15  LMER: Other Random-Effects and Covariates",
    "section": "15.2 Unconditional Growth Model with Random-Effects of Intercept and Grade-Level",
    "text": "15.2 Unconditional Growth Model with Random-Effects of Intercept and Grade-Level\nThe statistical model that includes random-effects for intercepts and grade-level can be expressed as:\n\\[\n\\mathrm{Reading~Score}_{ij} = \\big[\\beta_0 + b_{0j}\\big] + \\big[\\beta_1 + b_{1j}\\big](\\mathrm{Grade}_{ij}-5) + \\epsilon_{ij}\n\\]\nwhere,\n\n\\(\\mathrm{Reading~Score}_{ij}\\) is the reading score for Student j at time point i;\n\\(\\beta_0\\) is the fixed-effect of intercept;\n\\(b_{0j}\\) is the random-effect of intercept for Student j;\n\\(\\beta_1\\) is the fixed-effect of grade-level;\n\\(b_{1j}\\) is the random-effect of grade-level for Student j; and\n\\(\\epsilon_{ij}\\) is the error for Student j at time point i.\n\nWe fit the model and display the output below. We it the model using ML estimation.\n\n# Fit model\nlmer.c = lmer(reading_score ~ 1 + I(grade-5) + (1 + I(grade-5) | student_id), data = mpls, REML = FALSE)\n\n# Coefficient-level output\ntidy(lmer.c, effects = \"fixed\")\n\n# A tibble: 2 × 5\n  effect term         estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)    206.       4.23      48.7 \n2 fixed  I(grade - 5)     4.36     0.704      6.20\n\n\nUsing the fixed-effects estimates, the global fitted equation based on the fixed-effects model is:\n\\[\n\\hat{\\mathrm{Reading~Score}_{ij}} = 206 + 4.36(\\mathrm{Grade}_{ij}-5)\n\\]\nThe coefficient interpretations are:\n\nThe predicted average reading score for all students at the initial measurement occasion (in the 5th grade) is 206.\nEach subsequent grade-level is associated with a 4.36-point change in reading score, on average.\n\nThe fixed-effects did not change much as a result of allowing individual students to have different growth rates. We can also write the student specific fitted equations. Each student specific fitted equation is based on the fixed-effects AND the student-specific random-effects.\n\n# Obtain random effects\ntidy(lmer.c, effects = \"ran_vals\")\n\n# A tibble: 44 × 6\n   effect   group      level term        estimate std.error\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 ran_vals student_id 1     (Intercept)   -31.8       3.12\n 2 ran_vals student_id 2     (Intercept)    -3.89      3.12\n 3 ran_vals student_id 3     (Intercept)   -13.9       3.12\n 4 ran_vals student_id 4     (Intercept)   -10.5       3.12\n 5 ran_vals student_id 5     (Intercept)     1.32      3.12\n 6 ran_vals student_id 6     (Intercept)   -15.9       3.12\n 7 ran_vals student_id 7     (Intercept)    -5.00      3.12\n 8 ran_vals student_id 8     (Intercept)   -15.8       3.12\n 9 ran_vals student_id 9     (Intercept)   -55.2       3.12\n10 ran_vals student_id 10    (Intercept)    -2.87      3.12\n# ℹ 34 more rows\n\n# Obtain random effects for Student 1\ntidy(lmer.c, effects = \"ran_vals\") |&gt;\n  filter(level == 1)\n\n# A tibble: 2 × 6\n  effect   group      level term         estimate std.error\n  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 ran_vals student_id 1     (Intercept)    -31.8       3.12\n2 ran_vals student_id 1     I(grade - 5)     1.33      1.54\n\n\nFor example, Student 1’s fitted equation is:\n\\[\n\\begin{split}\n\\hat{\\mathrm{Reading~Score}_{i1}} &= \\bigg[206 - 31.8\\bigg] + \\bigg[4.36 + 1.33\\bigg](\\mathrm{Grade}_{i1}-5) \\\\[1ex]\n&= 174.2 + 5.69(\\mathrm{Grade}_{i1}-5) \\\\[1ex]\n\\end{split}\n\\]\n\nThe predicted reading score for Student 1 at the initial measurement occasion is 174.2 (31.8 points lower than the average student at that grade).\nFor Student 1, each subsequent grade is associated with a 5.69-point change in reading score, on average (a growth rate that is 1.33 points per grade higher than average).\n\nRecall the argument effects=\"ran_coefs\" in the tidy() function gives the student-specific effects directly.\n\n# Obtain student-specific coefficients for Student 2\ntidy(lmer.c, effects = \"ran_coefs\") |&gt;\n  filter(level == 2)\n\n# A tibble: 2 × 5\n  effect    group      level term         estimate\n  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;\n1 ran_coefs student_id 2     (Intercept)    202.  \n2 ran_coefs student_id 2     I(grade - 5)     3.42\n\n\nFrom here, we see that Student 2’s fitted equation is:\n\\[\n\\hat{\\mathrm{Reading~Score}_{i2}} = 202 + 3.42(\\mathrm{Grade}_{i2}-5)\n\\]\nFrom the two students’ fitted equations we see that by including a random-effect of both intercept and grade (time):\n\nStudents can have a different reading score than average at the initial measurement occasion.\nStudents can have a different growth rate than the average growth rate.\n\n\n\n15.2.1 Variance Estimates\nWe can also examine the variance components for the fitted model:\n\n# Obtain variance estimates\ntidy(lmer.c, effects = \"ran_pars\")\n\n# A tibble: 4 × 4\n  effect   group      term                          estimate\n  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;                            &lt;dbl&gt;\n1 ran_pars student_id sd__(Intercept)                 19.5  \n2 ran_pars student_id cor__(Intercept).I(grade - 5)   -0.360\n3 ran_pars student_id sd__I(grade - 5)                 2.72 \n4 ran_pars Residual   sd__Observation                  4.19 \n\n\nWe now have four total variance components:\n\nBetween-Student Variance (Intercept): \\(\\mathrm{Var}(b_0) = 19.5^2 = 380.25\\)\nBetween-Student Variance (Rate-of-Change): \\(\\mathrm{Var}(b_1) = 2.72^2 = 7.40\\)\nBetween-Student Correlation (between Intercepts and Rate-of-Change): \\(\\mathrm{Corr}(b_0,b_1) = -0.36\\)\nWithin-Student Variance: \\(\\mathrm{Var}(e) = 4.19^2 = 17.56\\)\n\n\nFYI\nBetween-student variance components are Level-2 variance components and the within-student variance component is the Level-1 variance component.\n\nAdding a random-effect resulted in two additional variance components. We now have split the between-student variance into two parts: (1) variation in students’ intercepts, and (2) variation in students growth rates. That is we are saying that variation in reading scores is due to student differences in reading scores at the initial measurement occasion, differences in students’ growth rates, and within-student variation (random error).\nThe correlation indicates the relationship between the student-specific random effects of intercept and rate-of-change. The fact that this is negative indicates that students who have a lower random-effect of intercept tend to have a higher random-effect of grade In other words, students who have a lower reading score at the initial measurement occasion tend to have a higher growth rate.\nBelow, we add Model C to our table of regression results.\n\n\nCode\nhtmlreg(\n  l = list(lmer.a, lmer.b, lmer.c),\n  stars = numeric(0),    #No p-value stars\n  digits = 2,\n  padding = 20, #Add space around columns (you may need to adjust this via trial-and-error)\n  custom.model.names = c(\"Model A\", \"Model B\", \"Model C\"),\n  custom.coef.names = c(\"Intercept\", \"Grade\"),\n  reorder.coef = c(2, 1), #Put intercept at bottom of table\n  include.loglik = FALSE, #Omit log-likelihood\n  include.aic = FALSE,    #Omit AIC\n  include.bic = FALSE,    #Omit BIC\n  include.nobs = FALSE,   #Omit sample size\n  include.groups = FALSE, #Omit group size\n  include.deviance = TRUE,\n  custom.gof.names = c(\"Deviance\", \"Level-2 Variance (Intercept)\", \"Level-1 Variance\", \"Level-2 Variance (Slope)\", \"Level-2 Covariance\"), # Rename variance component rows\n  custom.gof.rows = list(\n    AICc = c(AICc(lmer.a), AICc(lmer.b), AICc(lmer.c))  # Add AICc values\n  ),\n  reorder.gof = c(3, 5, 6, 4, 2, 1),\n  caption = NULL,\n  caption.above = TRUE, #Move caption above table\n  inner.rules = 1, #Include line rule before model-level output\n  outer.rules = 1 , #Include line rules around table\n  custom.note = \"*Note.* The grade predictor was centered at the initial measurement occasion of 5th-grade.\"\n)\n\n\n\n\n\n \n\n\nModel A\n\n\nModel B\n\n\nModel C\n\n\n\n\n\n\nGrade\n\n\n \n\n\n4.36\n\n\n4.36\n\n\n\n\n \n\n\n \n\n\n(0.52)\n\n\n(0.70)\n\n\n\n\nIntercept\n\n\n212.64\n\n\n206.09\n\n\n206.09\n\n\n\n\n \n\n\n(3.96)\n\n\n(4.04)\n\n\n(4.23)\n\n\n\n\nLevel-2 Variance (Intercept)\n\n\n329.66\n\n\n337.57\n\n\n381.44\n\n\n\n\nLevel-2 Variance (Slope)\n\n\n \n\n\n \n\n\n7.38\n\n\n\n\nLevel-2 Covariance\n\n\n \n\n\n \n\n\n-19.12\n\n\n\n\nLevel-1 Variance\n\n\n61.62\n\n\n29.89\n\n\n17.59\n\n\n\n\nDeviance\n\n\n680.78\n\n\n633.02\n\n\n622.52\n\n\n\n\nAICc\n\n\n687.07\n\n\n641.50\n\n\n635.56\n\n\n\n\n\n\nNote. The grade predictor was centered at the initial measurement occasion of 5th-grade.\n\n\n\n\n\nNotice that the htmlreg() function produces the covariance between the intercept and slope random effects rather than the correlation that is produced from tidy(). There is a direct relationship between the correlation and covariance:\n\\[\n\\mathrm{Cov}(b_0,b_1) = \\mathrm{Corr}(b_0,b_1) \\times \\sqrt{\\mathrm{Var}(b_0)} \\times \\sqrt{\\mathrm{Var}(b_1)}\n\\]\nIn our example,\n\\[\n\\begin{split}\n\\mathrm{Cov}(b_0,b_1) &= -0.360 \\times \\sqrt{380.25} \\times \\sqrt{7.3984} \\\\[1ex]\n&= -19.09\n\\end{split}\n\\] Sometimes the variances and covariances of the random-effects are reported in a matrix referred to as the variance-covariance matrix of random-effects and denoted as G.\n\\[\n\\mathbf{G} = \\begin{bmatrix}\\mathrm{Var}(b_0) & \\mathrm{Cov}(b_0,b_1)\\\\ \\mathrm{Cov}(b_0,b_1) & \\mathrm{Var}(b_1)\\end{bmatrix}\n\\]\nIn our example,\n\\[\n\\mathbf{G} = \\begin{bmatrix}380.25 & -19.09\\\\ -19.09 & 7.40\\end{bmatrix}\n\\]\n\n\n\n15.2.2 Explained Variation and Pseudo-\\(R^2\\) Values\nIt is difficult to compare the variance components for Model C to those from the unconditional random intercepts model to get pseudo-\\(R^2\\) values, like we did for Model B. This is because Model C includes an additional source of unexplained variation, namely \\(Var(b_1)\\) that the unconditional random intercepts model did not include.\nAs we add other fixed-effects predictors to the model, the variance components from those models could be compared to the variance components from Model C (so long as these models continue to include both the random intercepts and slopes). In other words, Model C becomes the new baseline model for other models that also include random intercepts and slopes.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>LMER: Other Random-Effects and Covariates</span>"
    ]
  },
  {
    "objectID": "05-03-lmer-other-random-effects-and-covariates.html#group-differences-in-the-average-growth-profiles",
    "href": "05-03-lmer-other-random-effects-and-covariates.html#group-differences-in-the-average-growth-profiles",
    "title": "15  LMER: Other Random-Effects and Covariates",
    "section": "15.3 Group Differences in the Average Growth Profiles",
    "text": "15.3 Group Differences in the Average Growth Profiles\nOne question we might have is whether the average growth profile varies across different groups. For example, we might wonder if (and how) the average growth profile is different for special education and non-special education students. To explore this, we will again create a spaghetti plot, but this time we will facet on special education status.\n\nggplot(data = mpls, aes(x = grade, y = reading_score)) +\n  geom_line(aes(group = student_id), alpha = 0.3) +  #Add individual profiles\n  stat_summary(fun = mean, geom = \"line\", size = 2, group = 1, color = \"#FF2D21\") + #Add mean profile line\n  theme_light() +\n  xlab(\"Grade\") +\n  ylab(\"Reading Score\") +\n  facet_wrap(~special_ed)\n\n\n\n\nPlot showing the change in reading scores over time conditioned on special education status. The average growth profile is displayed as a thicker line.\n\n\n\n\nFrom this plot we see that the average profiles are different depending on special education status. To evaluate whether these differences are due to more than just sampling variation, we will fit a mixed-effects model that includes special education status as a fixed-effect. The statistical model that includes random-effects for intercepts and grade-level can be expressed as:\n\\[\n\\mathrm{Reading~Score}_{ij} = \\big[\\beta_0 + b_{0j}\\big] + \\big[\\beta_1 + b_{1j}\\big](\\mathrm{Grade}_{ij}-5) + \\beta_2(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\n\\]\nwhere,\n\n\\(\\mathrm{Reading~Score}_{ij}\\) is the reading score for Student j at time point i;\n\\(\\beta_0\\) is the fixed-effect of intercept;\n\\(b_{0j}\\) is the random-effect of intercept for Student j;\n\\(\\beta_1\\) is the fixed-effect of grade-level;\n\\(b_{1j}\\) is the random-effect of grade-level for Student j;\n\\(\\beta_2\\) is the fixed-effect of special education status; and\n\\(\\epsilon_{ij}\\) is the error for Student j at time point i.\n\nNotice that th special education predictor only includes a j subscript. That is because it only varies between students. In our data, a single student has the same special education status at every time point. Therefore we do not put an i subscript on this predictor. We fit the model and display the output below. We again fit the model using ML estimation. We obtain both the fixed-effect estimates and estimated variance components for the fitted model and add these results to our table of regression results.\n\n# Create dummy-coded special education status\nmpls = mpls |&gt;\n  mutate(\n    sped = if_else(special_ed == \"Yes\", 1, 0)\n    )\n\n# Fit model\nlmer.d = lmer(reading_score ~ 1 + I(grade-5) + sped + (1 + I(grade-5) | student_id), data = mpls, REML = FALSE)\n\n# Coefficient-level output\ntidy(lmer.d, effects = \"fixed\")\n\n# A tibble: 3 × 5\n  effect term         estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)    208.       4.26      48.8 \n2 fixed  I(grade - 5)     4.36     0.704      6.20\n3 fixed  sped           -15.8     11.0       -1.44\n\n# Obtain variance estimates\ntidy(lmer.d, effects = \"ran_pars\")\n\n# A tibble: 4 × 4\n  effect   group      term                          estimate\n  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;                            &lt;dbl&gt;\n1 ran_pars student_id sd__(Intercept)                 18.4  \n2 ran_pars student_id cor__(Intercept).I(grade - 5)   -0.312\n3 ran_pars student_id sd__I(grade - 5)                 2.72 \n4 ran_pars Residual   sd__Observation                  4.19 \n\n\n\n\nCode\nhtmlreg(\n  l = list(lmer.a, lmer.b, lmer.c, lmer.d),\n  stars = numeric(0),    #No p-value stars\n  digits = 2,\n  padding = 20, #Add space around columns (you may need to adjust this via trial-and-error)\n  custom.model.names = c(\"Model A\", \"Model B\", \"Model C\", \"Model D\"),\n  custom.coef.names = c(\"Intercept\", \"Grade\", \"Special Education Status\"),\n  reorder.coef = c(2, 3, 1), #Put intercept at bottom of table\n  include.loglik = FALSE, #Omit log-likelihood\n  include.aic = FALSE,    #Omit AIC\n  include.bic = FALSE,    #Omit BIC\n  include.nobs = FALSE,   #Omit sample size\n  include.groups = FALSE, #Omit group size\n  include.deviance = TRUE,\n  custom.gof.names = c(\"Deviance\", \"Level-2 Variance (Intercept)\", \"Level-1 Variance\", \"Level-2 Variance (Slope)\", \"Level-2 Covariance\"), # Rename variance component rows\n  custom.gof.rows = list(\n    AICc = c(AICc(lmer.a), AICc(lmer.b), AICc(lmer.c), AICc(lmer.d))  # Add AICc values\n  ),\n  reorder.gof = c(3, 5, 6, 4, 2, 1),\n  caption = NULL,\n  caption.above = TRUE, #Move caption above table\n  inner.rules = 1, #Include line rule before model-level output\n  outer.rules = 1 , #Include line rules around table\n  custom.note = \"*Note.* The grade predictor was centered at the initial measurement occasion of 5th-grade. Special education status was dummy-coded using non-special education students as the reference group.\"\n)\n\n\n\n\n\n \n\n\nModel A\n\n\nModel B\n\n\nModel C\n\n\nModel D\n\n\n\n\n\n\nGrade\n\n\n \n\n\n4.36\n\n\n4.36\n\n\n4.36\n\n\n\n\n \n\n\n \n\n\n(0.52)\n\n\n(0.70)\n\n\n(0.70)\n\n\n\n\nSpecial Education Status\n\n\n \n\n\n \n\n\n \n\n\n-15.77\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(10.96)\n\n\n\n\nIntercept\n\n\n212.64\n\n\n206.09\n\n\n206.09\n\n\n208.24\n\n\n\n\n \n\n\n(3.96)\n\n\n(4.04)\n\n\n(4.23)\n\n\n(4.26)\n\n\n\n\nLevel-2 Variance (Intercept)\n\n\n329.66\n\n\n337.57\n\n\n381.44\n\n\n338.62\n\n\n\n\nLevel-2 Variance (Slope)\n\n\n \n\n\n \n\n\n7.38\n\n\n7.38\n\n\n\n\nLevel-2 Covariance\n\n\n \n\n\n \n\n\n-19.12\n\n\n-15.60\n\n\n\n\nLevel-1 Variance\n\n\n61.62\n\n\n29.89\n\n\n17.59\n\n\n17.59\n\n\n\n\nDeviance\n\n\n680.78\n\n\n633.02\n\n\n622.52\n\n\n620.61\n\n\n\n\nAICc\n\n\n687.07\n\n\n641.50\n\n\n635.56\n\n\n636.01\n\n\n\n\n\n\nNote. The grade predictor was centered at the initial measurement occasion of 5th-grade. Special education status was dummy-coded using non-special education students as the reference group.\n\n\n\n\n\nUsing the fixed-effects estimates, the global fitted equation based on the fixed-effects model is:\n\\[\n\\hat{\\mathrm{Reading~Score}_{ij}} = 208 + 4.36(\\mathrm{Grade}_{ij}-5) - 15.8(\\mathrm{Special~Education}_j)\n\\]\nThe coefficient interpretations are:\n\nThe predicted average reading score for non-special education students at the initial measurement occasion (in the 5th grade) is 208.\nEach subsequent grade-level is associated with a 4.36-point change in reading score, on average, controlling for special education status.\nSpecial education students have a reading score that is 15.8 points lower than non-special education students, on average, at every grade level.\n\nWe again have four total variance components:\n\nBetween-Student Variance (Intercept): \\(\\mathrm{Var}(b_0) = 18.4^2 = 338.56\\)\nBetween-Student Variance (Rate-of-Change): \\(\\mathrm{Var}(b_1) = 2.72^2 = 7.40\\)\nBetween-Student Correlation (between Intercepts and Rate-of-Change): \\(\\mathrm{Corr}(b_0,b_1) = -0.312\\)\nWithin-Student Variance: \\(\\mathrm{Var}(e) = 4.19^2 = 17.56\\)\n\nSince this model includes the same set of random effects as Model C (unconditional growth model), we can use Model C as a baseline to consider the variation explained by special education status. Here we compute pseudo-\\(R^2\\) values based on using Model C as the comparison model.\n\nBetween-Student Variance (Intercept): \\((380.25 - 338.56)/380.25 = 0.110\\)\nBetween-Student Variance (Rate-of-Change): \\((7.40 - 7.40)/7.40 = 0\\)\nWithin-Student Variance: \\((17.56 - 17.56)/17.56=0\\)\n\nIncluding special education status explained 11% of the between-student variation in intercepts. This means it is helping to explain why students reading scores are different at the initial measurement occasion. It does not explain variation in students’ growth rates, nor does it explain within-student variation.\n\n\n15.3.1 Student-Specific Fitted Equations\nWe again need to obtain the estimated random-effects for students in order to write their fitted equations. Moreover, we will also need their special education status. For example, we obtain Student 1’s random-effects using:\n\n# Obtain student-specific coefficients for Student 2\ntidy(lmer.d, effects = \"ran_vals\") |&gt;\n  filter(level == 1)\n\n# A tibble: 2 × 6\n  effect   group      level term         estimate std.error\n  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 ran_vals student_id 1     (Intercept)    -33.7       3.12\n2 ran_vals student_id 1     I(grade - 5)     1.26      1.54\n\n\nWe also note that Student 1 is a non-special education student. Writing this student’s fitted equation:\n\\[\n\\begin{split}\n\\hat{\\mathrm{Reading~Score}_{i1}} &= \\bigg[208 -33.7\\bigg] + \\bigg[4.36 + 1.26\\bigg](\\mathrm{Grade}_{i1}-5) - 15.77(0) \\\\[1ex]\n&= 174.3 + 5.62(\\mathrm{Grade}_{i1}-5)\n\\end{split}\n\\]\nStudent 8, on the other hand is a special education student. That student’s random-effects are \\(\\hat{b}_0=-2.14\\) and \\(\\hat{b}_1=-2.51\\). Their fitted equation is:\n\\[\n\\begin{split}\n\\hat{\\mathrm{Reading~Score}_{i1}} &= \\bigg[208 - 2.14\\bigg] + \\bigg[4.36 - 2.51\\bigg](\\mathrm{Grade}_{i1}-5) - 15.77(1) \\\\[1ex]\n&= 190.09 + 1.85(\\mathrm{Grade}_{i1}-5)\n\\end{split}\n\\]\nWe could also plot the average profiles and also any student profiles that we wanted to. Here I will plot the two average profiles (one for each special education status), along with the profile for Student 8. Since Student 8 is a special education student, I will use the same color for that student’s profile and the average profile for special education students.\n\nggplot(data = mpls, aes(x = grade-5, y = reading_score)) +\n  geom_point(alpha = 0) +  #Add individual profiles\n  geom_abline(intercept = 208, slope = 4.36, color = \"#D55E00\") + #Non-Sped\n  geom_abline(intercept = 192.2, slope = 4.36, color = \"#0072B2\") + #Sped\n  geom_abline(intercept = 190.09, slope = 1.85, color = \"#0072B2\", linetype = \"dashed\") + #Student 8 (Sped)\n  theme_light() +\n  scale_x_continuous(\n    name = \"Grade\",\n    breaks = c(0, 1, 2, 3),\n    labels = c(\"5th\", \"6th\", \"7th\", \"8th\")\n    ) +\n  ylab(\"Reading Score\")\n\n\n\n\n\n\n\nFigure 15.3: Plot showing the average growth profile of reading scores over time for special education (blue) and non-special education (vermillion) students based on Model D. The individual growth profile for Student 8 (dotted line) is also displayed.\n\n\n\n\n\n\nPROTIP\nSince the model was based on grade level being centered at the initial measurement occasion, we need to use the centered grade in the x= argument of the ggplot() aesthetic mapping. Then we can re-label the x-scale to our original grade levels for easier interpretation for readers.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>LMER: Other Random-Effects and Covariates</span>"
    ]
  },
  {
    "objectID": "05-03-lmer-other-random-effects-and-covariates.html#interaction-between-grade-and-special-education-status",
    "href": "05-03-lmer-other-random-effects-and-covariates.html#interaction-between-grade-and-special-education-status",
    "title": "15  LMER: Other Random-Effects and Covariates",
    "section": "15.4 Interaction Between Grade and Special Education Status",
    "text": "15.4 Interaction Between Grade and Special Education Status\nModel D included grade-level and special education status as fixed-effects. This produces average growth profiles that have the same rate-of-growth for special education students and non-special education students. Substantively, the reason those students have different average reading scores is that they have different averages at the initial measurement occasion.\nThe model presumes that special education and non-special education students have, on average, the same growth rate over the duration of the study (5th through 8th grade).1 This is probably not a realistic assumption. We can allow different rates-of-growth by including an interaction between grade-level and special education status in addition to the main effects. The statistical model can be expressed as:\n\\[\n\\mathrm{Reading~Score}_{ij} = \\big[\\beta_0 + b_{0j}\\big] + \\big[\\beta_1 + b_{1j}\\big](\\mathrm{Grade}_{ij}-5) + \\beta_2(\\mathrm{Special~Education}_j) + \\beta_3(\\mathrm{Grade}_{ij}-5)(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\n\\]\nwhere,\n\n\\(\\mathrm{Reading~Score}_{ij}\\) is the reading score for Student j at time point i;\n\\(\\beta_0\\) is the fixed-effect of intercept;\n\\(b_{0j}\\) is the random-effect of intercept for Student j;\n\\(\\beta_1\\) is the fixed-effect of grade-level;\n\\(b_{1j}\\) is the random-effect of grade-level for Student j;\n\\(\\beta_2\\) is the fixed-effect of special education status;\n\\(\\beta_3\\) is the interaction-effect (fixed-effect) between special education status grade-level; and\n\\(\\epsilon_{ij}\\) is the error for Student j at time point i.\n\nWe fit the model using ML estimation, obtain both the fixed-effect estimates and estimated variance components, and add these results to our table of regression results.\n\n# Fit model\nlmer.e = lmer(reading_score ~ 1 + I(grade-5) + sped + I(grade-5):sped +\n                (1 + I(grade-5) | student_id), data = mpls, REML = FALSE)\n\n# Coefficient-level output\ntidy(lmer.e, effects = \"fixed\")\n\n# A tibble: 4 × 5\n  effect term              estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)         209.       4.29     48.7  \n2 fixed  I(grade - 5)          4.11     0.742     5.53 \n3 fixed  sped                -19.4     11.6      -1.67 \n4 fixed  I(grade - 5):sped     1.89     2.01      0.942\n\n# Obtain variance estimates\ntidy(lmer.e, effects = \"ran_pars\")\n\n# A tibble: 4 × 4\n  effect   group      term                          estimate\n  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;                            &lt;dbl&gt;\n1 ran_pars student_id sd__(Intercept)                 18.4  \n2 ran_pars student_id cor__(Intercept).I(grade - 5)   -0.305\n3 ran_pars student_id sd__I(grade - 5)                 2.64 \n4 ran_pars Residual   sd__Observation                  4.19 \n\n\n\n\nCode\nhtmlreg(\n  l = list(lmer.a, lmer.b, lmer.c, lmer.d, lmer.e),\n  stars = numeric(0),    #No p-value stars\n  digits = 2,\n  padding = 20, #Add space around columns (you may need to adjust this via trial-and-error)\n  custom.model.names = c(\"Model A\", \"Model B\", \"Model C\", \"Model D\", \"Model E\"),\n  custom.coef.names = c(\"Intercept\", \"Grade\", \"Special Education Status\", \"Grade x Special Education Status\"),\n  reorder.coef = c(2:4, 1), #Put intercept at bottom of table\n  include.loglik = FALSE, #Omit log-likelihood\n  include.aic = FALSE,    #Omit AIC\n  include.bic = FALSE,    #Omit BIC\n  include.nobs = FALSE,   #Omit sample size\n  include.groups = FALSE, #Omit group size\n  include.deviance = TRUE,\n  custom.gof.names = c(\"Deviance\", \"Level-2 Variance (Intercept)\", \"Level-1 Variance\", \"Level-2 Variance (Slope)\", \"Level-2 Covariance\"), # Rename variance component rows\n  custom.gof.rows = list(\n    AICc = c(AICc(lmer.a), AICc(lmer.b), AICc(lmer.c), AICc(lmer.d), AICc(lmer.e))  # Add AICc values\n  ),\n  reorder.gof = c(3, 5, 6, 4, 2, 1),\n  caption = NULL,\n  caption.above = TRUE, #Move caption above table\n  inner.rules = 1, #Include line rule before model-level output\n  outer.rules = 1 , #Include line rules around table\n  custom.note = \"*Note.* The grade predictor was centered at the initial measurement occasion of 5th-grade. Special education status was dummy-coded using non-special education students as the reference group.\"\n)\n\n\n\n\n\n \n\n\nModel A\n\n\nModel B\n\n\nModel C\n\n\nModel D\n\n\nModel E\n\n\n\n\n\n\nGrade\n\n\n \n\n\n4.36\n\n\n4.36\n\n\n4.36\n\n\n4.11\n\n\n\n\n \n\n\n \n\n\n(0.52)\n\n\n(0.70)\n\n\n(0.70)\n\n\n(0.74)\n\n\n\n\nSpecial Education Status\n\n\n \n\n\n \n\n\n \n\n\n-15.77\n\n\n-19.40\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(10.96)\n\n\n(11.61)\n\n\n\n\nGrade x Special Education Status\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n1.89\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(2.01)\n\n\n\n\nIntercept\n\n\n212.64\n\n\n206.09\n\n\n206.09\n\n\n208.24\n\n\n208.74\n\n\n\n\n \n\n\n(3.96)\n\n\n(4.04)\n\n\n(4.23)\n\n\n(4.26)\n\n\n(4.29)\n\n\n\n\nLevel-2 Variance (Intercept)\n\n\n329.66\n\n\n337.57\n\n\n381.44\n\n\n338.62\n\n\n337.07\n\n\n\n\nLevel-2 Variance (Slope)\n\n\n \n\n\n \n\n\n7.38\n\n\n7.38\n\n\n6.96\n\n\n\n\nLevel-2 Covariance\n\n\n \n\n\n \n\n\n-19.12\n\n\n-15.60\n\n\n-14.79\n\n\n\n\nLevel-1 Variance\n\n\n61.62\n\n\n29.89\n\n\n17.59\n\n\n17.59\n\n\n17.59\n\n\n\n\nDeviance\n\n\n680.78\n\n\n633.02\n\n\n622.52\n\n\n620.61\n\n\n619.74\n\n\n\n\nAICc\n\n\n687.07\n\n\n641.50\n\n\n635.56\n\n\n636.01\n\n\n637.56\n\n\n\n\n\n\nNote. The grade predictor was centered at the initial measurement occasion of 5th-grade. Special education status was dummy-coded using non-special education students as the reference group.\n\n\n\n\n\nThe fitted equation for the average model is:\n\\[\n\\mathrm{Reading~Score}_{ij} = 209 + 4.11(\\mathrm{Grade}_{ij}-5) - 19.4(\\mathrm{Special~Education}_j) + 1.89(\\mathrm{Grade}_{ij}-5)(\\mathrm{Special~Education}_j)\n\\]\nThe interaction effect is interpreted like any other interaction effect, either\n\nThe effect of grade-level on reading scores varies by special education status. OR\nThe effect of special education status on reading scores varies by grade-level.\n\nTo better understand this interaction, we can write out and then plot the average fitted equation for special education students and that for non-special education students.\n\\[\n\\begin{split}\n\\mathbf{Non\\mbox{-}SpEd:~}\\mathrm{Reading~Score}_{ij} &= 209 + 4.11(\\mathrm{Grade}_{ij}-5) - 19.4(0) + 1.89(\\mathrm{Grade}_{ij}-5)(0) \\\\[1ex]\n&= 209 + 4.11(\\mathrm{Grade}_{ij}-5) \\\\[4ex]\n\\mathbf{SpEd:~}\\mathrm{Reading~Score}_{ij} &= 209 + 4.11(\\mathrm{Grade}_{ij}-5) - 19.4(1) + 1.89(\\mathrm{Grade}_{ij}-5)(1) \\\\[1ex]\n&= 189.6 + 6.00(\\mathrm{Grade}_{ij}-5)\n\\end{split}\n\\]\n\nggplot(data = mpls, aes(x = grade-5, y = reading_score)) +\n  geom_point(alpha = 0) +  #Add individual profiles\n  geom_abline(intercept = 209, slope = 4.11, color = \"#D55E00\") + #Non-Sped\n  geom_abline(intercept = 189.6, slope = 6.00, color = \"#0072B2\") + #Sped\n  theme_light() +\n  scale_x_continuous(\n    name = \"Grade\",\n    breaks = c(0, 1, 2, 3),\n    labels = c(\"5th\", \"6th\", \"7th\", \"8th\")\n    ) +\n  ylab(\"Reading Score\")\n\n\n\n\n\n\n\nFigure 15.4: Plot showing the average growth profile of reading scores over time for special education (blue) and non-special education (vermillion) students based on Model E.\n\n\n\n\n\n\nINTERPRETATION\nNon-special education students have a higher average reading score in the 5th grade than special education students. Moreover, they continue to have a higher average reading score than their special education peers throughout the duration of the study. However, the gap in reading scores gets smaller over time since the growth rate for special education students is higher than that for non-special education students.\n\n\n\n15.4.1 Controlling for Attendance\nWe will fit one additional model that adds to Model E by controlling for attendance. The statistical model can be expressed as:\n\\[\n\\begin{split}\n\\mathrm{Reading~Score}_{ij} = &\\big[\\beta_0 + b_{0j}\\big] + \\big[\\beta_1 + b_{1j}\\big](\\mathrm{Grade}_{ij}-5) + \\beta_2(\\mathrm{Special~Education}_j) + \\beta_3(\\mathrm{Attendance}_j) + \\\\\n&\\beta_4(\\mathrm{Grade}_{ij}-5)(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\n\\end{split}\n\\]\nwhere,\n\n\\(\\mathrm{Reading~Score}_{ij}\\) is the reading score for Student j at time point i;\n\\(\\beta_0\\) is the fixed-effect of intercept;\n\\(b_{0j}\\) is the random-effect of intercept for Student j;\n\\(\\beta_1\\) is the fixed-effect of grade-level;\n\\(\\mathrm{Grade}_{ij}\\) is the grade-level for Student j at time point i;\n\\(b_{1j}\\) is the random-effect of grade-level for Student j;\n\\(\\beta_2\\) is the fixed-effect of special education status;\n\\(\\mathrm{Special~Education}_{j}\\) is the dummy-coded special education status for Student j\n\\(\\beta_3\\) is the fixed-effect of attendance;\n\\(\\mathrm{Attendance}_{j}\\) is the attendance for Student j\n\\(\\beta_4\\) is the interaction-effect (fixed-effect) between special education status grade-level; and\n\\(\\epsilon_{ij}\\) is the error for Student j at time point i.\n\nWe fit the model using ML estimation, obtain both the fixed-effect estimates and estimated variance components, and add these results to our table of regression results. To facilitate better interpretation of the intercept, we will mean center the attendance variable.\n\n# Fit model\nlmer.f = lmer(reading_score ~ 1 + I(grade-5) + sped + I(attendance - 0.9564) + I(grade-5):sped +\n                (1 + I(grade-5) | student_id), data = mpls, REML = FALSE)\n\n# Coefficient-level output\ntidy(lmer.f, effects = \"fixed\")\n\n# A tibble: 5 × 5\n  effect term                   estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)              209.       4.18     50.0  \n2 fixed  I(grade - 5)               4.11     0.742     5.53 \n3 fixed  sped                     -22.0     11.4      -1.93 \n4 fixed  I(attendance - 0.9564)   218.      89.8       2.43 \n5 fixed  I(grade - 5):sped          1.89     2.01      0.942\n\n# Obtain variance estimates\ntidy(lmer.f, effects = \"ran_pars\")\n\n# A tibble: 4 × 4\n  effect   group      term                          estimate\n  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;                            &lt;dbl&gt;\n1 ran_pars student_id sd__(Intercept)                 17.9  \n2 ran_pars student_id cor__(Intercept).I(grade - 5)   -0.506\n3 ran_pars student_id sd__I(grade - 5)                 2.64 \n4 ran_pars Residual   sd__Observation                  4.19 \n\n\n\n\nCode\nhtmlreg(\n  l = list(lmer.a, lmer.b, lmer.c, lmer.d, lmer.e, lmer.f),\n  stars = numeric(0),    #No p-value stars\n  digits = 2,\n  padding = 20, #Add space around columns (you may need to adjust this via trial-and-error)\n  custom.model.names = c(\"Model A\", \"Model B\", \"Model C\", \"Model D\", \"Model E\", \"Model F\"),\n  custom.coef.names = c(\"Intercept\", \"Grade\", \"Special Education Status\", \"Attendance\",\n                        \"Grade x Special Education Status\"),\n  reorder.coef = c(2:5, 1), #Put intercept at bottom of table\n  include.loglik = FALSE, #Omit log-likelihood\n  include.aic = FALSE,    #Omit AIC\n  include.bic = FALSE,    #Omit BIC\n  include.nobs = FALSE,   #Omit sample size\n  include.groups = FALSE, #Omit group size\n  include.deviance = TRUE,\n  custom.gof.names = c(\"Deviance\", \"Level-2 Variance (Intercept)\", \"Level-1 Variance\", \"Level-2 Variance (Slope)\", \"Level-2 Covariance\"), # Rename variance component rows\n  custom.gof.rows = list(\n    AICc = c(AICc(lmer.a), AICc(lmer.b), AICc(lmer.c), AICc(lmer.d), AICc(lmer.e), AICc(lmer.f))  # Add AICc values\n  ),\n  reorder.gof = c(3, 5, 6, 4, 2, 1),\n  caption = NULL,\n  caption.above = TRUE, #Move caption above table\n  inner.rules = 1, #Include line rule before model-level output\n  outer.rules = 1 , #Include line rules around table\n  custom.note = \"*Note.* The grade predictor was centered at the initial measurement occasion of 5th-grade. Special education status was dummy-coded using non-special education students as the reference group. The attendance variable was mean centered.\"\n)\n\n\n\n\n\n \n\n\nModel A\n\n\nModel B\n\n\nModel C\n\n\nModel D\n\n\nModel E\n\n\nModel F\n\n\n\n\n\n\nGrade\n\n\n \n\n\n4.36\n\n\n4.36\n\n\n4.36\n\n\n4.11\n\n\n4.11\n\n\n\n\n \n\n\n \n\n\n(0.52)\n\n\n(0.70)\n\n\n(0.70)\n\n\n(0.74)\n\n\n(0.74)\n\n\n\n\nSpecial Education Status\n\n\n \n\n\n \n\n\n \n\n\n-15.77\n\n\n-19.40\n\n\n-22.01\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(10.96)\n\n\n(11.61)\n\n\n(11.38)\n\n\n\n\nAttendance\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n1.89\n\n\n1.89\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(2.01)\n\n\n(2.01)\n\n\n\n\nGrade x Special Education Status\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n218.39\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(89.83)\n\n\n\n\nIntercept\n\n\n212.64\n\n\n206.09\n\n\n206.09\n\n\n208.24\n\n\n208.74\n\n\n209.10\n\n\n\n\n \n\n\n(3.96)\n\n\n(4.04)\n\n\n(4.23)\n\n\n(4.26)\n\n\n(4.29)\n\n\n(4.18)\n\n\n\n\nLevel-2 Variance (Intercept)\n\n\n329.66\n\n\n337.57\n\n\n381.44\n\n\n338.62\n\n\n337.07\n\n\n320.00\n\n\n\n\nLevel-2 Variance (Slope)\n\n\n \n\n\n \n\n\n7.38\n\n\n7.38\n\n\n6.96\n\n\n6.96\n\n\n\n\nLevel-2 Covariance\n\n\n \n\n\n \n\n\n-19.12\n\n\n-15.60\n\n\n-14.79\n\n\n-23.85\n\n\n\n\nLevel-1 Variance\n\n\n61.62\n\n\n29.89\n\n\n17.59\n\n\n17.59\n\n\n17.59\n\n\n17.59\n\n\n\n\nDeviance\n\n\n680.78\n\n\n633.02\n\n\n622.52\n\n\n620.61\n\n\n619.74\n\n\n615.05\n\n\n\n\nAICc\n\n\n687.07\n\n\n641.50\n\n\n635.56\n\n\n636.01\n\n\n637.56\n\n\n635.36\n\n\n\n\n\n\nNote. The grade predictor was centered at the initial measurement occasion of 5th-grade. Special education status was dummy-coded using non-special education students as the reference group. The attendance variable was mean centered.\n\n\n\n\n\nThe fitted equation for the average model is:\n\\[\n\\begin{split}\n\\mathrm{Reading~Score}_{ij} = &209 + 4.11(\\mathrm{Grade}_{ij}-5) - 22(\\mathrm{Special~Education}_j) + 218(\\mathrm{Attendance}_j) +\\\\ &1.89(\\mathrm{Grade}_{ij}-5)(\\mathrm{Special~Education}_j)\n\\end{split}\n\\]\nInterpreting the interaction effect:\n\nThe effect of grade-level on reading scores varies by special education status after controlling for differences in attendance. OR\nThe effect of special education status on reading scores varies by grade-level after controlling for differences in attendance.\n\nTo better understand this interaction, we can write out and then plot the average fitted equation for special education students and that for non-special education students. We will control for attendance by setting it to its mean value. Note that since we mean centered attendance, this will drop the attendance term from the fitted equation.\n\\[\n\\begin{split}\n\\mathbf{Non\\mbox{-}SpEd:~}\\mathrm{Reading~Score}_{ij} &= 209 + 4.11(\\mathrm{Grade}_{ij}-5) - 22(0) + 218(0.9564 - 0.9564) + 1.89(\\mathrm{Grade}_{ij}-5)(0) \\\\[1ex]\n&= 209 + 4.11(\\mathrm{Grade}_{ij}-5) \\\\[4ex]\n\\mathbf{SpEd:~}\\mathrm{Reading~Score}_{ij} &= 209 + 4.11(\\mathrm{Grade}_{ij}-5) - 22(1) + 218(0.9564 - 0.9564) + 1.89(\\mathrm{Grade}_{ij}-5)(1) \\\\[1ex]\n&= 187 + 6.00(\\mathrm{Grade}_{ij}-5)\n\\end{split}\n\\]\nThis results in essentially the same plot as Model E (the special education students profile will have a slightly lower reading score in the 5th grade). The difference is now the profiles represent the growth in reading scores for special education and non special education students who have an average attendance. Student specific fitted equations would have to be computed using the student’s special education status and the student’s actual attendance, as well as their random-effects.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>LMER: Other Random-Effects and Covariates</span>"
    ]
  },
  {
    "objectID": "05-03-lmer-other-random-effects-and-covariates.html#adopting-a-model",
    "href": "05-03-lmer-other-random-effects-and-covariates.html#adopting-a-model",
    "title": "15  LMER: Other Random-Effects and Covariates",
    "section": "15.5 Adopting a Model",
    "text": "15.5 Adopting a Model\nWe can use information criteria to select from out six candidate models since each model has the same outcome and has used the same data.\n\n# Model evidence\naictab(\n  cand.set = list(lmer.a, lmer.b, lmer.c, lmer.d, lmer.e, lmer.f),\n  modnames = c(\"Model A\", \"Model B\", \"Model C\", \"Model D\", \"Model E\", \"Model F\")\n)\n\n\nModel selection based on AICc:\n\n        K   AICc Delta_AICc AICcWt Cum.Wt      LL\nModel F 9 635.36       0.00   0.33   0.33 -307.53\nModel C 6 635.56       0.19   0.30   0.63 -311.26\nModel D 7 636.01       0.65   0.24   0.87 -310.31\nModel E 8 637.56       2.20   0.11   0.98 -309.87\nModel B 4 641.50       6.14   0.02   1.00 -316.51\nModel A 3 687.07      51.70   0.00   1.00 -340.39\n\n\nGiven the data and candidate set of models, Model F has the most empirical evidence. However, there also seems to be a fair amount of evidence for Models C, D, and E. All of these models included random-effects of both intercept and slope.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>LMER: Other Random-Effects and Covariates</span>"
    ]
  },
  {
    "objectID": "05-03-lmer-other-random-effects-and-covariates.html#rocking-a-prettier-table-of-regression-results",
    "href": "05-03-lmer-other-random-effects-and-covariates.html#rocking-a-prettier-table-of-regression-results",
    "title": "15  LMER: Other Random-Effects and Covariates",
    "section": "15.6 Rocking a Prettier Table of Regression Results",
    "text": "15.6 Rocking a Prettier Table of Regression Results\nWhile the table of regression results produced by htmlreg() is fine, there are several small tweaks that we might want to make, including:\n\nRows indicating the fixed-effects, variance components, and model-level summaries\nSome horizontal guide lines to separate these sections\n\nWhen you run the htmlreg() syntax, it produces the HTML syntax that is used to create the table. Embedding this in a code chunk with the code chunk option #| results: asis will actually render the HTML syntax and produce the table in the resulting HTML file.\nOne way of customizing the output is to modify the HTML syntax. To do this we run the htmlreg() in the Console.\n\n# View HTML code\nhtmlreg(\n  l = list(lmer.a, lmer.b, lmer.c, lmer.d, lmer.e, lmer.f),\n  stars = numeric(0),    #No p-value stars\n  digits = 2,\n  padding = 20, #Add space around columns (you may need to adjust this via trial-and-error)\n  custom.model.names = c(\"Model A\", \"Model B\", \"Model C\", \"Model D\", \"Model E\", \"Model F\"),\n  custom.coef.names = c(\"Intercept\", \"Grade\", \"Special Education Status\", \"Attendance\",\n                        \"Grade x Special Education Status\"),\n  reorder.coef = c(2:5, 1), #Put intercept at bottom of table\n  include.loglik = FALSE, #Omit log-likelihood\n  include.aic = FALSE,    #Omit AIC\n  include.bic = FALSE,    #Omit BIC\n  include.nobs = FALSE,   #Omit sample size\n  include.groups = FALSE, #Omit group size\n  include.deviance = TRUE,\n  custom.gof.names = c(\"Deviance\", \"Level-2 (Intercept)\", \"Level-1\", \"Level-2 (Slope)\", \"Level-2 Covariance\"), # Rename variance component rows\n  custom.gof.rows = list(\n    AICc = c(AICc(lmer.a), AICc(lmer.b), AICc(lmer.c), AICc(lmer.d), AICc(lmer.e), AICc(lmer.f))  # Add AICc values\n  ),\n  reorder.gof = c(3, 5, 6, 4, 2, 1),\n  caption = \"Taxonomy of models predicting longitudinal variation in students' reading scores.\",\n  caption.above = TRUE, #Move caption above table\n  inner.rules = 1, #Include line rule before model-level output\n  outer.rules = 1 , #Include line rules around table\n  custom.note = \"*Note.* The grade predictor was centered at the initial measurement occasion of 5th-grade. Special education status was dummy-coded using non-special education students as the reference group. The attendance variable was mean centered.\"\n)\n\nThis produces the raw HTML code that creates a table (below). Copy and paste the resulting HTML syntax into your QMD file. (Paste this syntax outside of a code chunk!)\n\n&lt;table class=\"texreg\" style=\"margin: 10px auto;border-collapse: collapse;border-spacing: 0px;color: #000000;border-top: 1px solid #000000;\"&gt;\n&lt;caption&gt;Taxonomy of models predicting longitudinal variation in students' reading scores.&lt;/caption&gt;\n&lt;thead&gt;\n&lt;tr&gt;\n&lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/th&gt;\n&lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;Model A&lt;/th&gt;\n&lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;Model B&lt;/th&gt;\n&lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;Model C&lt;/th&gt;\n&lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;Model D&lt;/th&gt;\n&lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;Model E&lt;/th&gt;\n&lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;Model F&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr style=\"border-top: 1px solid #000000;\"&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Grade&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;4.36&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;4.36&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;4.36&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;4.11&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;4.11&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(0.52)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(0.70)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(0.70)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(0.74)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(0.74)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Special Education Status&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-15.77&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-19.40&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-22.01&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(10.96)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(11.61)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(11.38)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Attendance&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;1.89&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;1.89&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(2.01)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(2.01)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Grade x Special Education Status&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;218.39&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(89.83)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Intercept&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;212.64&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;206.09&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;206.09&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;208.24&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;208.74&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;209.10&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(3.96)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(4.04)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(4.23)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(4.26)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(4.29)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(4.18)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr style=\"border-top: 1px solid #000000;\"&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Level-2 (Intercept)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;329.66&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;337.57&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;381.44&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;338.62&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;337.07&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;320.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Level-2 (Slope)&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;7.38&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;7.38&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;6.96&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;6.96&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Level-2 Covariance&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-19.12&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-15.60&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-14.79&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-23.85&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Level-1&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;61.62&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;29.89&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;17.59&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;17.59&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;17.59&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;17.59&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Deviance&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;680.78&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;633.02&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;622.52&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;620.61&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;619.74&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;615.05&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr style=\"border-bottom: 1px solid #000000;\"&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;AICc&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;687.07&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;641.50&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;635.56&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;636.01&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;637.56&lt;/td&gt;\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;635.36&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;tfoot&gt;\n&lt;tr&gt;\n&lt;td style=\"font-size: 0.8em;\" colspan=\"7\"&gt;*Note.* The grade predictor was centered at the initial measurement occasion of 5th-grade. Special education status was dummy-coded using non-special education students as the reference group. The attendance variable was mean centered.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tfoot&gt;\n&lt;/table&gt;\n\n\nIf you are going to create the table by modifying the HTML, you should set the caption= argument in htmlreg() to the caption you want.\n\nHTML code consists of tags that define certain features. There is typically an opening tag &lt;tag&gt; and a closing tag &lt;/tag&gt;. The tags in the syntax produced by htmlreg() are all related to producing different features of a table:\n\n&lt;table&gt; and &lt;/table&gt; define a table\n&lt;caption&gt; and &lt;/caption&gt; define a the table caption\n&lt;thead&gt; and &lt;/thead&gt; define the table header\n&lt;tbody&gt; and &lt;/tbody&gt; define the table body\n&lt;tfoot&gt; and &lt;/tfoot&gt; define the table footer\n&lt;tr&gt; and &lt;/tr&gt; define a table row\n&lt;td&gt; and &lt;/td&gt; define a cell (table data) in a row (Note that in the header we use &lt;th&gt; and &lt;/th&gt;. This makes the cell content bold by default.)\n\nWithin these tags their are certain arguments (called attributes in the HTML nomenclature). The most common is the style= attribute. This consists of CSS syntax that styles the contents of the tag. For example, the syntax:\n\n&lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Grade&lt;/td&gt;\n\nproduces a cell in the table that includes the content ‘Grade’. The CSS syntax in style= includes both left- and right-side padding to include space around the cell content.\nWe will manually include additional HTML and CSS syntax to the existing table syntax to customize our output. For example to add a row that includes the text Fixed-effects and also includes a top border (horizontal guide line), we use:\n\n&lt;tr style=\"border-top: 1px solid #000000;\"&gt;\n  &lt;td style=\"text-align: center; padding-top:5px; padding-bottom:10px;\" colspan=\"7\"&gt;Fixed-effects\n&lt;/tr&gt;\n\nThis adds a row with top border that is 1 pixel thick, solid, and black. The text in th cell will be centered and have some padding space above and below the text (this allows the content of the row to breathe). The colspan=\"7\" attribute spans the cell content across all seven columns of the table.\nI have added this to include a row to indicate ‘Fixed-effects’. ‘Random-effects (Variance components)’, and ‘Model-level summaries’. (You can view the syntax used in the RMD file. I have included comments to indicate where these rows are added.)\n\n\nCode\n&lt;!-- MODIFIED TABLE --&gt;\n\n&lt;table class=\"texreg\" style=\"margin: 10px auto;border-collapse: collapse;border-spacing: 0px;color: #000000;border-top: 1px solid #000000;\"&gt;\n&lt;caption&gt;Taxonomy of models predicting longitudinal variation in students' reading scores.&lt;/caption&gt;\n&lt;thead&gt;\n  &lt;tr&gt;\n    &lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/th&gt;\n    &lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;Model A&lt;/th&gt;\n    &lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;Model B&lt;/th&gt;\n    &lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;Model C&lt;/th&gt;\n    &lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;Model D&lt;/th&gt;\n    &lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;Model E&lt;/th&gt;\n    &lt;th style=\"padding-left: 20px;padding-right: 20px;\"&gt;Model F&lt;/th&gt;\n  &lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;!-- Add row indicating fixed-effects --&gt;\n&lt;tr style=\"border-top: 1px solid #000000;\"&gt;\n  &lt;td style=\"text-align: center; padding-top:5px; padding-bottom:10px;\" colspan=\"7\"&gt;Fixed-effects\n&lt;/tr&gt;\n&lt;!-- In the next row, we remove the border that was there so we don't get a double border --&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Grade&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;4.36&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;4.36&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;4.36&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;4.11&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;4.11&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(0.52)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(0.70)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(0.70)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(0.74)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(0.74)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Special Education Status&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-15.77&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-19.40&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-22.01&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(10.96)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(11.61)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(11.38)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Attendance&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;1.89&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;1.89&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(2.01)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(2.01)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Grade x Special Education Status&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;218.39&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(89.83)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Intercept&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;212.64&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;206.09&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;206.09&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;208.24&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;208.74&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;209.10&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(3.96)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(4.04)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(4.23)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(4.26)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(4.29)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;(4.18)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;!-- Add row indicating random-effects --&gt;\n&lt;tr style=\"border-top: 1px solid #000000;\"&gt;\n  &lt;td style=\"text-align: center; padding-top:5px; padding-bottom:10px;\" colspan=\"7\"&gt;Random-effects (Variance Components)\n&lt;/tr&gt;\n&lt;!-- In the next row, we remove the border that was there so we don't get a double border --&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Level-2 (Intercept)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;329.66&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;337.57&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;381.44&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;338.62&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;337.07&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;320.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Level-2 (Slope)&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;7.38&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;7.38&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;6.96&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;6.96&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Level-2 Covariance&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;&nbsp;&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-19.12&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-15.60&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-14.79&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;-23.85&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Level-1&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;61.62&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;29.89&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;17.59&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;17.59&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;17.59&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;17.59&lt;/td&gt;\n&lt;/tr&gt;\n&lt;!-- Add row indicating model-level summaries--&gt;\n&lt;tr style=\"border-top: 1px solid #000000;\"&gt;\n  &lt;td style=\"text-align: center; padding-top:5px; padding-bottom:10px;\" colspan=\"7\"&gt;Model-level summaries\n&lt;/tr&gt;\n&lt;tr&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;Deviance&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;680.78&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;633.02&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;622.52&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;620.61&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;619.74&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;615.05&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr style=\"border-bottom: 1px solid #000000;\"&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;AICc&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;687.07&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;641.50&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;635.56&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;636.01&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;637.56&lt;/td&gt;\n  &lt;td style=\"padding-left: 20px;padding-right: 20px;\"&gt;635.36&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;tfoot&gt;\n&lt;tr&gt;\n  &lt;td style=\"font-size: 0.8em;\" colspan=\"7\"&gt;&lt;i&gt;Note.&lt;/i&gt; The grade predictor was centered at the initial measurement occasion of 5th-grade. Special education status was dummy-coded using non-special education students as the reference group. The attendance variable was mean centered.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tfoot&gt;\n&lt;/table&gt;\n\n\n\n\n\nTaxonomy of models predicting longitudinal variation in students’ reading scores.\n\n\n\n\n \n\n\nModel A\n\n\nModel B\n\n\nModel C\n\n\nModel D\n\n\nModel E\n\n\nModel F\n\n\n\n\n\n\n\nFixed-effects\n\n\n\n\nGrade\n\n\n \n\n\n4.36\n\n\n4.36\n\n\n4.36\n\n\n4.11\n\n\n4.11\n\n\n\n\n \n\n\n \n\n\n(0.52)\n\n\n(0.70)\n\n\n(0.70)\n\n\n(0.74)\n\n\n(0.74)\n\n\n\n\nSpecial Education Status\n\n\n \n\n\n \n\n\n \n\n\n-15.77\n\n\n-19.40\n\n\n-22.01\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(10.96)\n\n\n(11.61)\n\n\n(11.38)\n\n\n\n\nAttendance\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n1.89\n\n\n1.89\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(2.01)\n\n\n(2.01)\n\n\n\n\nGrade x Special Education Status\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n218.39\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(89.83)\n\n\n\n\nIntercept\n\n\n212.64\n\n\n206.09\n\n\n206.09\n\n\n208.24\n\n\n208.74\n\n\n209.10\n\n\n\n\n \n\n\n(3.96)\n\n\n(4.04)\n\n\n(4.23)\n\n\n(4.26)\n\n\n(4.29)\n\n\n(4.18)\n\n\n\n\n\nRandom-effects (Variance Components)\n\n\n\n\nLevel-2 (Intercept)\n\n\n329.66\n\n\n337.57\n\n\n381.44\n\n\n338.62\n\n\n337.07\n\n\n320.00\n\n\n\n\nLevel-2 (Slope)\n\n\n \n\n\n \n\n\n7.38\n\n\n7.38\n\n\n6.96\n\n\n6.96\n\n\n\n\nLevel-2 Covariance\n\n\n \n\n\n \n\n\n-19.12\n\n\n-15.60\n\n\n-14.79\n\n\n-23.85\n\n\n\n\nLevel-1\n\n\n61.62\n\n\n29.89\n\n\n17.59\n\n\n17.59\n\n\n17.59\n\n\n17.59\n\n\n\n\n\nModel-level summaries\n\n\n\nDeviance\n\n\n680.78\n\n\n633.02\n\n\n622.52\n\n\n620.61\n\n\n619.74\n\n\n615.05\n\n\n\n\nAICc\n\n\n687.07\n\n\n641.50\n\n\n635.56\n\n\n636.01\n\n\n637.56\n\n\n635.36\n\n\n\n\n\n\nNote. The grade predictor was centered at the initial measurement occasion of 5th-grade. Special education status was dummy-coded using non-special education students as the reference group. The attendance variable was mean centered.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>LMER: Other Random-Effects and Covariates</span>"
    ]
  },
  {
    "objectID": "05-03-lmer-other-random-effects-and-covariates.html#footnotes",
    "href": "05-03-lmer-other-random-effects-and-covariates.html#footnotes",
    "title": "15  LMER: Other Random-Effects and Covariates",
    "section": "",
    "text": "Note that the random effects we included in Model D allow individual students to have different growth rates from the average growth rate.↩︎",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>LMER: Other Random-Effects and Covariates</span>"
    ]
  },
  {
    "objectID": "05-04-lmer-alt-representations-and-assumptions.html",
    "href": "05-04-lmer-alt-representations-and-assumptions.html",
    "title": "19  LMER: Alternative Representations and Assumptions",
    "section": "",
    "text": "19.1 Writing the Statistical Model as a Set of Multilevel Equations\nIn this set of notes, you will learn alternative ways of representing the linear mixed-effects model. You will also learn about the underlying assumptions for the linear mixed-effects model, as well as how to evaluate them empirically. To do this, we will use data from the file minneapolis.csv (see data codebook).\nTo illustrate some the concepts in this set of notes, we will consider the following three models:\n\\[\n\\begin{split}\n\\mathbf{Model~1:~~}\\mathrm{Reading~Score}_{ij} = &\\big[\\beta_0 + b_{0j}\\big] + \\beta_1(\\mathrm{Grade}_{ij}-5) + \\\\\n&~~\\beta_2(\\mathrm{Special~Education}_j)+ \\epsilon_{ij}\\\\[2em]\n\\mathbf{Model~2:~~}\\mathrm{Reading~Score}_{ij} = &\\big[\\beta_0 + b_{0j}\\big] + \\big[\\beta_1 + b_{1j}\\big](\\mathrm{Grade}_{ij}-5) + \\\\ &~~\\beta_2(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\\\\[2em]\n\\mathbf{Model~3:~~}\\mathrm{Reading~Score}_{ij} = &\\big[\\beta_0 + b_{0j}\\big] + \\big[\\beta_1 + b_{1j}\\big](\\mathrm{Grade}_{ij}-5) + \\\\ &~~\\beta_2(\\mathrm{Special~Education}_j) + \\\\\n&~~\\beta_3(\\mathrm{Grade}_{ij}-5)(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\n\\end{split}\n\\]\nwhere,\nAnother way we can express the model is by separating the mixed-effects model equation into multiple equations; one for each level of variation. For example, each of the mixed-effects models listed above could be separated into two equations: a Level-1 equation that includes effects and residuals that explain the within-student variation and a set of Level-2 equations that include the effects and residuals that explain the between-student variation. As an example, take the equation for Model 1:\n\\[\n\\begin{split}\n\\mathrm{Reading~Score}_{ij} = &\\big[\\beta_0 + b_{0j}\\big] + \\beta_1(\\mathrm{Grade}_{ij}-5) + \\\\\n&~~\\beta_2(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\n\\end{split}\n\\]\nWe initially write the Level-1 equation which includes the effect for the intercept, an effect for each of the within-student predictors, and the within-student error term from the mixed-effects model. When writing the Level-1 model, we add a j subscript to the intercept and predictor effects to indicate that the particular effect may be unique to a particular student. The Level-1 equation for Model 1 is:\n\\[\n\\mathrm{Reading~Score}_{ij} = \\beta_{0j} + \\beta_{1j}(\\mathrm{Grade~Level}_{ij}) + \\epsilon_{ij}\n\\]\nThe Level-1 equation describes the within-student variation in reading scores. It says that this variation is decomposed into that which is explained by differences in grade-level and unexplained random error. Remember, the j subscript indicates that the term may vary across students. So from this equation we can see that the intercept and effect of grade-level may vary across students. (We won’t know for sure whether they vary or not until we see the level-2 equations.)\nAfter writing the Level-1 equation, we can write out the Level-2 equation(s). There will be a Level-2 equation for each of the effects in the Level-1 model. In our example, since we have two effects in the Level-1 model (\\(\\beta_{0j}\\) and \\(\\beta_{1j}\\)), there will be two Level-2 equations. In each Level-2 equations, the outcome is one of the effects from the Level-1 equation. These equations describe how the student-specific intercept and slopes differ across students. As such, they include the random-effects and any student-level effects from the mixed-effects representation. For example, we can write the Level-2 equations for Model 2 as:\n\\[\n\\begin{split}\n\\beta_{0j} &= \\beta_0 + \\beta_2(\\mathrm{Special~Education}_j) + b_{0j} \\\\\n\\beta_{1j} &= \\beta_1\\\\\n\\end{split}\n\\]\nThese equations indicate that the student-specific intercepts are a function of some part common to all schools (\\(\\beta_0\\)), an effect of special education status (\\(\\beta_2\\)), and student-to-student error (\\(b_{0j}\\)). The random-effect is the reason that students can have different intercepts. The student-specific slope (\\(\\beta_{1j}\\)), on the other hand, does not vary across students; it is the same for each student. This is because there is no random-effect for slope in Model 1.\nTogether these equations are referred to as the set of multilevel equations:\n\\[\n\\begin{split}\n&\\mathbf{Level\\mbox{-}1:}\\\\\n&\\qquad\\mathrm{Reading~Score}_{ij} = \\beta_{0j} + \\beta_{1j}(\\mathrm{Grade~Level}_{ij}) + \\epsilon_{ij}\\\\\n&\\mathbf{Level\\mbox{-}2:}\\\\\n&\\qquad\\beta_{0j} = \\beta_0 + \\beta_2(\\mathrm{Special~Education}_j) b_{0j}\\\\\n&\\qquad\\beta_{1j} = \\beta_1\\\\\n\\end{split}\n\\]",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>LMER: Alternative Representations and Assumptions</span>"
    ]
  },
  {
    "objectID": "05-04-lmer-alt-representations-and-assumptions.html#writing-the-statistical-model-as-a-set-of-multilevel-equations",
    "href": "05-04-lmer-alt-representations-and-assumptions.html#writing-the-statistical-model-as-a-set-of-multilevel-equations",
    "title": "19  LMER: Alternative Representations and Assumptions",
    "section": "",
    "text": "19.1.1 Multilevel Equation for Model 2\nAs a second example, consider the composite (mixed-effects) model for Model 2:\n\\[\n\\begin{split}\n\\mathrm{Reading~Score}_{ij} = &\\big[\\beta_0 + b_{0j}\\big] + \\big[\\beta_1 + b_{1j}\\big](\\mathrm{Grade}_{ij}-5) +\\\\\n&~~\\beta_2(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\n\\end{split}\n\\]\nThe Level-1 equation is again:\n\\[\n\\mathrm{Reading~Score}_{ij} = \\beta_{0j} + \\beta_{1j}(\\mathrm{Grade~Level}_{ij}) + \\epsilon_{ij}\n\\]\nThe Level-2 equations are:\n\\[\n\\begin{split}\n\\beta_{0j} &= \\beta_0 + \\beta_2(\\mathrm{Special~Education}_j) + b_{0j} \\\\\n\\beta_{1j} &= \\beta_1 + b_{1j}\\\\\n\\end{split}\n\\]\nThe Level-2 intercept equation hasn’t changed from that for Model 1. However, the equation for the effect of grade-level now includes the random-effect of slope. This means that student’s growth rate is allowed to vary from the average.\n\n\n\n19.1.2 Going from Multilevel Equations to the Mixed-Effects Model\nIf we have the multilevel equations, we can substitute the Level-2 equation(s) into the Level-1 equation to get the composite equation or mixed-effects equation. For example, for Model 2, substituting \\(\\beta_0 + \\beta_2(\\mathrm{Special~Education}_j) + b_{0j}\\) into \\(\\beta_{0j}\\) and \\(\\beta_1 + b_{1j}\\) into \\(\\beta_{1j}\\) gives us:\n\\[\n\\mathrm{Reading~Score}_{ij} = \\bigg[\\beta_0 + \\beta_2(\\mathrm{Special~Education}_j) + b_{0j}\\bigg] + \\bigg[\\beta_1 + b_{1j}\\bigg](\\mathrm{Grade~Level}_{ij}) + \\epsilon_{ij}\n\\]\nRe-arranging this:\n\\[\n\\begin{split}\n\\mathrm{Reading~Score}_{ij} = &\\bigg[\\beta_0 + b_{0j}\\bigg] + \\bigg[\\beta_1 + b_{1j}\\bigg](\\mathrm{Grade~Level}_{ij}) + \\\\\n&~~\\beta_2(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\n\\end{split}\n\\]",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>LMER: Alternative Representations and Assumptions</span>"
    ]
  },
  {
    "objectID": "05-04-lmer-alt-representations-and-assumptions.html#model-3-interaction-effect-with-time-predictor",
    "href": "05-04-lmer-alt-representations-and-assumptions.html#model-3-interaction-effect-with-time-predictor",
    "title": "19  LMER: Alternative Representations and Assumptions",
    "section": "19.2 Model 3: Interaction Effect with Time Predictor",
    "text": "19.2 Model 3: Interaction Effect with Time Predictor\nRecall the composite (a.k.a., mixed-effects) expression of Model 3 was:\n\\[\n\\begin{split}\n\\mathrm{Reading~Score}_{ij} = &\\big[\\beta_0 + b_{0j}\\big] + \\big[\\beta_1 + b_{1j}\\big](\\mathrm{Grade}_{ij}-5) + \\\\\n&~~\\beta_2(\\mathrm{Special~Education}_j) + \\\\\n&~~\\beta_3(\\mathrm{Grade}_{ij}-5)(\\mathrm{Special~Education}_j) + \\epsilon_{ij}\n\\end{split}\n\\]\nThis model includes an interaction effect between the time predictor (grade-level) and the special education status predictor. Let’s dRe-express this model by distributing the time predictor across the fixed- and random-effects of slope, and move all the terms with a time predictor so they are together in the equation, and all the terms without a time predictor together. (I will use large square brackets to show this grouping.) The within-student residual will stay at the end of the equation.\n\\[\n\\begin{split}\n\\mathrm{Reading~Score}_{ij} = &\\bigg[\\beta_0 + \\beta_2(\\mathrm{Special~Education}_j) + b_{0j}\\bigg] + \\\\\n&\\bigg[\\beta_1(\\mathrm{Grade}_{ij}-5) + \\beta_3(\\mathrm{Grade}_{ij}-5)(\\mathrm{Special~Education}_j) + b_{1j}(\\mathrm{Grade}_{ij}-5)\\bigg] + \\epsilon_{ij}\n\\end{split}\n\\]\nExtracting the time predictor from the second group:\n\\[\n\\begin{split}\n\\mathrm{Reading~Score}_{ij} = &\\bigg[\\underbrace{\\beta_0 + \\beta_2(\\mathrm{Special~Education}_j) + b_{0j}}_{\\beta_{0j}}\\bigg] + \\\\\n&\\bigg[\\underbrace{\\beta_1 +\\beta_3(\\mathrm{Special~Education}_j) + b_{1j}}_{\\beta_{1j}}\\bigg](\\mathrm{Grade}_{ij}-5) + \\epsilon_{ij}\n\\end{split}\n\\]\nThis helps us write the multilevel equations:\n\\[\n\\begin{split}\n&\\mathbf{Level\\mbox{-}1:}\\\\\n&\\qquad\\mathrm{Reading~Score}_{ij} = \\beta_{0j} + \\beta_{1j}(\\mathrm{Grade}_{ij}-5) + \\epsilon_{ij}\\\\\n&\\mathbf{Level\\mbox{-}2:}\\\\\n&\\qquad\\beta_{0j} = \\beta_0 + \\beta_2(\\mathrm{Special~Education}_j) + b_{0j}\\\\\n&\\qquad\\beta_{1j} = \\beta_1 + \\beta_3(\\mathrm{Special~Education}_j) + b_{1j}\\\n\\end{split}\n\\]\nYou can substitute the Level-2 equations back into the Level-1 equation and that should give you the initial composite equation.\n\nFYI\nNotice that between-student predictors that are main-effects appear in the Level-2 intercept equation, and between-student predictors that are included in an interaction-effect with a within-student predictor appear in the Level-2 slope equation. Because we need to include the main-effects of any predictor that is also a part of an interaction term, any predictors included in the Level-2 slope equation also need to be included in the Level-2 intercept equation.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>LMER: Alternative Representations and Assumptions</span>"
    ]
  },
  {
    "objectID": "05-04-lmer-alt-representations-and-assumptions.html#guidelines-for-writing-the-multilevel-equations",
    "href": "05-04-lmer-alt-representations-and-assumptions.html#guidelines-for-writing-the-multilevel-equations",
    "title": "19  LMER: Alternative Representations and Assumptions",
    "section": "19.3 Guidelines for Writing the Multilevel Equations",
    "text": "19.3 Guidelines for Writing the Multilevel Equations\nHere are some guidelines in helping you think about writing multilevel equations.\n\nWrite the Level-1 equation first. This will be an equation that expresses the outcome’s relationship to a series of within-student parameters, and a within-student residual.\nThe number of within-student parameters in the Level-1 equation (aside from the residual) dictate the number of Level-2 equations you will have.\nThe within-student parameters from the Level-1 equation will be the outcomes in the Level-2 equations.\nRandom-effects are the residuals in the Level-2 equations, and therefore are in the Level-2 equations; one per equation.\nVariables from the data go to their appropriate level. For example within-student variables (i.e., having an ij subscript) will be put in the Level-1 equation, and between-student predictors (i.e., having only a j subscript) will be put in one or more of the Level-2 equations.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>LMER: Alternative Representations and Assumptions</span>"
    ]
  },
  {
    "objectID": "05-04-lmer-alt-representations-and-assumptions.html#multilevel-equations-for-fixed-effects-models",
    "href": "05-04-lmer-alt-representations-and-assumptions.html#multilevel-equations-for-fixed-effects-models",
    "title": "19  LMER: Alternative Representations and Assumptions",
    "section": "19.4 Multilevel Equations for Fixed-Effects Models",
    "text": "19.4 Multilevel Equations for Fixed-Effects Models\nOur conventional fixed-effects regression models (LM) can also be expressed as a multilevel model. For example, consider the fixed-effect model that includes an intercept and effect of grade-level:\n\\[\n\\mathrm{Reading~Score}_{i} = \\beta_{0} + \\beta_{1}(\\mathrm{Grade}_{i}-5) + \\epsilon_{i}\n\\]\nThe multilevel model would specify that the Level-2 equations would only include fixed-effects (no random-effects). Thus when we substitute them back into the Level-1 model we only have fixed-effects in the model:\n\\[\n\\begin{split}\n\\mathbf{Level\\mbox{-}1:}\\\\\n&~ \\mathrm{Reading~Score}_{ij} = \\beta_{0j} + \\beta_{1j}(\\mathrm{Grade}_{ij}-5) + \\epsilon_{ij}\\\\\n\\mathbf{Level\\mbox{-}2:}\\\\\n&~ \\beta_{0j} = \\beta_{0}\\\\\n&~ \\beta_{1j} = \\beta_{1}\n\\end{split}\n\\]",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>LMER: Alternative Representations and Assumptions</span>"
    ]
  },
  {
    "objectID": "05-04-lmer-alt-representations-and-assumptions.html#why-are-multilevel-expressions-helpful",
    "href": "05-04-lmer-alt-representations-and-assumptions.html#why-are-multilevel-expressions-helpful",
    "title": "19  LMER: Alternative Representations and Assumptions",
    "section": "19.5 Why are Multilevel Expressions Helpful?",
    "text": "19.5 Why are Multilevel Expressions Helpful?\nExpressing the model as a set of multilevel equations can be helpful for readers. First, it explicitly separates the sources of variation and the predictors of these sources of variation into different levels. In our example there are two sources of variation: within-student variation and between-student variation. The Level-1 model attempts to describe the within-student variation and, hence, only includes within-student predictors. The Level-2 models attempts to describe the between-student variation and only includes between-student predictors.\nSecondly, the multilevel expression of the model helps us think about what the predictors at each level are actually doing. Level-1 predictors explain variation in the outcome. In our example, they are explaining variation in students’ reading scores. The Level-2 predictors are explaining variation in the student-specific intercepts and slopes—they explain Level-2 variation.\nThirdly, the multilevel expression of the model helps us see that the random-effects are residuals; they are residuals of the Level-2 models. This helps us think about the more general statistical model. For example, the general statistical model for a model that includes fixed-effects of two predictors and a random-effect of intercept is:\n\\[\n\\begin{split}\n\\mathbf{Level\\mbox{-}1:}\\\\\n&~ Y_{ij} = \\beta_{0j} + \\beta_{1j}(X_{1ij}) + \\beta_{2j}(X_{2ij})  + \\epsilon_{ij}\\\\\n\\mathbf{Level\\mbox{-}2:}\\\\\n&~ \\beta_{0j} = \\beta_{0} + b_{0j}\\\\\n&~ \\beta_{1j} = \\beta_{1} \\\\\n&~ \\beta_{2j} = \\beta_{2}\n\\end{split}\n\\]\nwhere\n\\[\n\\begin{split}\n\\epsilon_{ij} &\\sim \\mathcal{N}\\bigg(0,\\sigma^2_{\\epsilon}\\bigg)\\\\[1em]\nb_{0j} &\\sim \\mathcal{N}\\bigg(0,\\sigma^2_{0}\\bigg)\n\\end{split}\n\\]\nIn the mixed-effects model we put distributional assumptions on both the Level-1 residuals and the Level-2 residuals.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>LMER: Alternative Representations and Assumptions</span>"
    ]
  },
  {
    "objectID": "05-04-lmer-alt-representations-and-assumptions.html#evaluating-the-assumptions-an-example",
    "href": "05-04-lmer-alt-representations-and-assumptions.html#evaluating-the-assumptions-an-example",
    "title": "19  LMER: Alternative Representations and Assumptions",
    "section": "19.6 Evaluating the Assumptions: An Example",
    "text": "19.6 Evaluating the Assumptions: An Example\nEvaluating the assumptions in a mixed-effects model is a bit more complicated than it is in a fixed-effects model since the mixed-effects model includes multiple residuals; Level-1 residuals and the Level-2 residuals. There are also potential assumptions bout the covariances between the Level-2 residuals, depending on the model that was fitted. To simplify things, in this course, we will evaluate the distributional assumptions placed on the Level-1 residuals, and we will also evaluate the normality assumption on the random-effects.\nTo illustrate assumption checking in practice, we will evaluate the assumptions for fitting Model 2. Recall that the multilevel expression of Model 2 was:\n\\[\n\\begin{split}\n&\\mathbf{Level\\mbox{-}1:}\\\\\n&\\qquad\\mathrm{Reading~Score}_{ij} = \\beta_{0j} + \\beta_{1j}(\\mathrm{Grade}_{ij}-5) + \\epsilon_{ij}\\\\[2ex]\n&\\mathbf{Level\\mbox{-}2:}\\\\\n&\\qquad\\beta_{0j} = \\beta_0 + \\beta_2(\\mathrm{Special~Education}_j) + b_{0j}\\\\\n&\\qquad\\beta_{1j} = \\beta_1 + b_{1j}\\\\\n\\end{split}\n\\]\nwhere,\n\\[\n\\begin{split}\n\\epsilon_{ij} &\\sim \\mathcal{N}\\bigg(0,\\sigma^2_{\\epsilon}\\bigg)\\\\[1em]\nb_{0j} &\\sim \\mathcal{N}\\bigg(0,\\sigma^2_{0}\\bigg) \\\\[1em]\nb_{1j} &\\sim \\mathcal{N}\\bigg(0,\\sigma^2_{1}\\bigg)\n\\end{split}\n\\]\nThe assumptions are based on the Level-1 residuals (\\(\\epsilon_{ij}\\)) and the Level-2 residuals, or random-effects (\\(b_{0j}\\) and \\(b_{1j}\\)).1 So we need to examine the distributions of those three components. To begin, we will fit the mixed-effects model, which is:\n\\[\n\\mathrm{Reading~Score}_{ij} = \\beta_0 +  \\beta_1(\\mathrm{Grade}_{ij}-5) + \\beta_2(\\mathrm{Special~Education}_j) + b_{0j} + b_{1j}(\\mathrm{Grade}_{ij}-5) + \\epsilon_{ij}\n\\]\n\n# Fit Model 2\nlmer.2 = lmer(reading_score ~ 1 + I(grade-5) + special_ed + (1 + I(grade-5) | student_id),\n              data = mpls, REML = FALSE)\n\n\n\n19.6.1 Evaluate Assumptions about the Level-1 Residuals\nWe will evaluate the Level-1 residuals in the exact same way we evalauted the residuals from a fixed-effects (LM) analysis. The augment() function from the {broom.mixed} package produces the Level-1 residuals and fitted values.\n\n# Augment the model to get the Level-1 residuals and fitted values\nout_2 = augment(lmer.2)\n\n# View\nout_2\n\n# A tibble: 88 × 15\n   reading_score `I(grade - 5)` special_ed student_id .fitted  .resid  .hat\n           &lt;dbl&gt;       &lt;I&lt;dbl&gt;&gt; &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1           172              0 No                  1    175. -2.52   0.561\n 2           185              1 No                  1    180.  4.86   0.282\n 3           179              2 No                  1    186. -6.76   0.280\n 4           194              3 No                  1    191.  2.62   0.554\n 5           200              0 No                  2    202. -2.25   0.561\n 6           210              1 No                  2    206.  4.34   0.282\n 7           209              2 No                  2    209. -0.0705 0.280\n 8           210              3 No                  2    212. -2.48   0.554\n 9           191              0 No                  3    192. -1.25   0.561\n10           199              1 No                  3    199.  0.143  0.282\n# ℹ 78 more rows\n# ℹ 8 more variables: .cooksd &lt;dbl&gt;, .fixed &lt;dbl&gt;, .mu &lt;dbl&gt;, .offset &lt;dbl&gt;,\n#   .sqrtXwt &lt;dbl&gt;, .sqrtrwt &lt;dbl&gt;, .weights &lt;dbl&gt;, .wtres &lt;dbl&gt;\n\n\nThe Level-1 residuals are found in the .resid column, and the .fitted column contains the \\(\\hat{Y}\\) values. As with LM residual analysis, we want to examine the normality of the residuals in a density plot (or some other plot that allows you to evaluate this), and the other assumptions by plotting the residuals against the fitted values in a scatterplot.\n\n# Density plot of the level-1 residuals\np1 = ggplot(data = out_2, aes(x = .resid)) +\n  stat_density_confidence(model = \"normal\") +\n  stat_density(geom = \"line\") +\n  theme_bw() +\n  xlab(\"Level-1 residuals\")\n\n\n# Scatterplot of the Level-1 residuals versus the fitted values\np2 = ggplot(data = out_2, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_smooth() +\n  geom_hline(yintercept = 0) +\n  theme_bw() +\n  xlab(\"Fitted values\") +\n  ylab(\"Level-1 residuals\")\n\n# Plot side-by-side\np1 | p2\n\n\n\n\n\n\n\nFigure 19.1: Plots to evaluate the level-1 residuals.\n\n\n\n\n\nBased on the plots, the distributional assumptions for the Level-1 residuals seem reasonably satisfied. The density plot suggests that the normality assumption is tenable. The scatterplot shows symmetry around the \\(Y=0\\) line (average residual is 0), and that the data are consistent with the assumption of homoskedasticity.\n\n\n\n19.6.2 Assumptions about the Random-Effects\nWe also need to examine the assumptions for any random-effects included in the model. For this course, we will examine the normality assumption. In our example that means we need to examine the normality assumption about the intercept and grade-level random-effects. To do this we need to extract the random-effects from the model into a data frame so we can use ggplot2 functions to evaluate normality.\n\n# Obtain a data frame of the random-effects\nlevel_2 = tidy(lmer.2, effects = \"ran_vals\")\n\n# View random-effects\nlevel_2\n\n# A tibble: 44 × 6\n   effect   group      level term        estimate std.error\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 ran_vals student_id 1     (Intercept)  -33.7        3.12\n 2 ran_vals student_id 2     (Intercept)   -5.99       3.12\n 3 ran_vals student_id 3     (Intercept)  -16.0        3.12\n 4 ran_vals student_id 4     (Intercept)  -12.6        3.12\n 5 ran_vals student_id 5     (Intercept)   -0.799      3.12\n 6 ran_vals student_id 6     (Intercept)  -17.9        3.12\n 7 ran_vals student_id 7     (Intercept)   -7.12       3.12\n 8 ran_vals student_id 8     (Intercept)   -2.14       3.12\n 9 ran_vals student_id 9     (Intercept)  -41.4        3.12\n10 ran_vals student_id 10    (Intercept)   -4.99       3.12\n# ℹ 34 more rows\n\n\nRecall that this tibble includes both the intercept and the grade-level random-effects. We will facet_wrap() on the term column to plot each of these distributions separately.\n\n# Density plot of the RE for intercept\nggplot(data = level_2, aes(x = estimate)) +\n  stat_density_confidence(model = \"normal\") +\n  stat_density(geom = \"line\") +\n  theme_bw() +\n  xlab(\"Level-2 residuals\") +\n  facet_wrap(~term)\n\n\n\n\n\n\n\nFigure 19.2: Density plot of the estimated random-effects for intercept (left) and grade-level (right). The confidence envelope is based on the expected variation in a normal distribution.\n\n\n\n\n\nNeither distribution of random-effects seem consistent with the assumption of normality.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>LMER: Alternative Representations and Assumptions</span>"
    ]
  },
  {
    "objectID": "05-04-lmer-alt-representations-and-assumptions.html#fixing-the-model-to-better-meet-the-assumptions",
    "href": "05-04-lmer-alt-representations-and-assumptions.html#fixing-the-model-to-better-meet-the-assumptions",
    "title": "19  LMER: Alternative Representations and Assumptions",
    "section": "19.7 Fixing the Model to Better Meet the Assumptions",
    "text": "19.7 Fixing the Model to Better Meet the Assumptions\nGiven the incredibly small amount of variation in the random-effects of grade-level, we will only include a random-effect for intercept in the model. Examination of the assumptions for this model (not shown) results in good fit to the Level-1 residual assumptions and some slight mis-fit to the normality assumption of the Level-2 residuals, with the resulting distribution being slightly right-skewed.\n\nFYI\nNote that, in practice, the violation of the normality assumption would not be a deal breaker since the remaining assumptions were met in this model. However, to facilitate understanding of log-transformations in these models, we will fit and interpret a log-transformed model.\n\nSimilar to fixed-effects models, we can apply transformations to the outcome or any of the continuous predictors. Here the log-transform can be applied to the outcome to help alleviate the non-normality we observed in the Level-2 residuals. (There are no 0 values or negative values in the outcome, so we can directly apply the log-transformation.)\n\n# Fit model\nlmer.3 = lmer(log(reading_score) ~ 1 + I(grade-5) + special_ed +\n                (1  | student_id), data = mpls, REML = FALSE)\n\n# Augment model\nout_3 = augment(lmer.3)\n\n# Obtain a data frame of the random-effects\nlevel_2 = tidy(lmer.3, effects = \"ran_vals\")\n\n# Density plot of the level-1 residuals\np1 = ggplot(data = out_3, aes(x = .resid)) +\n  stat_density_confidence(model = \"normal\") +\n  stat_density(geom = \"line\") +\n  theme_bw() +\n  xlab(\"Level-1 residuals\")\n\n# Scatterplot of the Level-1 residuals versus the fitted values\np2 = ggplot(data = out_3, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  theme_bw() +\n  xlab(\"Fitted values\") +\n  ylab(\"Level-1 residuals\")\n\n# Density plot of the level-2 residuals (RE of intercept)\np3 = ggplot(data = level_2, aes(x = estimate)) +\n  stat_density_confidence(model = \"normal\") +\n  stat_density(geom = \"line\") +\n  theme_bw() +\n  xlab(\"Level-2 residuals\")\n\n# Plot side-by-side\np1 | p2 | p3\n\n\n\n\n\n\n\nFigure 19.3: Plots to evaluate the level-1 and level-2 residuals.\n\n\n\n\n\nThe plots suggest that all the distributional assumptions for the Level-1 residuals are tenable. The density plot of the level-2 residuals also shows consistency with the assumption of normality.\n\n\n19.7.1 Interpreting the Output from the Log-Transformed Model\nWe interpret fixed-effect coefficients from the LMER similar to those from the LM. We now just apply the same interpretational changes that we did when previously log-transformed the outcomes.\n\ntidy(lmer.3, effects = \"fixed\")\n\n# A tibble: 3 × 5\n  effect term          estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)     5.34     0.0201     266.  \n2 fixed  I(grade - 5)    0.0210   0.00262      8.03\n3 fixed  special_edYes  -0.0905   0.0533      -1.70\n\n\n\nThe intercept is the average log-transformed reading score for non-special education students in the 5th grade. Back-transforming this, we find the average reading score for these students is \\(e^{5.34} = 208.52\\).\nBack-transforming the grade-level effect, we find that each one grade difference is associated with a 2% (\\(e^{0.021} = 1.02\\)) increase in reading scores, on average, after controlling for differences in special education status.\nBack-transforming the special education status effect, we find that special education students have reading scores that are \\(e^{-0.0905} = 0.91\\) times those for non-special education students, on average, after controlling for differences in grade-level.\n\nWe can also plot the average growth trajectory and any individual students’ growth trajectories for the back-transformed models to aid in the interpretation.\n\nggplot(data = mpls, aes(x = (grade-5), y = reading_score)) +\n  geom_function(fun = function(x){exp(5.34 - 0.0905 + 0.0210*x)}, color = \"#0072B2\") + #SpEd (Avg)\n  geom_function(fun = function(x){exp(5.34 + 0.0210*x)}, color = \"#E69F00\") + #Non-SpEd (Avg)\n  geom_function(fun = function(x){exp(5.34 - 0.157 + 0.0210*x)}, color = \"#E69F00\", linetype = \"dashed\") + #Student 1; non-SpEd\n  theme_light() +\n  scale_x_continuous(\n    name = \"Grade\",\n    breaks = c(0, 1, 2, 3),\n    labels = c(\"5th\", \"6th\", \"7th\", \"8th\")\n  ) +\n  ylab(\"Reading Score\")\n\n\n\n\n\n\n\nFigure 19.4: Change in reading scores over time for the average special education (solid, blue line) and non-special education student (solid, orange line). Student 1’s growth trajectory (dashed line) is also displayed.\n\n\n\n\n\nThe almost linear rate-of-change is further indication that the log-transformation is probably not necessary.",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>LMER: Alternative Representations and Assumptions</span>"
    ]
  },
  {
    "objectID": "05-04-lmer-alt-representations-and-assumptions.html#footnotes",
    "href": "05-04-lmer-alt-representations-and-assumptions.html#footnotes",
    "title": "19  LMER: Alternative Representations and Assumptions",
    "section": "",
    "text": "Technically, there is also a covariance between \\(b_{0j}\\) and \\(b_{1j}\\), but this term does not play into the assumptions.↩︎",
    "crumbs": [
      "Modeling Nonindependence",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>LMER: Alternative Representations and Assumptions</span>"
    ]
  },
  {
    "objectID": "04-00-modeling-dichotomous-outcomes.html",
    "href": "04-00-modeling-dichotomous-outcomes.html",
    "title": "Modeling Dichotomous Outcomes",
    "section": "",
    "text": "Educational scientists are often interested in outcomes that are categorical. In this section, you will be introduced to several methods for modeling relationships when the outcome of interest is dichotmous (e.g., predicting whether or not students graduate). Modeling a dichotomous categorical outcome violates many of the assumptions underlying the general linear model. Because of this educational scientists use logistic regression, one of the varieties of what is referred to as the generalized linear model.",
    "crumbs": [
      "Modeling Dichotomous Outcomes"
    ]
  },
  {
    "objectID": "04-01-linear-probability-model.html",
    "href": "04-01-linear-probability-model.html",
    "title": "13  Linear Probability Model",
    "section": "",
    "text": "13.1 Data Exploration\nIn this chapter, you will learn how about linear probability models, and why they are not typically used to model dichotomous categorical outcome variables (e.g., dummy coded outcome). We will use data from the file graduation.csv to predict variation in college graduation. See data codebook for additional infromation about the data.\nNote that in these analyses the outcome variable (degree) is a categorical variable indicating whether or not a student graduated.\nTo begin the analysis, we will explore the outcome variable degree. Since this is a categorical variable, we can look at counts and proportions.\n# Compute total number of cases\nnrow(grad)\n\n[1] 2344\n\n# Compute counts/proportions by outcome level\ngraduated = grad |&gt;\n  group_by(degree) |&gt;\n  summarize(\n    Count = n(), \n    Prop = n() / 2344\n    )\n\n# View results\ngraduated\n\n# A tibble: 2 × 3\n  degree Count  Prop\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 No       627 0.267\n2 Yes     1717 0.733\nThese counts or proportions could also be used to create a bar plot of the distribution. To do this we use the stat=\"Identity\") argument to geom_bar() since the height of the bar (Count) is given in the data. (We aren’t computing it from raw data; rather from a summary measure that has already been computed.1) We also include the counts/proportions as text in the plot. To do this, we create a new column that includes the text we want to include.2\n# New column with text to add to bars\ngraduated = graduated |&gt;\n  mutate(\n    count_prop = paste0(Count, \"\\n(\", round(Prop, 3), \")\")\n      )\n\n# Barplot\nggplot(data = graduated, aes(x = degree, y = Count)) +\n  geom_bar(stat = \"Identity\") +\n  geom_text(aes(label = count_prop), vjust = 1.1, color = \"white\") +\n  theme_light() +\n  xlab(\"Graduated\") +\n  ylab(\"Prop\")\n\n\n\n\n\n\n\nFigure 13.1: Barplot showing graduation status for the sample.\nThe analysis suggests that most students in the sample (73%) tend to graduate. Because we ultimately want to use this variable in our analysis, we will need to create a numeric indicator variable for use in those analysis by dummy-coding the degree variable.\n# Create dummy-coded degree variable\ngrad = grad |&gt;\n  mutate(\n    got_degree = if_else(degree == \"Yes\", 1, 0)\n  )\n\n# View data\ngrad\n\n# A tibble: 2,344 × 8\n   student degree   act scholarship ap_courses first_gen non_traditional\n     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;          \n 1       1 Yes       21         0            0 No        No             \n 2       2 Yes       19         0            0 No        No             \n 3       3 Yes       27         0            0 Yes       No             \n 4       4 Yes       25         0.5          0 Yes       No             \n 5       5 No        28         0           17 Yes       No             \n 6       6 Yes       21         0            0 No        Yes            \n 7       7 Yes       27         0            8 Yes       No             \n 8       8 No        20         0            0 No        No             \n 9       9 Yes       26         0            0 Yes       No             \n10      10 Yes       25         0            4 Yes       No             \n# ℹ 2,334 more rows\n# ℹ 1 more variable: got_degree &lt;dbl&gt;\nWe will also explore the act variable, which we will use as a predictor in the analysis.\n# Density plot\nggplot(data = grad, aes(x = act)) +\n  geom_density() +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Probability density\")\n\n# Summary measures\ngrad |&gt; \n  summarize( \n    M = mean(act), \n    SD = sd(act) \n    )\n\n# A tibble: 1 × 2\n      M    SD\n  &lt;dbl&gt; &lt;dbl&gt;\n1  24.8  4.15\n\n\n\n\n\n\n\n\nFigure 13.2: Density plot of the ACT scores.\nThe distribution of ACT scores is unimodal and symmetric. It indicates that the sample of students have a mean ACT score near 25. While there is a great deal of variation in ACT scores (scores range from 10 to 36), most students have a score between 21 and 29.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Probability Model</span>"
    ]
  },
  {
    "objectID": "04-01-linear-probability-model.html#data-exploration",
    "href": "04-01-linear-probability-model.html#data-exploration",
    "title": "13  Linear Probability Model",
    "section": "",
    "text": "13.1.1 Relationship between ACT Score and Proportion of Students who Obtain a Degree\nRecall that the regression model predicts the average Y at each X. Since our outcome is a dichotomous dummy-coded variable, the average represents the proportion of students with a 1. In other words, the regression will predict the proportion of students who obtained a degree for a particular ACT score. To get a sense for this, we can compute the sample proportion of students who obtained a degree for each ACT score from the data.\n\n# Compute \nprop_grad = grad |&gt;\n  group_by(act, degree) |&gt;\n  summarize(\n    N = n()  # Compute sample sizes by degree for each ACT score\n    ) |&gt;\n  mutate(\n    Prop = N / sum (N) #Compute proportion by degree for each ACT score\n    ) |&gt;\n  filter(degree == \"Yes\") |&gt; # Only use the \"Yes\" responses\n  ungroup() #Makes the resulting tibble regular\n\n# View data\nprop_grad |&gt;\n  print(n = Inf) #Print all the rows\n\n# A tibble: 24 × 4\n     act degree     N  Prop\n   &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n 1    11 Yes        1 0.25 \n 2    13 Yes        4 0.5  \n 3    14 Yes        6 0.5  \n 4    15 Yes       13 0.448\n 5    16 Yes        6 0.24 \n 6    17 Yes       18 0.429\n 7    18 Yes       26 0.531\n 8    19 Yes       50 0.685\n 9    20 Yes       74 0.679\n10    21 Yes       97 0.678\n11    22 Yes      103 0.632\n12    23 Yes      149 0.764\n13    24 Yes      168 0.789\n14    25 Yes      163 0.700\n15    26 Yes      190 0.802\n16    27 Yes      147 0.778\n17    28 Yes      133 0.787\n18    29 Yes      132 0.830\n19    30 Yes       92 0.793\n20    31 Yes       61 0.813\n21    32 Yes       41 0.82 \n22    33 Yes       24 0.828\n23    34 Yes       16 1    \n24    35 Yes        3 0.6  \n\n\nIn general, the proportion of students who obtain their degree is higher for higher SAT values. We can also see this same relationship by plotting the proportion of students who graduate versus ACT score.\n\n# Scatterplot\nggplot(data = prop_grad, aes(x = act, y = Prop)) +\n  geom_point(aes(size = N)) +\n  geom_smooth(data = grad, aes(y = got_degree), method = \"loess\", se = FALSE, \n              color = \"red\", linetype = \"dashed\") +\n  geom_smooth(data = grad, aes(y = got_degree), method = \"lm\", se = FALSE) +\n  theme_bw() +\n  xlab(\"ACT score\") +\n  ylab(\"Proportion of students who obtained a degree\")\n\n\n\n\n\n\n\nFigure 13.3: Proportion of students who graduate conditional on ACT score. Size of the dot is proportional to sample size. The regression smoother (solid, blue line) and loess smoother (dashed, red line) are based on the raw data.\n\n\n\n\n\nThere are a few things to note here:\n\nThe regression line is based on the raw data rather than the proportion data. This is important because the sample sizes differ (e.g., there are far more students who have an ACT score near 25 than near 10).\nBoth the regression line and loess smoother indicate there is a positive relationship between ACT score and the proportion of students who graduate. This suggests that higher ACT scores are associated with higher proportions of students who graduate.\nThe loess smoother suggests there may be a curvilinear relationship between ACT score and the proportion of students who obtain their degree.\n\n\nFYI\nBecause the regression is about the proportion of students who obtain their degree, the scatterplot we show should also indicate the proportion of students who graduate. Thus, when the outcome is categorical, we tend not to look at a scatterplot of the raw data in practice.\n\n# Scatterplot\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() +\n  xlab(\"ACT score\") +\n  ylab(\"Graduated\")\n\n\n\n\n\n\n\nFigure 13.4: Scatterplot of whether a student graduated versus ACT score. The regression smoother (blue, solid line) and loess smoother (red, dashed line) shows the positive relationship between ACT score and the proportion of students who obtain their degree.\n\n\n\n\n\nSince the raw data can only have outcome values of 0 or 1, we don’t see many of the observations (this is called overplotting). More importantly, the regression line and loess smoother, which indicates the proportion of students who graduate is using a different scale (values between 0 and 1) than the raw data (only values of 0 or 1).Because of this scale incompatibility, this plot is generally not provided in practice.\n\nWe can also see the positive relationship between ACT score and the proportion of students who obtain their degree by examining the correlation matrix between the two variables. The correlation coefficient suggests a weak, positive relationship, but we interpret this with caution since the relationship may not be linear (as suggested by the loess smoother).\n\n# Correlation\ngrad |&gt; \n  select(got_degree, act) |&gt; \n  correlate()\n\n# A tibble: 2 × 3\n  term       got_degree    act\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1 got_degree     NA      0.195\n2 act             0.195 NA",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Probability Model</span>"
    ]
  },
  {
    "objectID": "04-01-linear-probability-model.html#fitting-the-linear-probability-proportion-model",
    "href": "04-01-linear-probability-model.html#fitting-the-linear-probability-proportion-model",
    "title": "13  Linear Probability Model",
    "section": "13.2 Fitting the Linear Probability (Proportion) Model",
    "text": "13.2 Fitting the Linear Probability (Proportion) Model\nWe can fit the linear model shown by the regression line in the plot using the lm() function as we always have. The name of this model when we have a dichotomous outcome in a linear model, is the linear probability (proportion) model.\n\n# Fit the model\nlm.1 = lm(got_degree ~ 1 + act, data = grad)\n\n# Model-level- output\nglance(lm.1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0381        0.0377 0.434      92.7 1.50e-21     1 -1370. 2746. 2764.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nDifferences in ACT score account for 3.8% of the variation in graduation.\n\n# Coefficient-level- output\ntidy(lm.1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   0.217    0.0543       3.99 6.79e- 5\n2 act           0.0208   0.00216      9.63 1.50e-21\n\n\nThe fitted model is:\n\\[\n\\hat{\\pi}_i = 0.22 + 0.02(\\mathrm{ACT~Score}_i)\n\\]\nwhere \\(\\hat{\\pi}_i\\) is the predicted proportion of students who graduate. (Note that we use \\(\\hat{\\pi}_i\\) rather than \\(Y_i\\) since the outcome is now a proportion rather than the raw outcome value.) Interpreting the coefficients,\n\nOn average, 0.22 of students having an ACT score of 0 are predicted to graduate. (Extrapolation)\nEach one-point difference in ACT score is associated with an additional 0.02 predicted improvement in the proportion of students graduating, on average.\n\nLet’s examine the model assumptions.\n\n# Examine residual plots\nresidual_plots(lm.1)\n\n\n\n\n\n\n\nFigure 13.5: LEFT: Density plot of the standardized residuals from the linear probability model. The confidence envelope shows where we might expect the density to be if the population of residuals were normally distributed. RIGHT: Scatterplot of the standardized residuals versus the fitted values from the linear probability model. The loess smoother along with the confidence envelope is also displayed.\n\n\n\n\n\nIt is clear that the assumptions associated with linear regression are violated. First off, the residuals are not normally distributed. They are in fact, bimodal. The scatterplot of the residuals versus the fitted values also indicates violation of the linearity assumption, as the average residual at each fitted value is not zero. The only assumption that seems tenable is homoskedasticity.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Probability Model</span>"
    ]
  },
  {
    "objectID": "04-01-linear-probability-model.html#understanding-the-residuals-from-the-linear-probability-model",
    "href": "04-01-linear-probability-model.html#understanding-the-residuals-from-the-linear-probability-model",
    "title": "13  Linear Probability Model",
    "section": "13.3 Understanding the Residuals from the Linear Probability Model",
    "text": "13.3 Understanding the Residuals from the Linear Probability Model\nLook closely at the scatterplot of the residuals versus the fitted values. At each fitted value, there are only two residual values. Why is this? Recall that residuals are computed as \\(\\epsilon_i=Y_i - \\hat{Y}_i\\). Now, remember that \\(Y_i\\) can only be one of two values, 0 or 1. Also remember that in the linear probability model \\(\\hat{Y}_i=\\hat{\\pi}_i\\). Thus, for \\(Y=0\\),\n\\[\n\\begin{split}\n\\epsilon_i &= 0 - \\hat{Y}_i \\\\\n&= - \\hat{\\pi}_i\n\\end{split}\n\\]\nAnd, if \\(Y=1\\),\n\\[\n\\begin{split}\n\\epsilon_i &= 1 - \\hat{Y}_i \\\\\n&= 1 - \\hat{\\pi}_i\n\\end{split}\n\\]\nThis means that the residual computed using a particular fitted value can only take on one of two values: \\(- \\hat{\\pi}_i\\) or \\(1 - \\hat{\\pi}_i\\). Likewise, the standardized residuals can only take on two values for each fitted value.That is why in the plot of the standardized residuals versus the fitted values we do not see any scatter; there are only two possibilities for the standardized residuals at each fitted value.\nFurthermore, the residual (and therefore the standardized residuals) are always the same distance apart for each fitted value, and get smaller for higher fitted values. This is why we see the two parallel strips (having negative slopes) in this plot.\nIf we plot a distribution of these residuals, we will get two humps; one centered based on the distribution of \\(1 - \\hat{\\pi}_i\\) residuals and one centered based on the \\(-\\hat{\\pi}_i\\) residuals. This is why we see the bimodal distribution when we look at the density plot of the standardized residuals.\n\nWARNING\nFitting a linear model when the outcome is dichotomous will result in gross violations of the distributional assumptions. In the next set of notes we will examine more appropriate models for modeling variation in dichotomous categorical outcomes.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Probability Model</span>"
    ]
  },
  {
    "objectID": "04-01-linear-probability-model.html#footnotes",
    "href": "04-01-linear-probability-model.html#footnotes",
    "title": "13  Linear Probability Model",
    "section": "",
    "text": "We could also have used the original grad data set in our data= argument of ggplot(). In that case we would not use stat=\"Identity\") in the geom_bar() layer since the counts would be computed for us from the raw data.↩︎\nNote that \\n creates a newline to put the proportions on a separate line from the counts.↩︎",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Probability Model</span>"
    ]
  },
  {
    "objectID": "04-02-logistic-regression.html",
    "href": "04-02-logistic-regression.html",
    "title": "14  Logistic Regression",
    "section": "",
    "text": "14.1 Preparation\nIn this chapterr, you will learn how to use logistic regression models to model dichotomous categorical outcome variables (e.g., dummy coded outcome). We will use data from the file graduation.csv to explore predictors of college graduation.\n# Load libraries\nlibrary(AICcmodavg)\nlibrary(broom)\nlibrary(corrr)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(performance)\nlibrary(tidyverse)\n\n# Read in data and create dummy variable for outcome\ngrad = read_csv(file = \"https://raw.githubusercontent.com/zief0002/fluffy-ants/main/data/graduation.csv\") |&gt;\n  mutate(\n    got_degree = if_else(degree == \"Yes\", 1, 0)\n  )\n\n# View data\ngrad\n\n# A tibble: 2,344 × 8\n   student degree   act scholarship ap_courses first_gen non_traditional\n     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;          \n 1       1 Yes       21         0            0 No        No             \n 2       2 Yes       19         0            0 No        No             \n 3       3 Yes       27         0            0 Yes       No             \n 4       4 Yes       25         0.5          0 Yes       No             \n 5       5 No        28         0           17 Yes       No             \n 6       6 Yes       21         0            0 No        Yes            \n 7       7 Yes       27         0            8 Yes       No             \n 8       8 No        20         0            0 No        No             \n 9       9 Yes       26         0            0 Yes       No             \n10      10 Yes       25         0            4 Yes       No             \n# ℹ 2,334 more rows\n# ℹ 1 more variable: got_degree &lt;dbl&gt;\nWe will also examine the empirical proportions of students who obtain a degree at different ACT scores to remind us about the relationship we will be modeling.\n# Obtain the proportion who obtain a degree for each ACT score\nprop_grad = grad |&gt; \n  group_by(act, degree) |&gt; \n  summarize(N = n()) |&gt; \n  mutate(\n    Prop = N / sum (N)\n    ) |&gt;\n  ungroup() |&gt;\n  filter(degree == \"Yes\")\n\n# Scatterplot\nggplot(data = prop_grad, aes(x = act, y = Prop)) +\n  geom_point(aes(size = N)) +\n  geom_smooth(aes(weight = N), method = \"loess\", se = FALSE) +\n  theme_bw() +\n  xlab(\"ACT score\") +\n  ylab(\"Proportion who obtained a degree\")\n\n\n\n\n\n\n\nFigure 14.1: Proportion of students who obtain a degree conditional on ACT score. Size of the dot is proportional to sample size. The loess smoother is based on the raw data.\nBecause the sample sizes differs across ACT scores, we need to account for that by weighting the loess smoother. To do this we include aes(weight=) in the geom_smooth() layer.\nIn the last set of notes, we saw that using the linear probability model leads to direct violations of the linear model’s assumptions. If that isn’t problematic enough, it is possible to run into severe issues when we make predictions. For example, given the constant effect of X in these models it is possible to have an X value that results in a predicted proportion that is either greater than 1 or less than 0. This is a problem since proportions are constrained to the range of \\(\\left[0,~1\\right]\\).",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-02-logistic-regression.html#preparation",
    "href": "04-02-logistic-regression.html#preparation",
    "title": "14  Logistic Regression",
    "section": "",
    "text": "CSV File\nData Codebook",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-02-logistic-regression.html#alternative-models-to-the-linear-probability-model",
    "href": "04-02-logistic-regression.html#alternative-models-to-the-linear-probability-model",
    "title": "14  Logistic Regression",
    "section": "14.2 Alternative Models to the Linear Probability Model",
    "text": "14.2 Alternative Models to the Linear Probability Model\nMany of the non-linear models that are typically used to model dichotomous outcome data are “S”-shaped models. Below is a plot of one-such “S”-shaped model.\n\n\n\n\n\n\n\n\nFigure 14.2: An ‘S’-shaped model.\n\n\n\n\n\nThe non-linear “S”-shaped model has many attractive features. First, the predicted proportions (\\(\\pi_i\\)) are bounded between 0 and 1. Furthermore, as X gets smaller, \\(\\pi_i\\) approaches 0 at a slower rate. Similarly, as X gets larger, \\(\\pi_i\\) approaches 1 at a slower rate. Lastly, this model curve is monotonic; smaller values of X are associated with smaller predicted proportions and larger values of X are associated with larger predicted proportions. (Or, if the “S” were backwards, smaller values of X are associated with larger predicted proportions and larger values of X would be associated with smaller predicted proportions). The key is that there are no bends in the curve; it is always growing or always decreasing.\nIn our student example, the empirical data maps well to this curve. Higher ACT scores are associated with a higher proportion of students who obtain a degree (monotonic). The effect of ACT, however, is not constant, and seems to diminish at higher ACT scores. Lastly, we want to bound the proportion who obtain a degree at every ACT score to lie between 0 and 1.\n\n\n14.2.1 A Transformation to Fit the “S”-shaped Curve\nWe need to identify a mathematical function that relates X to Y in such a way that we produce this “S”-shaped curve. To do this we apply a transformation function, call it \\(\\Lambda\\) (Lambda), that fits the criteria for such a function (monotonic, nonlinear, maps to \\([0,~1]\\) space). There are several mathematical functions that do this. One commonly used function that meets these specifications is the logistic function. Mathematically, the logistic function is:\n\\[\n\\Lambda(x) = \\frac{1}{1 + e^{-x}}\n\\]\nwhere x is the value fed into the logistic function. For example, to logistically transform \\(x=3\\), we use\n\\[\n\\begin{split}\n\\Lambda(3) &= \\frac{1}{1 + e^{-3}} \\\\[1ex]\n&= 0.953\n\\end{split}\n\\]\nBelow we show how to transform many such values using R.\n\n# Create w values and transformed values\nexample = tibble(\n  x = seq(from = -4, to = 4, by = 0.01)  # Set up values\n  ) |&gt;\n  mutate(\n    Lambda = 1 / (1 + exp(-x))  # Transform using logistic function\n  )\n\n# View data\nexample\n\n# A tibble: 801 × 2\n       x Lambda\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1 -4    0.0180\n 2 -3.99 0.0182\n 3 -3.98 0.0183\n 4 -3.97 0.0185\n 5 -3.96 0.0187\n 6 -3.95 0.0189\n 7 -3.94 0.0191\n 8 -3.93 0.0193\n 9 -3.92 0.0195\n10 -3.91 0.0196\n# ℹ 791 more rows\n\n# Plot the results\nggplot(data = example, aes(x = x, y = Lambda)) +\n  geom_line() +\n  theme_light()\n\n\n\n\n\n\n\nFigure 14.3: Plot of the logistically transformed values for a sequence of values from -4 to 4.\n\n\n\n\n\nYou can see that by using this transformation we get a monotonic “S”-shaped curve. Now try substituting a really large value of x into the function. This gives an asymptote at 1. Also substitute a really “large”” negative value in for x. This gives an asymptote at 0. So this function also bounds the output between 0 and 1.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-02-logistic-regression.html#using-the-logistic-function-in-regression",
    "href": "04-02-logistic-regression.html#using-the-logistic-function-in-regression",
    "title": "14  Logistic Regression",
    "section": "14.3 Using the Logistic Function in Regression",
    "text": "14.3 Using the Logistic Function in Regression\nHow does this work in a regression? There, we are relating the predicted values, that is, the \\(\\beta_0 + \\beta_1(X_i)\\) values, to the probability of Y. So we need to apply the “S”-shaped logistic transformation to convert those predicted values:\n\\[\n\\pi_i = \\Lambda\\bigg[\\beta_0 + \\beta_1(X_i)\\bigg]\n\\]\nSubstituting the predicted equation into the logistic transformation, we get:\n\\[\n\\pi_i = \\frac{1}{1 + e^{-\\big[\\beta_0 + \\beta_1(X_i)\\big]}}\n\\]\nSince we took a linear model (\\(\\beta_0 + \\beta_1(X_i)\\)) and applied a logistic transformation, we refer to the resulting model as the linear logistic model or more simply, the logistic model.\n\n\n14.3.1 Re-Expressing a Logistic Transformation\nThe logistic model expresses the proportion of 1s (\\(\\pi_i\\)) as a function of the predictor \\(X\\). It can be mathematically expressed as:\n\\[\n\\pi_i = \\frac{1}{1 + e^{-\\big[\\beta_0 + \\beta_1(X_i)\\big]}}\n\\]\nWe can re-express this using algebra and rules of logarithms.\n\\[\n\\begin{split}\n\\pi_i &= \\frac{1}{1 + e^{-\\big[\\beta_0 + \\beta_1(X_i)\\big]}} \\\\[2ex]\n\\pi_i \\times (1 + e^{-\\big[\\beta_0 + \\beta_1(X_i)\\big]} ) &= 1 \\\\[2ex]\n\\pi_i + \\pi_i(e^{-\\big[\\beta_0 + \\beta_1(X_i)\\big]}) &= 1 \\\\[2ex]\n\\pi_i(e^{-\\big[\\beta_0 + \\beta_1(X_i)\\big]}) &= 1 - \\pi_i \\\\[2ex]\ne^{-\\big[\\beta_0 + \\beta_1(X_i)\\big]} &= \\frac{1 - \\pi_i}{\\pi_i} \\\\[2ex]\ne^{\\big[\\beta_0 + \\beta_1(X_i)\\big]} &= \\frac{\\pi_i}{1 - \\pi_i} \\\\[2ex]\n\\ln \\bigg(e^{\\big[\\beta_0 + \\beta_1(X_i)\\big]}\\bigg) &= \\ln \\bigg( \\frac{\\pi_i}{1 - \\pi_i} \\bigg) \\\\[2ex]\n\\beta_0 + \\beta_1(X_i) &= \\ln \\bigg( \\frac{\\pi_i}{1 - \\pi_i}\\bigg)\n\\end{split}\n\\]\nOr,\n\\[\n\\ln \\bigg( \\frac{\\pi_i}{1 - \\pi_i}\\bigg) = \\beta_0 + \\beta_1(X_i)\n\\]\n\nFYI\nThe logistic model expresses the natural logarithm of \\(\\frac{\\pi_i}{1 - \\pi_i}\\) as a linear function of X. Note that there is no error term on this model. This is because the model is for the mean structure only (the proportions), we are not modeling the actual \\(Y_i\\) values (i.e., the 0s and 1s) with the logistic regression model.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-02-logistic-regression.html#odds-a-ratio-of-probabilities",
    "href": "04-02-logistic-regression.html#odds-a-ratio-of-probabilities",
    "title": "14  Logistic Regression",
    "section": "14.4 Odds: A Ratio of Probabilities",
    "text": "14.4 Odds: A Ratio of Probabilities\nThe ratio that we are taking the logarithm of, \\(\\frac{\\pi_i}{1 - \\pi_i}\\), is referred to as odds. Odds are the ratio of two probabilities. Namely the chance an event occurs (\\(\\pi_i\\)) versus the chance that same event does not occur (\\(1 - \\pi_i\\)). As such, it gives the relative probability of that event occurring. To understand this better, we will look at a couple examples.\nLet’s assume that the probability of getting an “A” in a course is 0.7. Then we know the probability of NOT getting an “A” in that course is 0.3. The odds of getting an “A” are then\n\\[\n\\mathrm{Odds} = \\frac{0.7}{0.3} = 2.33\n\\]\nThe relative probability of getting an “A” is 2.33. That is, the probability of getting an “A” in the class is 2.33 times more likely than NOT getting an “A” in the class.\nAs another example, Fivethirtyeight.com predicted on March 08, 2022 that the probability the Minnesota Wild would win the Stanley Cup was 0.02. The odds of the Minnesota Wild winning the Stanley Cup is then:\n\\[\n\\mathrm{Odds} = \\frac{0.02}{0.98} = 0.0204\n\\]\nThe probability that the Minnesota Wild win the Stanley Cup is 0.0204 times as likely them NOT winning the Stanley Cup. (Odds less than 1 indicate that is is more likely for an event NOT to occur than to occur. Invert the fraction to compute how much more like the event is not to occur.)\n\nFYI\nIn the logistic model, we are predicting the log-odds (also referred to as the logit. When we get these values, we typically transform the logits to odds by inverting the log-transformation (take e to that power.)",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-02-logistic-regression.html#relationship-between-x-and-different-transformations",
    "href": "04-02-logistic-regression.html#relationship-between-x-and-different-transformations",
    "title": "14  Logistic Regression",
    "section": "14.5 Relationship between X and Different Transformations",
    "text": "14.5 Relationship between X and Different Transformations\nOdds are a re-expression of the probabilities, and similarly log-odds (or logits) is a transformation of the odds onto the log scale. While the logistic regression is performed on the logits of Y, like any model where we have transformed variables to work in the model, we can back-transform when we present the results. In logistic regression we can present our results in logits, odds, or probabilities. Because of the transformations, each of these will have a different relationship with the predictor.\nFor example, let’s re-examine the plot of the “S”-shaped example data. This depicted the relationship between a predictor X and the probability of Y. We could also depict the relationship between X and the odds of Y, or the relationship between X and the log-odds of Y.\n\n# Create odds and logit values\nexample = example |&gt;\n  mutate(\n    Odds = Lambda / (1 - Lambda),  # Transform to odds\n    Logits = log(Odds)\n  )\n\n# View data\nexample\n\n# A tibble: 801 × 4\n       x Lambda   Odds Logits\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -4    0.0180 0.0183  -4   \n 2 -3.99 0.0182 0.0185  -3.99\n 3 -3.98 0.0183 0.0187  -3.98\n 4 -3.97 0.0185 0.0189  -3.97\n 5 -3.96 0.0187 0.0191  -3.96\n 6 -3.95 0.0189 0.0193  -3.95\n 7 -3.94 0.0191 0.0194  -3.94\n 8 -3.93 0.0193 0.0196  -3.93\n 9 -3.92 0.0195 0.0198  -3.92\n10 -3.91 0.0196 0.0200  -3.91\n# ℹ 791 more rows\n\n\n\n\nCode\n# S-shaped curve (probabilities)\np1 = ggplot(data = example, aes(x = x, y = Lambda)) +\n  geom_line() +\n  theme_light() +\n  ylab(\"Probabilities\")\n\n# Exponential growth curve (odds)\np2 = ggplot(data = example, aes(x = x, y = Odds)) +\n  geom_line() +\n  theme_light() +\n  ylab(\"Odds\")\n\n# Linear (log-odds)\np3 = ggplot(data = example, aes(x = x, y = Logits)) +\n  geom_line() +\n  theme_light() +\n  ylab(\"Logits\")\n\n# Output plots\np1 | p2 | p3\n\n\n\n\n\n\n\n\nFigure 14.4: Plot showing the relationship between the probability of Y versus X (LEFT), the odds of Y versus X (CENTER), and the log-odds of Y versus X (RIGHT).\n\n\n\n\n\nNote that by transforming the probabilities into odds, we have changed the “S”-shaped curve into a classic exponential growth curve. (The Rule-of-the-Bulge suggests we can “fix” this by applying a downward power transformation on Y.) Then, by log-transforming the Y-values (the odds in this case), the resulting relationship is now linear. Thus an “S”-shaped curve can be “linearized” by transforming probabilities to logits (log-odds). Mathematically, this is equivalent to applying a logistic transformation to the predicted values.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-02-logistic-regression.html#binomially-distributed-errors",
    "href": "04-02-logistic-regression.html#binomially-distributed-errors",
    "title": "14  Logistic Regression",
    "section": "14.6 Binomially Distributed Errors",
    "text": "14.6 Binomially Distributed Errors\nThe logistic transformation fixed two problems: (1) the non-linearity in the conditional mean function, and (2) bounding any predicted values between 0 and 1. However, just fitting this transformation does not fix the problem of non-normality. Remember from the previous notes we learned that at each \\(X_i\\) there were only two potential values for \\(Y_i\\); 0 or 1. Rather than use a normal (or Gaussian) distribution to model the conditional distribution of \\(Y_i\\), we will use the binomial distribution.\nThe binomial distribution is a discrete probability distribution that gives the probability of obtaining exactly k successes out of n Bernoulli trials (where the result of each Bernoulli trial is true with probability \\(\\pi\\) and false with probability \\(1-\\pi\\)). This is appropriate since at each ACT value of there are \\(n_i\\) students, k of which obtained their degree (\\(Y=1\\)).",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-02-logistic-regression.html#fitting-the-binomial-logistic-model-in-r",
    "href": "04-02-logistic-regression.html#fitting-the-binomial-logistic-model-in-r",
    "title": "14  Logistic Regression",
    "section": "14.7 Fitting the Binomial Logistic Model in R",
    "text": "14.7 Fitting the Binomial Logistic Model in R\nTo fit a logistic regression model with binomial errors, we use the glm() function.1 The syntax to fit the logistic model using glm() is:\n\\[\n\\mathtt{glm(} \\mathrm{y} \\sim \\mathrm{1~+~x,~}\\mathtt{data=}~\\mathrm{dataframe,~}\\mathtt{family~=~binomial(link~=~\"logit\")}\n\\]\nThe formula depicting the model and the data= arguments are specified in the same manner as in the lm() function. We also need to specify the distribution for the conditional \\(Y_i\\) values (binomial) and the link function (logit) via the family= argument.\nFor our example,\n\n# Fit logistic model\nglm.1 = glm(got_degree ~ 1 + act, data = grad, family = binomial(link = \"logit\"))\n\nThe coefficients of the model can be printed using coef().\n\n# Get coefficients\ncoef(glm.1)\n\n(Intercept)         act \n -1.6108878   0.1075389 \n\n\nBased on this output, the fitted equation for the model is:\n\\[\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) = -1.61 + 0.11(\\mathrm{ACT~Score}_i)\n\\]\n\nINTERPRETATION\nWe interpret the coefficients in the same manner as we interpret coefficients from a linear model, with the caveat that the outcome is now in log-odds (or logits):\n\nThe predicted log-odds of obtaining a degree for students with an ACT score of 0 are \\(-1.61\\).\nEach one-point difference in ACT score is associated with a difference of 0.11 in the predicted log-odds of obtaining a degree, on average.\n\n\n\n\n14.7.1 Back-Transforming to Odds\nFor better interpretations, we can back-transform log-odds to odds. This is typically a better metric for interpretation of the coefficients. To back-transform to odds, we exponentiate both sides of the fitted equation and use the rules of exponents to simplify:\n\\[\n\\begin{split}\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) &= -1.61 + 0.11(\\mathrm{ACT~Score}_i) \\\\[4ex]\ne^{\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg)} &= e^{-1.61 + 0.11(\\mathrm{ACT~Score}_i)} \\\\[2ex]\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{-1.61} \\times e^{0.11(\\mathrm{ACT~Score}_i)}\n\\end{split}\n\\]\nWhen ACT score = 0, the predicted odds of obtaining a degree are\n\\[\n\\begin{split}\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{-1.61} \\times e^{0.11(0)} \\\\\n&= e^{-1.61} \\times 1 \\\\\n&= e^{-1.61} \\\\\n&= 0.2\n\\end{split}\n\\]\nThe odds of obtaining a degree for students with an ACT score of 0 are 0.2. That is, for these students, the probability of obtaining a degree is 0.2 times that of not obtaining a degree (Although it is extrapolating, it is far more likely these students will not obtain their degree!)\nTo interpret the effect of ACT on the odds of obtaining a degree, we will compare the odds of obtaining a degree for students that have ACT score that differ by one point. Say ACT = 0 and ACT = 1.\nWe already know the predicted odds for students with ACT = 0, namely \\(e^{-1.61}\\). For students with an ACT of 1, their predicted odds of obtaining a degree are:\n\\[\n\\begin{split}\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{-1.61} \\times e^{0.11(1)} \\\\\n&= e^{-1.61} \\times e^{0.11} \\\\\n\\end{split}\n\\]\nThese students odds of obtaining a degree are \\(e^{0.11}\\) times greater than students with an ACT score of 0. Moreover, this increase in the odds, on average, is the case for every one-point difference in ACT score. In general,\n\nThe predicted odds for \\(X=0\\) are \\(e^{\\hat\\beta_0}\\).\nEach one-unit difference in \\(X\\) is associated with a \\(e^{\\hat\\beta_1}\\) times increase (decrease) in the odds.\n\nWe can obtain these values in R by using the coef() function to obtain the fitted model’s coefficients and then exponentiating them using the exp() function.\n\n# Exponentiate the coefficients\nexp(coef(glm.1))\n\n(Intercept)         act \n  0.1997102   1.1135342 \n\n\n\nINTERPRETATION\nFrom these values, we interpret the coefficients in the odds metric as:\n\nThe predicted odds of obtaining a degree for students with an ACT score of 0 are 0.20.\nEach one-point difference in ACT score is associated with 1.11 times greater odds of obtaining a degree.\n\n\nTo even further understand and interpret the fitted model, we can plot the predicted odds of obtaining a degree for a range of ACT scores. Recall, the general fitted equation for the logistic regression model is written as:\n\\[\n\\ln\\bigg[\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg] = \\hat\\beta_0 + \\hat\\beta_1(x_i)\n\\]\nWe need to predict odds rather than log-odds on the left-hand side of the equation. To do this we exponentiate both sides of the equation:\n\\[\n\\begin{split}\ne^{\\ln\\bigg[\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg]} &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} \\\\[1em]\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}\n\\end{split}\n\\]\nWe include the right-side of this in the argument fun= of the geom_function() layer, substituting in the values for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\). Below we plot the results from our fitted logistic model.\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {exp(-1.611 + 0.108*x)}\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted odds of obtaining a degree\")\n\n\n\n\n\n\n\nFigure 14.5: Predicted odds of obtaining a degree as a function of ACT score.\n\n\n\n\n\nThe monotonic increase in the curve indicates the positive effect of ACT score on the odds of obtaining a degree. The exponential growth curve indicates that students with higher ACT scores have increasingly higher odds of obtaining a degree.\n\n\n\n14.7.2 Back-Transforming to Probability\nWe can also back-transform from odds to probability. To do this, we will again start with the logistic fitted equation and use algebra to isolate the probability of obtaining a degree (\\(\\pi_i\\)) on the left-hand side of the equation.\n\\[\n\\begin{split}\n\\ln\\bigg[\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg] &= \\hat\\beta_0 + \\hat\\beta_1(x_i) \\\\[1em]\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} \\\\[1em]\n\\hat\\pi_i &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} (1 - \\hat\\pi_i) \\\\[1em]\n\\hat\\pi_i &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} - e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}(\\hat\\pi_i) \\\\[1em]\n\\hat\\pi_i + e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}(\\hat\\pi_i) &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} \\\\[1em]\n\\hat\\pi_i(1 + e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}) &= e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} \\\\[1em]\n\\hat\\pi_i &= \\frac{e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}}{1 + e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}} \\\\[1em]\n\\hat\\pi_i &= \\frac{e^{\\hat Y_i}}{1 + e^{\\hat Y_i}}\n\\end{split}\n\\]\nThat is, to obtain the probability of obtaining a degree, we can transform the fitted values (i.e., the predicted log-odds) from the logistic model. For example, the intercept from the logistic fitted equation, \\(-1.61\\) was the predicted log-odds of obtaining a degree for students with an ACT of 0. To obtain the predicted probability of obtaining a degree for students with an ACT of 0:\n\n# Predicted probability of obtaining a degree; ACT=0\nexp(-1.61) / (1 + exp(-1.61))\n\n[1] 0.1665886\n\n\nFor students with ACT of 0, the predicted probability of obtaining a degree is 0.17.\nThis transformation from log-odds to probability, is non-linear, which means that there is not a clean interpretation of the effects of ACT (i.e., the slope) on the probability of obtaining a degree To understand this effect we can plot the probability of obtaining a degree across the range of ACT scores. To do this, we use geom_function() and input the transformation to probability with the fitted equation:\n\\[\n\\hat\\pi_i = \\frac{e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}}{1 + e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}}\n\\]\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {exp(-1.611 + 0.108*x) / (1 + exp(-1.611 + 0.108*x))}\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted probability of obtaining a degree\") +\n  ylim(0, 1)\n\n\n\n\n\n\n\nFigure 14.6: Predicted probability of obtaining a degree as a function of ACT score.\n\n\n\n\n\nThe effect of ACT on the probability of obtaining a degree follows a monotonic increasing “S”-curve. While there is always an increasing effect of ACT on the probability of obtaining a degree, the magnitude of this effect depends on ACT score. For lower ACT scores there is a larger effect of ACT score on the probability of obtaining a degree than for higher ACT scores.\nOne interesting point on the plot is the ACT score where the probability of obtaining a degree is 0.5. In our example, this is at an ACT score of approximately 15. This implies that students who score less than 15 are more likely to not obtain a degree than to obtain a degree (on average), and those that score higher than 15 are more likely to obtain a degree than not (on average).\n\n\n14.7.2.1 Rough Interpretation of the Slope\nOne rough interpretation that is often used is to divide the slope coefficient by 4 to get an upper bound of the predictive difference in probability of Y per unit increase in X. In our example, each one-point difference in ACT score is associated with, at most, a \\(0.108/4=.027\\) difference in the probability of obtaining a degree, on average.\n\nThe mathematics behind this rough interpretation is based on maximizing the rate-of-change, which is based on setting the first derivative of the logistic function to zero and solving. Recall that the logistic function relates the probabilities to the fitted equation as:\n\\[\n\\hat\\pi_i = \\frac{e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}}{1 + e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}}\n\\]\nThe first derivative, with respect to x is:\n\\[\n\\frac{\\hat\\beta_1e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)}}{\\bigg[e^{\\hat\\beta_0 + \\hat\\beta_1(x_i)} + 1\\bigg]^2}\n\\]\nThis is maximized when \\(\\hat\\beta_0 + \\hat\\beta_1(x_i)=0\\), which means we can substitute 0 into the derivative to determine the maximum rate-of-change:\n\\[\n\\begin{split}\n&= \\frac{\\hat\\beta_1e^{0}}{\\bigg[e^{0} + 1\\bigg]^2} \\\\[1ex]\n&= \\frac{\\hat\\beta_1}{\\bigg[1 + 1\\bigg]^2} \\\\[1ex]\n&= \\frac{\\hat\\beta_1}{4} \\\\[1ex]\n\\end{split}\n\\]\nThus the maximum rate-of-change is the based on the slope coefficient divided by four.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-02-logistic-regression.html#model-level-summaries",
    "href": "04-02-logistic-regression.html#model-level-summaries",
    "title": "14  Logistic Regression",
    "section": "14.8 Model-Level Summaries",
    "text": "14.8 Model-Level Summaries\nThe glance() output for the GLM model also included model-level information. For the model we fitted, the model-level output was:\n\n# Model-level output\nglance(glm.1) |&gt;\n  print(width = Inf)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1         2723.    2343 -1317. 2637. 2649.    2633.        2342  2344\n\n\nThe metric of measuring residual fit is the deviance (remember the deviance was \\(-2 \\times\\) log-likelihood). The value in the null.deviance column is the residual deviance from fitting the intercept-only model. It acts as a baseline to compare other models.\n\n# Fit intercept-only model\nglm.0 = glm(got_degree ~ 1, data = grad, family = binomial(link = \"logit\"))\n\n# Compute deviance\n-2 * logLik(glm.0)[[1]]\n\n[1] 2722.546\n\n\nThe value in the deviance column is the residual deviance from fitting whichever model was fitted, in our case the model that used ACT score as a predictor.\n\n# Compute deviance for glm.1\n-2 * logLik(glm.1)[[1]]\n\n[1] 2633.236\n\n\nRecall that deviance is akin to the sum of squared residuals (SSE) in conventional linear regression; smaller values indicate less error. In our case, the model that includes ACT score as a predictor has less error than the intercept only model; its deviance is 90 less than the intercept-only model.\nThe deviance values or log-likelihoods are often reported in a table of regression results. They are also used to compute differenct information criteria.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-02-logistic-regression.html#evaluating-the-effect-of-act-acore",
    "href": "04-02-logistic-regression.html#evaluating-the-effect-of-act-acore",
    "title": "14  Logistic Regression",
    "section": "14.9 Evaluating the Effect of ACT Acore",
    "text": "14.9 Evaluating the Effect of ACT Acore\nTo evaluate the effect of ACT we compute the AICc values and other metrics in out table of model evidence.\n\naictab(\n  cand.set = list(glm.0, glm.1),\n  modnames = c(\"Intercept-Only\", \"Effect of ACT\")\n)\n\n\nModel selection based on AICc:\n\n               K    AICc Delta_AICc AICcWt Cum.Wt       LL\nEffect of ACT  2 2637.24       0.00      1      1 -1316.62\nIntercept-Only 1 2724.55      87.31      0      1 -1361.27\n\n\nHere, given the data and the candidate set of models, there is overwhelming empirical support to adopt the model that includes ACT score.\n\n\n14.9.1 Assessing Statistical Significance of the Predictor 🤮\nWe can obtain p-values to evaluate statistical significance of predictors. Similar to the regression model, we can obtain p-values under the classical framework and under the likelihood framework for inference. Under the classical framework we use tidy(). The z-values and p-values from tidy() are sometimes referred to as Wald statistics and Wald p-values.\n\n# Classical framework for inference\n# Wald values\ntidy(glm.1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -1.61     0.283      -5.70 1.21e- 8\n2 act            0.108    0.0116      9.25 2.20e-20\n\n\nWe can also use the log-likelihood values to carry out a likelihood ratio test to test the improvement in deviance from the baseline model (using the likelihood framework). Since the intercept-only model is nested in the model that includes ACT as a predictor, we can use a Likelihood Ratio Test to examine this. To do so, we use the anova() function with the added argument test=\"LRT\".\n\n# Likelihood framework for inference\nanova(glm.0, glm.1, test = \"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: got_degree ~ 1\nModel 2: got_degree ~ 1 + act\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1      2343     2722.6                          \n2      2342     2633.2  1    89.31 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe null hypothesis of this test is that there is NO improvement in the deviance. The results of this test, \\(\\chi^2(1)=89.3\\), \\(p&lt;.001\\), indicate that the observed difference of 89.3 is more than we would expect if the null hypothesis was true. In practice, this implies that the more complex model has significantly less error than the intercept-only model and should be adopted.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-02-logistic-regression.html#pseudo-r-squared-values",
    "href": "04-02-logistic-regression.html#pseudo-r-squared-values",
    "title": "14  Logistic Regression",
    "section": "14.10 Pseudo R-Squared Values",
    "text": "14.10 Pseudo R-Squared Values\nWhen we fitted linear regression models, we computed an \\(R^2\\) value for the model as a summary measure of the model. This value quantified the amount of variation in the outcome that was explainable by the predictors included in the model. To compute this, we computed:\n\\[\nR^2 = \\frac{\\mathrm{SSE}_{\\mathrm{Baseline}} - \\mathrm{SSE}_{\\mathrm{Model}}}{\\mathrm{SSE}_{\\mathrm{Baseline}}}\n\\]\nwhere the baseline model was the intercept-only model. This measured the proportion of reduction in the residuals from the baseline to the fitted model.\nWith logistic models, there is no sum of squared error, so we cannot compute an \\(R^2\\) value. However, the residual deviance is a similar measure to the sum of squared error. We can substitute the deviance in for SSE in our computation,\n\\[\n\\mathrm{Pseudo}\\mbox{-}R^2 = \\frac{\\mathrm{Deviance}_{\\mathrm{Baseline}} - \\mathrm{Deviance}_{\\mathrm{Model}}}{\\mathrm{Deviance}_{\\mathrm{Baseline}}}\n\\]\nThis measure is referred to as a pseudo-\\(R^2\\) value. To compute this measure for our example:\n\n# Baseline residual deviance: 2722.6\n# Model residual deviance: 2633.2\n\n# Compute pseudo R-squared\n(2722.6 - 2633.2) / 2722.6\n\n[1] 0.03283626\n\n\nInterpreting pseudo \\(R^2\\) values are somewhat problematic. A naive interpretation is that differences in ACT scores explains 3.28% of the variation in graduation status. However, this interpretation is a bit sketchy. Pseudo \\(R^2\\) values mimic \\(R^2\\) values in that they are generally on a similar scale, ranging from 0 to 1 (though remember pseudo \\(R^2\\) values can be negative). Moreover, higher pseudo \\(R^2\\) values, like \\(R^2\\) values, indicate better model fit. So while I wouldn’t offer the earlier interpretation of the value of 0.0328 in a paper, the value close to 0 does suggest that ACT scores are not perhaps incredibly predictive of the log-odds (or odds or probability) of obtaining a degree.\n\nFYI In logistic regression, several other pseudo \\(R^2\\) values have also been proposed. See https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/ for more information.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-02-logistic-regression.html#assumptions-of-the-logistic-model",
    "href": "04-02-logistic-regression.html#assumptions-of-the-logistic-model",
    "title": "14  Logistic Regression",
    "section": "14.11 Assumptions of the Logistic Model",
    "text": "14.11 Assumptions of the Logistic Model\nThe assumptions for the logistic model are quite different than those for the linear model. The three major assumptions are:\n\nThe outcome is binary (only two possible outcomes).\nThe observations (or residuals) are independent.\nThere is a linear relationship between the predictors and the logit of the outcome. (Or that the average residual from the fitted logistic model is 0 for all fitted values.)\n\nUnlike with linear regression, we have no assumption that the residuals are normally distributed, nor that there is homoskedasticity. In our example, Assumption 1 is met since the only two values of the outcome were “obtained a degree” or “did not obtain a degree”. We need to use the same logic we used in linear regression to verify the independence assumption. Here it seems tenable, since knowing whether one student obtained a degree, does not give us information about whether another student obtains a degree.\nTo evaluate the third assumption, we will again plot the residuals versus the fitted values and again look to ensure the \\(Y=0\\) line is encompassed in the confidence envelope for the loess smoother. However, as we saw earlier, plots of the raw and standardized residuals from a logistic regression are not that useful. Instead we will create a binned residual plot. Binned residuals are obtained by ordering all the observations by the fitted values, separating them into g roughly equal bins, and calculating the average residual value in each bin. The binned residual plot is then created by plotting the average residuals in each bin versus the average fitted value in each bin.\nTo obtain the average fitted values and average residuals in each bin, we will use the binned_residuals() function from the {performance} package. The print() function will then be used to create the binned residual plot.\n\n# Obtain average fitted values and average residuals\nout.1 = binned_residuals(glm.1)\n\n# View binned residuals\nas.data.frame(out.1)\n\n                xbar         ybar   n      x.lo      x.hi        se\nconf_int   0.4773158 -0.092349642  54 0.3946137 0.5005491 0.3142217\nconf_int1  0.5441433 -0.409240401  67 0.5274063 0.5541059 0.2716741\nconf_int2  0.5804968 -0.065295207  49 0.5804968 0.5804968 0.3333386\nconf_int3  0.6064352  0.254763834  73 0.6064352 0.6064352 0.2538567\nconf_int4  0.6317871  0.196713174 109 0.6317871 0.6317871 0.2088610\nconf_int5  0.6564311  0.152169899 143 0.6564311 0.6564311 0.1828020\nconf_int6  0.6802604 -0.001183704 163 0.6802604 0.6802604 0.1773462\nconf_int7  0.7031843  0.273568198 195 0.7031843 0.7031843 0.1432526\nconf_int8  0.7251288  0.292833498 213 0.7251288 0.7251288 0.1323670\nconf_int9  0.7460368  0.038106405 233 0.7460368 0.7460368 0.1428258\nconf_int10 0.7658680  0.247625181 237 0.7658680 0.7658680 0.1238422\nconf_int11 0.7845977  0.152357342 189 0.7845977 0.7845977 0.1455285\nconf_int12 0.8022160  0.138971890 169 0.8022160 0.8022160 0.1525677\nconf_int13 0.8187263  0.211235395 159 0.8187263 0.8187263 0.1452254\nconf_int14 0.8341432  0.085452419 116 0.8341432 0.8341432 0.1849288\nconf_int15 0.8484916  0.103580947  75 0.8484916 0.8484916 0.2233596\nconf_int16 0.8618044  0.089109799  50 0.8618044 0.8618044 0.2726814\nconf_int17 0.8741210  0.078271252  29 0.8741210 0.8741210 0.3574272\nconf_int18 0.8879763  0.240126242  21 0.8854857 0.8959465 0.3366564\n                 CI_low     CI_high group\nconf_int   -0.230751241  0.04605196   yes\nconf_int1  -0.529241205 -0.28923960    no\nconf_int2  -0.211260477  0.08067006   yes\nconf_int3   0.143218379  0.36630929    no\nconf_int4   0.105486192  0.28794016   yes\nconf_int5   0.072776878  0.23156292   yes\nconf_int6  -0.077710885  0.07534348   yes\nconf_int7   0.211745203  0.33539119    no\nconf_int8   0.235925500  0.34974150    no\nconf_int9  -0.022604005  0.09881681   yes\nconf_int10  0.194957304  0.30029306    no\nconf_int11  0.090769880  0.21394480    no\nconf_int12  0.074659052  0.20328473   yes\nconf_int13  0.150047581  0.27242321    no\nconf_int14  0.008085150  0.16281969   yes\nconf_int15  0.009914453  0.19724744   yes\nconf_int16 -0.025193868  0.20341347   yes\nconf_int17 -0.071374483  0.22791699   yes\nconf_int18  0.094113698  0.38613879   yes\n\n\n\n# Residual plot\n# This will also require you to install the {see} package\nplot(out.1)\n\n\n\n\n\n\n\nFigure 14.7: Binned residuals versus the binned fitted values for the model that includes the linear effect of ACT to predict the log-odds of obtaining a degree.\n\n\n\n\n\nGood fit to the assumption of linearity is indicated by most of the residuals being encompassed within the error bounds of the \\(Y=0\\) line. We can see some misfit in the binned residual plot. The warning message produced from the binned_residuals() function also indicates that we may have some misfits given that only 89% of the residuals are within the error bounds.\nIt is unclear from this plot what the misfit is due to. It may be because the relationship between ACT and the log-odds of obtaining a degree is non-linear. It may be because there are other predictors (main effects or interactions) that we are missing. (We will explore this in the next set of notes.)",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-02-logistic-regression.html#footnotes",
    "href": "04-02-logistic-regression.html#footnotes",
    "title": "14  Logistic Regression",
    "section": "",
    "text": "The logistic regression model is from a family of models referred to as Generalized Linear Regression models. The General Linear Model (i.e., fixed-effects regression model) is also a member of the Generalized Linear Model family.↩︎",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-03-more-logistic-regression.html",
    "href": "04-03-more-logistic-regression.html",
    "title": "15  More Logistic Regression",
    "section": "",
    "text": "15.1 Preparation\nIn this chapter, you will expand your knowledge of the logistic regression model to include categorical predictors, non-linear predictor effects, covariates, and interaction terms. We will again use data from the file graduation.csv to explore predictors of college graduation.\nWithin the analysis, we will focus on the research question of whether the probability of obtaining a degree differs for first and non-first generation students. We will use information criteria as the framework of evidence to adopt different models, along with an evaluation of the models’ residuals.\n# Load libraries\nlibrary(AICcmodavg)\nlibrary(broom)\nlibrary(corrr)\nlibrary(patchwork)\nlibrary(performance)\nlibrary(texreg)\nlibrary(tidyverse)\n\n# Read in data and create dummy variable for categorical variables\ngrad = read_csv(file = \"https://raw.githubusercontent.com/zief0002/fluffy-ants/main/data/graduation.csv\") |&gt;\n  mutate(\n    got_degree = if_else(degree == \"Yes\", 1, 0),\n    is_firstgen = if_else(first_gen == \"Yes\", 1, 0),\n    is_nontrad = if_else(non_traditional == \"Yes\", 1, 0),\n  )\n\n# View data\ngrad\n\n# A tibble: 2,344 × 10\n   student degree   act scholarship ap_courses first_gen non_traditional\n     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;          \n 1       1 Yes       21         0            0 No        No             \n 2       2 Yes       19         0            0 No        No             \n 3       3 Yes       27         0            0 Yes       No             \n 4       4 Yes       25         0.5          0 Yes       No             \n 5       5 No        28         0           17 Yes       No             \n 6       6 Yes       21         0            0 No        Yes            \n 7       7 Yes       27         0            8 Yes       No             \n 8       8 No        20         0            0 No        No             \n 9       9 Yes       26         0            0 Yes       No             \n10      10 Yes       25         0            4 Yes       No             \n# ℹ 2,334 more rows\n# ℹ 3 more variables: got_degree &lt;dbl&gt;, is_firstgen &lt;dbl&gt;, is_nontrad &lt;dbl&gt;\n\n# Fit models from previous notes\nglm.0 = glm(got_degree ~ 1, data = grad, family = binomial(link = \"logit\"))\nglm.1 = glm(got_degree ~ 1 + act, data = grad, family = binomial(link = \"logit\"))",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>More Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-03-more-logistic-regression.html#preparation",
    "href": "04-03-more-logistic-regression.html#preparation",
    "title": "15  More Logistic Regression",
    "section": "",
    "text": "CSV File\nData Codebook",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>More Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-03-more-logistic-regression.html#categorical-predictors",
    "href": "04-03-more-logistic-regression.html#categorical-predictors",
    "title": "15  More Logistic Regression",
    "section": "15.2 Categorical Predictors",
    "text": "15.2 Categorical Predictors\nWe will next turn to evaluating whether first generation students have a different predicted probability of obtaining a degree than non-first generation students. As always, we start with computing summary statistics and plots. Because both the predictor and outcome are categorical, we will again focus on counts and proportions.\n\n# Counts of students by degree and first gen status\ngrad |&gt;\n  group_by(degree, first_gen) |&gt;\n  summarize(\n    N = n()\n  )\n\n# A tibble: 4 × 3\n# Groups:   degree [2]\n  degree first_gen     N\n  &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;\n1 No     No          267\n2 No     Yes         360\n3 Yes    No          440\n4 Yes    Yes        1277\n\n\n\n\n15.2.1 Contingency Tables\nThe primary method of displaying summary information when both the predictor and outcome are categorical is via a contingency table. In our two-way table, we will display counts from the four different combinations of first generation and degree obtainment status. Since each of those variables includes two levels (degree/no degree and first gen/non-first gen), this is referred to as a 2x2 table; each number refers to the number of levels in one of the variables being displayed.\n\n\nCode\ntab_01 = data.frame(\n  first_gen = c(\"No\", \"Yes\", \"Total\"),\n  no = c(267, 360, 627),\n  yes = c(440, 1277, 1717),\n  total = c(707, 1637, 2344)\n)\n\n# Create table\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    first_gen = md(\"*First Generation Status*\"),\n    no = md(\"No\"),\n    yes = md(\"Yes\"),\n    total = md(\"Total\")\n  ) |&gt;\n  cols_align(\n    columns = c(first_gen),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(no, yes, total),\n    align = \"center\"\n  ) |&gt;\n  tab_spanner(\n    label = md(\"*Obtained Degree*\"),\n    columns = c(no, yes)\n  ) |&gt;\n  tab_style(\n        style = cell_text(align = \"left\", indent = px(20)),\n        locations = cells_body(\n          columns = first_gen,\n          rows = c(1, 2)\n        )\n    )\n\n\n\n\nTable 15.1: 2x2 table of counts for degree obtainment by first generation status.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Generation Status\n\nObtained Degree\n\nTotal\n\n\nNo\nYes\n\n\n\n\nNo\n267\n440\n707\n\n\nYes\n360\n1277\n1637\n\n\nTotal\n627\n1717\n2344\n\n\n\n\n\n\n\n\n\n\nThere are three types of counts (sample sizes) that are typically included in contingency tables:\n\nCell Counts: The “cells” in the two-way table include counts for the four combinations of first generation and degree obtainment status. The counts in those four cells are referred to as cell counts or cell sample sizes. These are generally notated symbolically as \\(n_{i,j}\\) where i indicates the row and j indicates the column. For example, \\(n_{1,2}=440\\) indicates the number of non-first generation students who obtained a degree.\nMarginal Counts: The counts in the “Total” row and column are called the marginal counts, or marginal sample sizes. They indicate the total counts collapsing across either rows or columns. These are generally notated symbolically as \\(n_{\\bullet,j}\\) or \\(n_{i, \\bullet}\\) where the dot indicates that we are collapsing across either rows or columns. For example, \\(n_{2,\\bullet}=1637\\) indicates the number of first generation students.\nGrand Count: Finally, there is the cell that includes the grand total sample size. This is usually just denoted as N (or sometimes \\(n_{\\bullet,\\bullet}\\)). In our example, \\(n=2344\\), the total number of students in the sample.\n\nThe three different totals (row total, column total, grand total) means that we can compute three different proportions for each of the four cells. Each proportion has a different interpretation depending on which total is used in the denominator. For example, consider the cell that designates first generation students who obtained a degree (\\(n_{2,2}=1227\\)).\n\nThe grand total of 2344 indicates all student, so computing the proportion using that denominator, \\(1227/2344 = 0.523\\), indicates that 0.523 of all students are first generation who obtained a degree.\nThe row total of 1637 indicates the number of students who are first generation students, so computing the proportion using that denominator, \\(1227/1637 = 0.750\\), indicates that 0.750 of all first generation students obtained a degree.\nThe column total of 1717 indicates the number of students who obtained a degree, so computing the proportion using that denominator, \\(1227/1717 = 0.715\\), indicates that 0.715 of students who obtained a degree are first generation students.\n\n\n\n\n15.2.2 Computing Proportions: Order in group_by() Matters\nWe can compute proportions using the row and column totals in the denominator by using the same set of dplyr operations we used previously, namely grouping by the predictor and outcome, summarizing to compute the sample size, and then mutating to find the proportion. The key is the order in which we include the variables in group_by().\nIf we want to use the row totals (the number of first generation and non-first generation students) in the denominator, we need to include first_gen in the group_by() prior to degree. The computation of sum() in mutate() will continue to be based on the first generation grouping (group_by() is in effect until we explicitly use ungroup()).\n\n# Use first generations status totals in proportion\ngrad |&gt;\n  group_by(first_gen, degree) |&gt;\n  summarize(N = n()) |&gt;\n  mutate(\n    Prop = N / sum (N)\n    )\n\n# A tibble: 4 × 4\n# Groups:   first_gen [2]\n  first_gen degree     N  Prop\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 No        No       267 0.378\n2 No        Yes      440 0.622\n3 Yes       No       360 0.220\n4 Yes       Yes     1277 0.780\n\n\nTo compute proportions based on the number of students who obtained a degree and the number who did not obtain a degree, we need to include degree in the group_by() function prior to first_gen.\n\n# Use degree status totals in proportion\ngrad |&gt;\n  group_by(degree, first_gen) |&gt;\n  summarize(N = n()) |&gt;\n  mutate(\n    Prop = N / sum (N)\n    )\n\n# A tibble: 4 × 4\n# Groups:   degree [2]\n  degree first_gen     N  Prop\n  &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 No     No          267 0.426\n2 No     Yes         360 0.574\n3 Yes    No          440 0.256\n4 Yes    Yes        1277 0.744\n\n\nIf you want to base the proportions on the grand total, the order in group_by() doesn’t matter, but you need to explicitly ungroup() prior to computing the proportions.\n\n# Use grand total in proportion\ngrad |&gt;\n  group_by(degree, first_gen) |&gt;\n  summarize(N = n()) |&gt;\n  ungroup() |&gt;\n  mutate(\n    Prop = N / sum (N)\n    )\n\n# A tibble: 4 × 4\n  degree first_gen     N  Prop\n  &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 No     No          267 0.114\n2 No     Yes         360 0.154\n3 Yes    No          440 0.188\n4 Yes    Yes        1277 0.545\n\n\n\nFYI\nYou need to select the the denominator to compute your proportions based on your research questions. You also need to make sure that if you are using R (or another software) to compute proportions that the proportions you want are being outputted.\n\n\n\n\n15.2.3 Back to the RQ\nWe wanted to know whether first generation students have a different predicted probability of obtaining a degree than non-first generation students. Thus, we want to use the row totals in our denominator. So we are comparing the proportion of all first generation students who got a degree to the proportion of all non-first generation students who got a degree.\n\n# Proportions to answer our RQ\ngrad |&gt;\n  group_by(first_gen, degree) |&gt;\n  summarize(N = n()) |&gt;\n  mutate(\n    Prop = N / sum (N)\n    ) |&gt;\n  ungroup() |&gt;\n  filter(degree == \"Yes\")\n\n# A tibble: 2 × 4\n  first_gen degree     N  Prop\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 No        Yes      440 0.622\n2 Yes       Yes     1277 0.780\n\n\nBased on the sample proportions, the proportion of first generation students who obtain a degree (0.780) is higher by 0.158 than the proportion of non-first generation students who obtain a degree (0.622). We could also plot this using a bar chart. The argument position=position_dodge() creates the side-by-side plot. Otherwise the plot would be a stacked bar chart which gives the same information.\n\n# Side-by-side bar chart\ngrad |&gt;\n  group_by(first_gen, degree) |&gt;\n  summarize(N = n()) |&gt;\n  mutate(\n    Prop = N / sum (N)\n    ) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = first_gen, y = Prop, fill = degree)) +\n    geom_bar(stat = \"Identity\", position = position_dodge()) +\n    theme_light() +\n    xlab(\"First Generation\") +\n    ylab(\"Proportion\") +\n    scale_fill_manual(\n      name = \"Obtained\\n Degree\",\n      values = c(\"#2EC4B6\", \"#E71D36\")\n      )\n\n\n\n\nSide-by-side barplot showing the proportion of first generation and non-first generation students in the sample who obtained and did not obtain a degree.\n\n\n\n\nTo evaluate whether this sample difference is more than we expect because of chance, we could create a dummy-coded indicator for first generation status and use that as a predictor in our logistic regression model.\n\n# Dummy code first_gen\ngrad = grad |&gt;\n  mutate(\n    is_firstgen = if_else(first_gen == \"Yes\", 1, 0)\n  )\n\n# Fit  model\nglm.2 = glm(got_degree ~ 1 + is_firstgen, data = grad, family = binomial(link = \"logit\"))\n\n# Evaluate first generation predictor\naictab(\n  cand.set = list(glm.0, glm.2),\n  modnames = c(\"Intercept-Only\", \"Effect of First Gen\")\n)\n\n\nModel selection based on AICc:\n\n                    K    AICc Delta_AICc AICcWt Cum.Wt       LL\nEffect of First Gen 2 2666.09       0.00      1      1 -1331.04\nIntercept-Only      1 2724.55      58.46      0      1 -1361.27\n\n\nGiven the data and candidate models, the model that includes first generation status in the model has more evidence. Since we are comparing to the intercept-only model in this output, We can also use the information in the residual deviance column to compute a pseudo-\\(R^2\\) value.\n\n# Compute pseudo R2\n(2722.6 - 2662.1) / 2722.6\n\n[1] 0.02222141\n\n\nThe pseudo-\\(R^2\\) value of 0.022 is close to 0, which suggests that first generation status although statistically relevant, is not incredibly predictive of the log-odds (or odds or probability) of obtaining a degree. Next we turn to the fitted equation:\n\n# Get coefficients\ncoef(glm.2)\n\n(Intercept) is_firstgen \n  0.4995261   0.7666388 \n\n\nBased on this output, the fitted equation for the model is:\n\\[\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) = 0.50+ 0.77(\\mathrm{First~Generation}_i)\n\\]\nInterpreting these coefficients in the log-odds metric:\n\nNon-first generation students’ predicted log-odds of obtaining a degree is 0.50.\nFirst generation students’ predicted log-odds of obtaining a degree is 0.767 higher than non-first generation students.\n\nTo determine first generation students’ log-odds of obtaining a degree, we substitute 1 into the fitted equation:\n\\[\n\\begin{split}\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) &= 0.50+ 0.77(1) \\\\[1ex]\n&= 1.27\n\\end{split}\n\\]\nWe can also transform the fitted equation into the odds metric and interpret.\n\\[\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} = e^{0.50} \\times e^{0.77(\\mathrm{First~Generation}_i)}\n\\]\n\n# Transform coefficients to odds metric\nexp(coef(glm.2))\n\n(Intercept) is_firstgen \n   1.647940    2.152519 \n\n\nInterpreting these coefficients in the odds metric:\n\nNon-first generation students’ predicted odds of obtaining a degree is 1.65. That is their probability of obtaining a degree is 1.64 times that of not obtaining a degree.\nFirst generation students’ predicted odds of obtaining a degree is 2.15 times higher than non-first generation students’ odds of obtaining a degree.\n\nTo determine first generation students’ odds of obtaining a degree, we can either substitute 1 into the transformed fitted equation:\n\\[\n\\begin{split}\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) &= e^{0.50} \\times e^{0.77(1)} \\\\[1ex]\n&= 3.56\n\\end{split}\n\\]\nOr we could have transformed their log-odds directly (\\(e^{1.27}=3.56\\)). Finally, we could also transform the equation into the probability metric.\n\\[\n\\hat\\pi_i = \\frac{e^{0.50+ 0.77(\\mathrm{First~Generation}_i)}}{1 + e^{0.50+ 0.77(\\mathrm{First~Generation}_i)}}\n\\]\n\n# Prob non-first gen students\nexp(0.50) / (1 + exp(0.50))\n\n[1] 0.6224593\n\n# Prob first gen students\nexp(0.50 + 0.77) / (1 + exp(0.50 + 0.77))\n\n[1] 0.7807427\n\n\nIn the probability metric we just compute the predicted probabilities and report/compare them. There is no easy conversion of the slope coefficient to interpret how much higher the probability of obtaining a degree for first generation students is than for non-first generation students.\n\n\n\n15.2.4 Evaluating Assumptions\nWhen the only predictor in a logistic model is categorical, the only thing we need to pay attention to is whether the average residual for each level of the predictor is close to 0. We can again use the binned_residuals() function to evaluate this. When the predictor is categorical, the binning will be done on the levels of the predictor.\n\n# Obtain average fitted values and average residuals\nout.2 = binned_residuals(glm.2)\n\n# View data frame\nas.data.frame(out.2)\n\n               xbar       ybar    n      x.lo      x.hi         se     CI_low\nconf_int  0.6223479 0.07908158  707 0.6223479 0.6223479 0.08473425 0.04270069\nconf_int1 0.7800855 0.16704062 1637 0.7800855 0.7800855 0.04907564 0.14668272\n            CI_high group\nconf_int  0.1154625   yes\nconf_int1 0.1873985    no\n\n\nHere we see the average residual (ybar) for both groups is near zero. This satisfies the linearity assumption. If you examined the binned residual plot, you would also see this, albeit in a graphical form.\nThe results from glm.2 indicated that there were differences between first and non-first generation students in the probability of obtaining a degree. Based on this model’s result, we found that the predicted probability of obtaining a degre for first generation students was 0.780 while that for non-first generation students was 0.622. Does this difference persist after controlling for other covariates?\nTo examine this, we want to fit a set of models that systematically include potential covariates along with first generation status. Our strategy for this is to start with the most important covariate. We can compare the model that includes first generation status and this covariate to a model that just includes first generation status. If the evidence indicates that the model with only first generation status is important we are done. If the evidence points toward the model that also includes the most important covariate, then we will continue by comparing that model to one that includes first generation status and the two most important covariates. We will continue with these comparisons until we adopt a final model or models.\nTo determine the order of importance, we can compute the correlations between each covariate and the dummy-coded outcome.\n\n# Correlations\ngrad |&gt;\n  select(got_degree, act, scholarship, ap_courses,is_nontrad) |&gt;\n  correlate()\n\n# A tibble: 5 × 6\n  term        got_degree    act scholarship ap_courses is_nontrad\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 got_degree     NA       0.195      0.142      0.149     -0.0841\n2 act             0.195  NA          0.315      0.432     -0.103 \n3 scholarship     0.142   0.315     NA          0.197     -0.0343\n4 ap_courses      0.149   0.432      0.197     NA         -0.0552\n5 is_nontrad     -0.0841 -0.103     -0.0343    -0.0552    NA     \n\n\nThe rank ordering of covariates (based on the absolute value of the correlation coefficients) will be:\n\nACT scores (\\(r = 0.195\\), most important)\nNumber of AP courses (\\(r = 0.149\\))\nScholarship amount (\\(r = 0.142\\))\nNon-traditional status (\\(r = -0.084\\))",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>More Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-03-more-logistic-regression.html#non-linear-effect-of-act",
    "href": "04-03-more-logistic-regression.html#non-linear-effect-of-act",
    "title": "15  More Logistic Regression",
    "section": "15.3 Non-Linear Effect of ACT",
    "text": "15.3 Non-Linear Effect of ACT\nTo begin the analysis, we will include ACT scores as a covariate in the model. Since the act variable is continuous, we need to use the “correct” functional form when we include it in the model. In the previous set of notes, we saw some misfit when we examined the binned residual plot for the logistic regression model that included ACT as a predictor of the probability of obtaining a degree. One possibility is that this may be because the effect of ACT on these probabilities is non-linear. To examine this, we can plot the empirical log-odds of obtaining a degree versus ACT scores, and examine the functional form of the relationship.\n\n\nCode\n# Obtain the log-odds who obtain a degree for each ACT score\nprop_grad = grad |&gt; \n  group_by(act, degree) |&gt; \n  summarize(N = n()) |&gt; \n  mutate(\n    Prop = N / sum (N)\n    ) |&gt;\n  ungroup() |&gt;\n  filter(degree == \"Yes\") |&gt;\n  mutate(\n    Odds = Prop / (1 - Prop),\n    Logits = log(Odds)\n  )\n\n# Scatterplot\nggplot(data = prop_grad, aes(x = act, y = Logits)) +\n  geom_point(aes(size = N)) +\n  geom_smooth(aes(weight = N), method = \"loess\", se = FALSE) +\n  theme_bw() +\n  xlab(\"ACT score\") +\n  ylab(\"Log-odds of obtaining a degree\")\n\n\n\n\n\n\n\n\nFigure 15.1: Log-odds of obtaining a degree conditional on ACT score. Size of the dot is proportional to sample size. The loess smoother is also weighted based on the sample size.\n\n\n\n\n\nThis suggests a non-linear relationship between ACT score and the log-odds of obtaining a degree. Using the Rule-of-the-Bulge, we can try to linearize this relationship by:\n\nIncluding a quadratic effect of ACT; or\nLog-transforming the ACT variable.\n\n\n# Fit non-linear models\nglm.1_quad = glm(got_degree ~ 1 + act + I(act^2), data = grad, family = binomial(link = \"logit\"))\nglm.1_log = glm(got_degree ~ 1 + log(act), data = grad, family = binomial(link = \"logit\"))\n\n# Obtain binned residuals\nout.1_quad = binned_residuals(glm.1_quad)\nout.1_log = binned_residuals(glm.1_log)\n\n# Binned residual plots\nplot(out.1_quad) | plot(out.1_log)\n\n\n\n\n\n\n\nFigure 15.2: Binned residual plots for two models that include non-linear effects of ACT. The quadratic model (LEFT) and the log-linear model (RIGHT).\n\n\n\n\n\nThe binned residuals show slight misfit to both the log-linear model and the quadratic model. We also examine the empirical support via information criteria.\n\naictab(\n  cand.set = list(glm.1, glm.1_quad, glm.1_log),\n  modnames = c(\"Linear Effect\", \"Quadratic Effect\", \"Log-Linear Effect\")\n)\n\n\nModel selection based on AICc:\n\n                  K    AICc Delta_AICc AICcWt Cum.Wt       LL\nQuadratic Effect  3 2632.13       0.00   0.51   0.51 -1313.06\nLog-Linear Effect 2 2632.34       0.21   0.46   0.96 -1314.17\nLinear Effect     2 2637.24       5.11   0.04   1.00 -1316.62\n\n\nHere the most empirically supported model (given the data and the candidate models) is the quadratic model, although there is also a great deal of support for the log-linear model. I would probably adopt the log-linear model based on its simplicity and the theoretical rationale that I would not expect the probability of graduating to be smaller for high ACT values. But, for pedagogical purposes, we will look at and interpret the output for both models.\n\n\n15.3.1 Quadratic Effect of ACT\nWe will start our interpretation by examining the coef() output.\n\n# Get coefficients\ncoef(glm.1_quad)\n\n (Intercept)          act     I(act^2) \n-4.566663084  0.365752506 -0.005464371 \n\n\nThe fitted model is:\n\\[\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) = -4.57 + 0.37(\\mathrm{ACT}_i) - 0.005(\\mathrm{ACT}_i^2)\n\\]\nSince this is an interaction model, we interpret the effect of ACT as: The effect of ACT on the log-odds of obtaining a degree depends on ACT. (We could also graph this effect if we were interested in the effect of ACT on the log-odds of obtaining a degree.)\nTransforming the fitted equation to the odds metric:\n\\[\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} = e^{-4.57} \\times e^{0.37(\\mathrm{ACT}_i)} \\times e^ {-0.005(\\mathrm{ACT}_i^2)}\n\\]\nBecause changing ACT now impacts two exponent terms, one of which includes the quadratic of ACT, it is super essential that we create a plot to interpret the effects of ACT on the odds of obtaining a degree.\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {exp(-4.57 + 0.37*x - 0.005*x^2)}\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted odds of obtaining a degree\")\n\n\n\n\n\n\n\nFigure 15.3: Predicted odds of obtaining a degree as a function of ACT score.\n\n\n\n\n\nThe odds curve is somewhat “S”-shaped (although it is not constrained between 0 and 1 like the probability curve). This is because of the quadratic effect of ACT. The overall trend is one of exponential growth, although the rate-of-change is not constant within this curve. The most rapid change in the odds of obtaining a degree occurs at ACT scores between 20 and 30.\nFinally, we could also transform the equation into the probability metric. The fitted equation is:\n\\[\n\\hat\\pi_i = \\frac{e^{-4.57 + 0.37(\\mathrm{ACT}_i) - 0.005(\\mathrm{ACT}_i^2)}}{1 + e^{-4.57 + 0.37(\\mathrm{ACT}_i) - 0.005(\\mathrm{ACT}_i^2)}}\n\\]\nAgain, it is worthwhile to create a plot to interpret the effects of ACT on the probability of obtaining a degree.\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {exp(-4.57 + 0.37*x - 0.005*x^2) / (1 + exp(-4.57 + 0.37*x - 0.005*x^2))}\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted probability of obtaining a degree\")\n\n\n\n\n\n\n\nFigure 15.4: Predicted probability of obtaining a degree as a function of ACT score.\n\n\n\n\n\nThe probability curve is “S”-shaped and constrained between 0 and 1. The left-hand side of the “S” is compressed, probably due to the range of ACT scores. ACT has the largest effect on the probability of obtaining a degree for scores less than 25, and then the effect begins to plateau (although it is still growing as the logistic curve is monotonic).\n\n\n\n15.3.2 Log-Linear Effect of ACT\nWe will again start our interpretation by examining the coef() output.\n\n# Get coefficients\ncoef(glm.1_log)\n\n(Intercept)    log(act) \n  -6.938715    2.500968 \n\n\nThe fitted model is:\n\\[\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) = -6.94 + 2.50\\bigg[\\ln(\\mathrm{ACT}_i)\\bigg]\n\\]\nWe interpret the effect of ACT as: Each 1% difference in ACT score is associated with a .025-unit change in the log-odds of obtaining a degree.\nTransforming the fitted equation to the odds metric:\n\\[\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} = e^{-6.94} \\times e^{2.50\\bigg[\\ln(\\mathrm{ACT}_i)\\bigg]}\n\\]\nIt is again easier to interpret the effects of ACT on the odds of obtaining a degree by creating a plot .\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {exp(-6.94 + 2.50*log(x))}\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted odds of obtaining a degree\")\n\n\n\n\n\n\n\nFigure 15.5: Predicted odds of obtaining a degree as a function of ACT score.\n\n\n\n\n\nThe effect of ACT on the probability of obtaining a degree is positive and non-linear (exponential growth). The predicted odds of obtaining a degree increases exponentially for higher ACT scores.\nFinally, we could also transform the equation into the probability metric. The fitted equation is:\n\\[\n\\hat\\pi_i = \\frac{e^{-6.94 + 2.50\\bigg[\\ln(\\mathrm{ACT}_i)\\bigg]}}{1 + e^{-6.94 + 2.50\\bigg[\\ln(\\mathrm{ACT}_i)\\bigg]}}\n\\]\nAgain, it is worthwhile to create a plot to interpret the effects of ACT on the probability of obtaining a degree.\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  geom_function(\n    fun = function(x) {exp(-6.94 + 2.50*log(x)) / (1 + exp(-6.94 + 2.50*log(x)))}\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted probability of obtaining a degree\")\n\n\n\n\n\n\n\nFigure 15.6: Predicted probability of obtaining a degree as a function of ACT score.\n\n\n\n\n\nThe effect of ACT on the probability of obtaining a degree is positive and non-linear. ACT has a larger effect on the probability of obtaining a degree for smaller ACT scores and this effect, while still positive, diminishes for higher ACT scores.\n\nPROTIP\nOnce you include non-linear effects in the model, always create a plot of the fitted equation to try and interpret odds or probabilities.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>More Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-03-more-logistic-regression.html#adding-covariates-main-effects-models",
    "href": "04-03-more-logistic-regression.html#adding-covariates-main-effects-models",
    "title": "15  More Logistic Regression",
    "section": "15.4 Adding Covariates: Main Effects Models",
    "text": "15.4 Adding Covariates: Main Effects Models\nNow that we have settled on a functional form for the effect of ACT, we can include it as a covariate in a model with first generation status. Including covariates in the logistic model is done the same way as for lm() models. We fit that model with the following syntax:\n\n# Fit main effects model\nglm.3 = glm(got_degree ~ 1 + is_firstgen + log(act), data = grad, family = binomial(link = \"logit\"))\n\nTo evaluate this model, and to begin to provide an answer for our research question, we will examine the model evidence comparing this model to both the baseline model (intercept-only) and the model that included the main-effect of first generation status.\n\n# Model evidence\naictab(\n  cand.set = list(glm.0, glm.2, glm.3),\n  modnames = c(\"Intercept-Only\", \"First Gen.\", \"First Gen. + ACT\")\n)\n\n\nModel selection based on AICc:\n\n                 K    AICc Delta_AICc AICcWt Cum.Wt       LL\nFirst Gen. + ACT 3 2611.01       0.00      1      1 -1302.50\nFirst Gen.       2 2666.09      55.09      0      1 -1331.04\nIntercept-Only   1 2724.55     113.54      0      1 -1361.27\n\n\nGiven the data and candidate models fitted, the empirical evidence overwhelmingly supports including both ACT scores and first generation status in the model. Adopting this model, we next look at the coefficients:\n\n# Coefficients\ncoef(glm.3)\n\n(Intercept) is_firstgen    log(act) \n -5.9129810   0.5089689   2.0719199 \n\n\nThe fitted equation is:\n\\[\n\\ln \\bigg( \\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg) = -5.91 + 0.51(\\mathrm{First~Generation}_i) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]\n\\]\nUsing the logit/log-odds metric, we interpret the coefficients as:\n\nNon-first generations students with an ACT score of 1 have a predicted log-odds of graduating of \\(-5.91\\).\nEach 1% difference in ACT score is associated with a difference of 0.02 in the predicted log-odds of graduating, after controlling for first generation status.\nFirst generation college students, have a predicted log-odds of graduating that is 0.51 higher than students who are not first generation students, after controlling for differences in ACT scores.\n\n\n\n15.4.1 Back-Transforming to Odds\nIf we back-transform the coefficients to facilitate interpretations using the odds metric, the fitted equation is:\n\\[\n\\begin{split}\n\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{-5.91 + 0.51(\\mathrm{First~Generation}_i) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]} \\\\[1ex]\n&=e^{-5.91} \\times e^{0.51(\\mathrm{First~Generation}_i)} \\times e^{2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}\n\\end{split}\n\\]\nWe can interpret the intercept and the effect of first generation status directly:\n\nNon-first generations students with an ACT score of 1 have a predicted odds of graduating of \\(e^{-5.91}=.003\\).\nFirst generation college students, have a predicted odds of graduating that is \\(e^{0.51}=1.66\\) times higher than that for non-first generations students, after controlling for differences in ACT scores.\nEach one-point difference in ACT score is associated with improving the odds of graduating 1.09 times, on average, after controlling for whether or not the students are first generation college students.\n\nWhile it is not pertinent to the research question, if you wanted to interpret the effect of ACT, you could plot the fitted model. As always with main effects models, we find the fitted equation for first generation and non-first generation students, and plot the two curves separately using multiple geom_function() layers.\n\\[\n\\begin{split}\n\\mathbf{Non\\mbox{-}First~Generation:}\\quad\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{-5.91 + 0.51(0) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]} \\\\[1ex]\n&= e^{-5.91 + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]} \\\\[5ex]\n\\mathbf{First~Generation:}\\quad\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i} &= e^{-5.91 + 0.51(1) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]} \\\\[1ex]\n&= e^{-5.40 + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]} \\\\[1ex]\n\\end{split}\n\\]\n\n# Plot the fitted equation\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  # Non-first generation students\n  geom_function(\n    fun = function(x) {exp(-5.91 + 2.07*log(x))},\n    linetype = \"dashed\",\n    color = \"blue\"\n    ) +\n  # First generation students\n  geom_function(\n    fun = function(x) {exp(-5.40 + 2.07*log(x))},\n    linetype = \"solid\",\n    color = \"red\"\n    )+\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted odds of obtaining a degree\")\n\n\n\n\n\n\n\nFigure 15.7: Predicted odds of obtaining a degree as a function of ACT score first generation (solid, red line) and non-first generation (dashed, blue line) students.\n\n\n\n\n\nHere we see that the odds of graduating increase exponentially at higher ACT scores for both first generation and non-first generation students, on average. This rate of increase, however, is higher for first generation students. Moreover, first generation students have higher odds of graduating than non-first generation students, regardless of ACT score.\n\n\n\n15.4.2 Back-Transforming to Probability\nAgain, while not related to the research question, we can also plot the predicted probability of graduating as a function of ACT score. Algebraically manipulating the fitted equation,\n\\[\n\\hat\\pi_i = \\frac{e^{-5.91 + 0.51(\\mathrm{First~Generation}_i) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}{1 + e^{-5.91 + 0.51(\\mathrm{First~Generation}_i) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}\n\\]\nWe can then produce the fitted equations for non-first generation and first generation students by substituting either 0 or 1, respectively, into the firstgen variable. These equations are:\nNon-First Generation Students\n\\[\n\\begin{split}\n\\hat\\pi_i &= \\frac{e^{-5.91 + 0.51(0) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}{1 + e^{-5.91 + 0.51(0) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}} \\\\[3ex]\n&= \\frac{e^{-5.91 + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}{1 + e^{-5.91 + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}\n\\end{split}\n\\]\nFirst Generation Students\n\\[\n\\begin{split}\n\\hat\\pi_i &= \\frac{e^{-5.91 + 0.51(1) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}{1 + e^{-5.91 + 0.51(1) + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}} \\\\[1em]\n&= \\frac{e^{-5.40 + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}{1 + e^{-5.40 + 2.07\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]}}\n\\end{split}\n\\]\nWe can include each of these in a geom_function() layer in our plot.\n\n# Plot the fitted equations\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  # Non-first generation students\n  geom_function(\n    fun = function(x) {exp(-5.91 + 2.07*log(x)) / (1 + exp(-5.91 + 2.07*log(x)))},\n    linetype = \"dashed\",\n    color = \"blue\"\n    ) +\n  # First generation students\n  geom_function(\n    fun = function(x) {exp(-5.40 + 2.07*log(x)) / (1 + exp(-5.40 + 2.07*log(x)))},\n    linetype = \"solid\",\n    color = \"red\"\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted probability of obtaining a degree\") +\n  ylim(0, 1)\n\n\n\n\n\n\n\nFigure 15.8: Predicted probabilities of obtaining a degree as a function of ACT score first generation (solid, red line) and non-first generation (dashed, blue line) students.\n\n\n\n\n\nHere we see that the probability of graduatingobtaining a degree is positively associated with ACT score for both first generation and non-first generation students. The magnitude of the effect of ACT depends on ACT score for both groups. Although first generation students have higher probability of obtaining a degree than non-first generation students regardless of ACT score, the magnitude of this difference decreases at higher ACT scores.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>More Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-03-more-logistic-regression.html#including-other-covariates-in-the-model",
    "href": "04-03-more-logistic-regression.html#including-other-covariates-in-the-model",
    "title": "15  More Logistic Regression",
    "section": "15.5 Including Other Covariates in the Model",
    "text": "15.5 Including Other Covariates in the Model\nWe will now fit a series of models that systematically include the remainder of the potential covariates based on their importance. We will have to adopt the correct functional form for ALL continuous variables by examining the plots of the data and residuals from potential transformations. (NOTE: This work is not shown here, but you should undertake it.)\nThe second covariate we will include is the number of AP courses taken. We use a log-transformation of this variable given its curvilinear status with the empirical log-odds of obtaining a degree. Because it has values of 0, we also add 1 before log-transforming.\n\n# Log-linear effect of AP courses\nglm.4 = glm(got_degree ~ 1 + is_firstgen + log(act) + log(ap_courses + 1), data = grad, family = binomial(link = \"logit\"))\n\nNext, we include scholarship amount into the model. An examination of the empirical log-odds of obtaining a degree versus scholarship amount also suggested a curvilinear relationship. We adopt a log-linear model, again adding 1 prior to transforming using the logarithm since there are 0s in the variable.\n\n# Effect of scholarship\nglm.5 = glm(got_degree ~ 1 + is_firstgen + log(act) + log(ap_courses + 1) +  log(scholarship + 1), data = grad, family = binomial(link = \"logit\"))\n\nFinally, we include the effect of non-traditional student status. Since it is a dummy-coded variable, we do not have to worry about the functional form. However, since we don’t know the functional form we need for scholarship amount, we will consider both when we include this covariate.\n\n# Effect of non-traditional student\nglm.6 = glm(got_degree ~ 1 + is_firstgen + log(act) + log(ap_courses + 1) + log(scholarship + 1) +\n              is_nontrad, data = grad, family = binomial(link = \"logit\"))\n\nNow we will use information criteria to select from these potential models.\n\naictab(\n  cand.set = list(glm.2, glm.3, glm.4, glm.5, glm.6),\n  modnames = c(\"FG\", \"FG + ACT\", \"FG + ACT + AP\", \"FG + ACT + AP + Sch\",\n               \"FG + ACT + AP + Sch + NT\")\n)\n\n\nModel selection based on AICc:\n\n                         K    AICc Delta_AICc AICcWt Cum.Wt       LL\nFG + ACT + AP + Sch + NT 6 2552.39       0.00    0.9    0.9 -1270.18\nFG + ACT + AP + Sch      5 2556.80       4.41    0.1    1.0 -1273.39\nFG + ACT + AP            4 2579.98      27.59    0.0    1.0 -1285.98\nFG + ACT                 3 2611.01      58.62    0.0    1.0 -1302.50\nFG                       2 2666.09     113.70    0.0    1.0 -1331.04\n\n\nHere the empirical evidence supports the model that includes all the covariates. Examining the binned residuals (not shown), the model does not seem adequate. Next we turn to evaluating potential interaction terms to see if that improves the residual fit.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>More Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-03-more-logistic-regression.html#interaction-between-act-score-and-first-generation-status",
    "href": "04-03-more-logistic-regression.html#interaction-between-act-score-and-first-generation-status",
    "title": "15  More Logistic Regression",
    "section": "15.6 Interaction Between ACT Score and First Generation Status",
    "text": "15.6 Interaction Between ACT Score and First Generation Status\nSince this is an exploratory analysis, we should only fit first-order interactions, and only with the focal predictor (i.e., first generation status). We will adopt the same modeling strategy as before, building up interaction terms in the order of covariate importance.\n\n# Interaction models\nglm.7 = glm(got_degree ~ 1 + is_firstgen + log(act) + log(ap_courses + 1) + log(scholarship + 1) +\n              is_nontrad + is_firstgen:log(act), data = grad, family = binomial(link = \"logit\"))\n\nglm.8 = glm(got_degree ~ 1 + is_firstgen + log(act) + log(ap_courses + 1) + log(scholarship + 1) +\n              is_nontrad + is_firstgen:log(act) + is_firstgen:log(ap_courses + 1),\n            data = grad, family = binomial(link = \"logit\"))\n\nglm.9 = glm(got_degree ~ 1 + is_firstgen + log(act) + log(ap_courses + 1) + log(scholarship + 1) +\n              is_nontrad + is_firstgen:log(act) + is_firstgen:log(ap_courses + 1) +\n              is_firstgen:log(scholarship + 1),\n            data = grad, family = binomial(link = \"logit\"))\n\n\nglm.10 = glm(got_degree ~ 1 + is_firstgen + log(act) + log(ap_courses + 1) + log(scholarship + 1) +\n              is_nontrad + is_firstgen:log(act) + is_firstgen:log(ap_courses + 1) +\n              is_firstgen:log(scholarship + 1) + is_firstgen:is_nontrad,\n            data = grad, family = binomial(link = \"logit\"))\n\n# Evaluate\naictab(\n  cand.set = list(glm.6, glm.7, glm.8, glm.9, glm.10),\n  modnames = c(\"Main Effects\", \"FG:ACT\", \"FG:ACT + FG:AP\", \"FG:ACT + FG:APP + FG:Sch\",\n               \"FG:ACT + FG:APP + FG:Sch + FG:NT\")\n)\n\n\nModel selection based on AICc:\n\n                                  K    AICc Delta_AICc AICcWt Cum.Wt       LL\nFG:ACT                            7 2550.72       0.00   0.44   0.44 -1268.34\nFG:ACT + FG:AP                    8 2552.24       1.52   0.20   0.64 -1268.09\nMain Effects                      6 2552.39       1.67   0.19   0.83 -1270.18\nFG:ACT + FG:APP + FG:Sch          9 2553.65       2.94   0.10   0.93 -1267.79\nFG:ACT + FG:APP + FG:Sch + FG:NT 10 2554.53       3.82   0.07   1.00 -1267.22\n\n\nWhile the model that includes the interaction between first generation status and ACT has the most empirical evidence (given the data and candidate models), there is some degree of empirical evidence for almost all of the interaction models fitted. Evaluating the binned residuals (not shown) there is a reasonable (and roughly equal) degree of residual fit for glm.7, glm.8, and glm.10. Given this, the information from the AICc table, and the rule of parsimony, we will adopt glm.7.\n\n# Coefficients\ncoef(glm.7)\n\n         (Intercept)          is_firstgen             log(act) \n          -1.2481921           -3.0403893            0.5369378 \n log(ap_courses + 1) log(scholarship + 1)           is_nontrad \n           0.2798161            1.1082252           -0.8317753 \nis_firstgen:log(act) \n           1.1033996 \n\n\n\\[\n\\begin{split}\n\\ln\\bigg[\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg] = &-1.25 - 3.04(\\mathrm{First~Gen}_i) + 0.54\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg] + 0.28\\bigg[\\ln(\\mathrm{AP~Courses}_i + 1)\\bigg] +\\\\\n& 1.11\\bigg[\\ln(\\mathrm{Scholarship}_i + 1)\\bigg] - 0.83(\\mathrm{Non\\mbox{-}Traditional}_i) + 1.10\\bigg[\\ln(\\mathrm{ACT~Score}_i\\bigg](\\mathrm{First~Gen}_i)\n\\end{split}\n\\]",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>More Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-03-more-logistic-regression.html#interpreting-the-results-from-the-adopted-model",
    "href": "04-03-more-logistic-regression.html#interpreting-the-results-from-the-adopted-model",
    "title": "15  More Logistic Regression",
    "section": "15.7 Interpreting the Results from the Adopted Model",
    "text": "15.7 Interpreting the Results from the Adopted Model\nTo interpret these results, we will plot results freom the fitted model. In thinking about what needs to be displayed:\n\nThe effect of first-generation student wil be displayed as separate lines since it is the focal predictor.\nThe effect of ACT will be displayed since it is part of an interaction with the focal predictor.\nThe effect of AP Courses will be controlled out by setting to its mean value.\nThe effect of Scholarship will be controlled out by setting to its mean value.\nThe effect of Non-traditional will be controlled by setting the value to the reference level (e.g., traditional student).1\n\nWe next use the fitted equation to find the equations for the first and non-first generation students.\nNon-First Generation Students\n\\[\n\\begin{split}\n\\ln\\bigg[\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg] = &-1.25 - 3.04(0) + 0.54\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg] + 0.28\\bigg[\\ln(3.28 + 1)\\bigg] +\\\\\n& 1.11\\bigg[\\ln(0.18 + 1)\\bigg] - 0.83(0) + 1.10\\bigg[\\ln(\\mathrm{ACT~Score}_i\\bigg](0) \\\\[3ex]\n= & -0.66 + 0.54\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]\n\\end{split}\n\\]\nFirst Generation Students\n\\[\n\\begin{split}\n\\ln\\bigg[\\frac{\\hat\\pi_i}{1 - \\hat\\pi_i}\\bigg] = &-1.25 - 3.04(1) + 0.54\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg] + 0.28\\bigg[\\ln(3.28 + 1)\\bigg] +\\\\\n& 1.11\\bigg[\\ln(0.18 + 1)\\bigg] - 0.83(0) + 1.10\\bigg[\\ln(\\mathrm{ACT~Score}_i\\bigg](1) \\\\[3ex]\n= & -3.70 + 1.64\\bigg[\\ln(\\mathrm{ACT~Score}_i)\\bigg]\n\\end{split}\n\\]\nWe will plot these two equations using the probability metric.\n\n# Plot the fitted equations\nggplot(data = grad, aes(x = act, y = got_degree)) +\n  geom_point(alpha = 0) +\n  # Non-first generation students\n  geom_function(\n    fun = function(x) {exp(-0.66 + 0.54*log(x)) / (1 + exp(-0.66 + 0.5*log(x)))},\n    linetype = \"dashed\",\n    color = \"blue\"\n    ) +\n  # First generation students\n  geom_function(\n    fun = function(x) {exp(-3.70 + 1.64*log(x)) / (1 + exp(-3.70 + 1.647*log(x)))},\n    linetype = \"solid\",\n    color = \"red\"\n    ) +\n  theme_light() +\n  xlab(\"ACT score\") +\n  ylab(\"Predicted probability of obtaining a degree\") +\n  ylim(0, 1)\n\n\n\n\n\n\n\nFigure 15.9: Predicted probability of obtaining a degree as a function of ACT score for first generation (solid, red line) and non-first generation (dashed, blue line) who ar 18 years old at the time of enrollment. The effects for the number of AP credits and scholarship amount (in thousands of dollars) were controlled by setting them to their mean values of 3.28 and 0.18, respectively.\n\n\n\n\n\nBecause our research question is about the effect of first generation status, we will focus the interpretation from this plot on that effect.\nHere we see that first generation status and ACT score interact on the probability of obtaining a degree. For students with lower ACT scores the probability of obtaining a degree is smaller for first generation students than for non-first generation students. However, this difference diminishes for students with higher ACT scores.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>More Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-03-more-logistic-regression.html#presenting-a-table-of-logistic-regression-results",
    "href": "04-03-more-logistic-regression.html#presenting-a-table-of-logistic-regression-results",
    "title": "15  More Logistic Regression",
    "section": "15.8 Presenting a Table of Logistic Regression Results",
    "text": "15.8 Presenting a Table of Logistic Regression Results\nWe can present selected fitted models from the analysis in a table similar to those we have created for OLS models. In this analysis I would present glm.0, glm.2, glm.3, glm.6, and glm.7. Note that we do not have to present ALL the models we fitted in the table, only those that tell a coherent story about the analysis. (If you are worried about transparency, you can always link to our script file or RMD document which includes all the models fitted in the paper, or include some prose that indicates there were other models fitted.) For these models, we want to report:\n\nCoefficients and SEs\nResidual Deviance\nAICc values (This can be computed from the df and residual deviances, but since it was our main criterion for model selection, it is a good idea to report it!)\nPseudo-\\(R^2\\) values\n\nHere is a table I might present for this analysis.\n\nCode\n# Create the table\nhtmlreg(\n  l = list(glm.0, glm.2, glm.3, glm.6, glm.7),\n  stars = numeric(0),    #No p-value stars\n  digits = 2,\n  padding = 20,          #Add space around columns (you may need to adjust this via trial-and-error)\n  custom.model.names = c(\"Model A\", \"Model B\", \"Model C\", \"Model D\", \"Model E\"),\n  custom.coef.names = c(\"Intercept\", \"First Generation\", \"ln(ACT Score)\",\n                        \"ln(AP Courses + 1)\", \"ln(Scholarship Amount + 1)\", \"Non-Traditional Student\",\n                        \"First Generation x ln(ACT Score)\"),\n  reorder.coef = c(2:7, 1), #Put intercept at bottom of table\n  include.aic = FALSE, #Omit AIC\n  include.bic = FALSE, #Omit BIC\n  include.nobs = FALSE,  #Omit sample size\n  include.loglik = FALSE,   #Omit log-likelihood\n  custom.gof.rows = list(\n    AICc = c(AICc(glm.0), AICc(glm.2), AICc(glm.3), AICc(glm.6), AICc(glm.7)),\n    R2 = (2722.5 - c(NA, 2662.09, 2605.00, 2540.35, 2536.67)) /  2722.5\n    ), # Add AICc values\n  reorder.gof = c(3, 1, 2),\n  caption = NULL,\n  caption.above = TRUE, #Move caption above table\n  inner.rules = 1, #Include line rule before model-level output\n  outer.rules = 1 , #Include line rules around table\n  custom.note = \"The $R^2$ value is based on the proportion of reduced deviance from the intercept-only model (Model A)\"\n  )\n\n\n\n\nTable 15.2: Five candidate models predicting variation in the log-odds of obtaining a degree. The first generation and non-traditional student predictors were dummy-coded.\n\n\n\n\n\n\n \n\n\nModel A\n\n\nModel B\n\n\nModel C\n\n\nModel D\n\n\nModel E\n\n\n\n\n\n\nFirst Generation\n\n\n \n\n\n0.77\n\n\n0.51\n\n\n0.42\n\n\n-3.04\n\n\n\n\n \n\n\n \n\n\n(0.10)\n\n\n(0.10)\n\n\n(0.11)\n\n\n(1.80)\n\n\n\n\nln(ACT Score)\n\n\n \n\n\n \n\n\n2.07\n\n\n1.11\n\n\n0.54\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.28)\n\n\n(0.30)\n\n\n(0.42)\n\n\n\n\nln(AP Courses + 1)\n\n\n \n\n\n \n\n\n \n\n\n0.29\n\n\n0.28\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.06)\n\n\n(0.06)\n\n\n\n\nln(Scholarship Amount + 1)\n\n\n \n\n\n \n\n\n \n\n\n1.16\n\n\n1.11\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.26)\n\n\n(0.26)\n\n\n\n\nNon-Traditional Student\n\n\n \n\n\n \n\n\n \n\n\n-0.84\n\n\n-0.83\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.33)\n\n\n(0.33)\n\n\n\n\nFirst Generation x ln(ACT Score)\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n1.10\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.57)\n\n\n\n\nIntercept\n\n\n1.01\n\n\n0.50\n\n\n-5.91\n\n\n-3.03\n\n\n-1.25\n\n\n\n\n \n\n\n(0.05)\n\n\n(0.08)\n\n\n(0.86)\n\n\n(0.94)\n\n\n(1.31)\n\n\n\n\nDeviance\n\n\n2722.55\n\n\n2662.09\n\n\n2605.00\n\n\n2540.35\n\n\n2536.67\n\n\n\n\nAICc\n\n\n2724.55\n\n\n2666.09\n\n\n2611.01\n\n\n2552.39\n\n\n2550.72\n\n\n\n\nR2\n\n\n \n\n\n0.02\n\n\n0.04\n\n\n0.07\n\n\n0.07\n\n\n\n\n\n\nThe \\(R^2\\) value is based on the proportion of reduced deviance from the intercept-only model (Model A)\n\n\n\n\n\n\n\n\nFYI\nPseudo-\\(R^2\\) values are commonly reported in applied research. Because there are many potential pseudo-\\(R^2\\) values, you should always indicate the method used to calculate this metric.",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>More Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04-03-more-logistic-regression.html#footnotes",
    "href": "04-03-more-logistic-regression.html#footnotes",
    "title": "15  More Logistic Regression",
    "section": "",
    "text": "Another analyst might choose to use the non-traditional group or to show the effect by displaying both groups, perhaps in different panels.↩︎",
    "crumbs": [
      "Modeling Dichotomous Outcomes",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>More Logistic Regression</span>"
    ]
  }
]