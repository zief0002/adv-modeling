[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Modeling and Reproducibility for Educational Scientists",
    "section": "",
    "text": "Front Matter\nThe content in this “book”, as the title suggests, is related to advanced statistical modeling and reproducibility. More specifically, the content focuses on expanding the General Linear Model (GLM) to provide statistical evidence that can help answer substantive questions in the educational and social sciences. It also includes computing content for leveling-up educational scientists’ reproducibility of analyses.\nWhile it is a book intended for applied practitioners in the educational or social sciences, and the statistical content is hopefully presented in a manner that these domain scientists will find useful, it does require a good foundation in regression analysis using the GLM. Moreover, since the content is somewhat mathematical in nature, the reader will need a solid understanding of algebra for maximum benefit. The burden of calculation that typically accompanied statistical work in previous generations is now primarily carried out in a scientific computing environment. As Thisted & Velleman (1992) point out, “computational advances have changed the face of statistical practice by transforming what we do and by challenging how we think about scientific problems.” To support and help facilitate the use of scientific computing, examples using the R computer language will be used throughout this work.\nThe organization of content is consistent with the sequence this content is taught in EPsy 8252, the second of two applied statistics courses that form the foundational sequence for many graduate students in the educational and social sciences at the University of Minnesota. This course require that students have taken a previous statistics course at the graduate level focusing on the GLM.",
    "crumbs": [
      "Front Matter"
    ]
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Advanced Modeling and Reproducibility for Educational Scientists",
    "section": "Resources",
    "text": "Resources\nThis book refers to and uses several data sets throughout the text. Each of these data sets and their codebooks are available online at the book’s github repository, https://github.com/zief0002/adv-modeling/.",
    "crumbs": [
      "Front Matter"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Advanced Modeling and Reproducibility for Educational Scientists",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nMany thanks to all the students in my courses who have been through previous iterations of this material. Your feedback has been invaluable, and you are the world’s greatest copyeditors. In particular, I would like to thank the following students who have gone above and beyond in the feedback they have provided: Jonathan Brown, Pablo Vivas Corrales, Amaniel Mrutu, Corissa Rohloff, and Mireya Smith.",
    "crumbs": [
      "Front Matter"
    ]
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "Advanced Modeling and Reproducibility for Educational Scientists",
    "section": "Colophon",
    "text": "Colophon\nThe book is typeset using Atkinson Hyperlegible. The color palette was generated using coolors.co.\nIcon and note ideas and prototypes by Desirée De Leon.\nArtwork\n\nHedgehog banner by Alison Hill and Allison Horst, for RStudio.\nData science and statistics artwork by @allison_horst",
    "crumbs": [
      "Front Matter"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Advanced Modeling and Reproducibility for Educational Scientists",
    "section": "License",
    "text": "License\n\nAdvanced Modeling and Reproducibility for Educational Scientists by Andrew Zieffler is licensed under CC BY-NC-SA 4.0",
    "crumbs": [
      "Front Matter"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Advanced Modeling and Reproducibility for Educational Scientists",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nThisted, R. A., & Velleman, P. F. (1992). Computers and modern statistics. In D. C. Hoaglin & D. S. Moore (Eds.), Perspectives on contemporary statistics, MAA notes no. 21 (pp. 41–53). Mathematical Association of America.",
    "crumbs": [
      "Front Matter"
    ]
  },
  {
    "objectID": "01-00-reproducibility.html",
    "href": "01-00-reproducibility.html",
    "title": "Introduction to Statistical Reproducibility",
    "section": "",
    "text": "The first set of tools we will discuss will be related to reproducibility. Reproducibility, the idea that same results from a study should be obtained if the study is repeated, is a key part of the scientific process. Here, we are particularly concerned with statistical reproducibility. That is, given the same data and same code, two analysts should get the exact same results.\nIn practice, reproducibility means making your data and syntax available to other researchers. There are many ways to do this, but the Open Science Framework website is a fantastic option for making resources from research projects openly accessible to other researchers. Aside from the obvious advantages of minimizing fraud and bias, making research artifacts openly available also improves trust in the scientific process.\n\nPROTIP\nMaking your data and syntax available is something to consider duing the planning phases of a study so that you can make this transparent in the IRB proposal.\n\nThe initial chapters of this book will address:\n\nInstalling Quarto and other computational tools;\nProject organization; and\nUsing Quarto to integrate text and computation.",
    "crumbs": [
      "Introduction to Statistical Reproducibility"
    ]
  },
  {
    "objectID": "01-02-project-organization.html",
    "href": "01-02-project-organization.html",
    "title": "1  Project Organization",
    "section": "",
    "text": "1.1 Projects\nIn this chapter, you will learn some basic tips and tricks for organizing directories and files for project management. At the end of it, you will have an organized project directory to begin work on Assignment 1.\nIn any project, whether it is work for an RA-ship, an assignment in EPsy 8252, a milestone for your degree (e.g., thesis) or a paper you are working on, it is important to have an organized set of files and good documentation. Both of these help foster rigor and reproducibility of research, and are even more important in collaborative work.\nElectronically, a project consists of a set of directories (i.e., folders) and files. Organizationally, you will want to think about, among other things:\nWhile there are any of a number of alternative organizational structures that may work in different situations, here I will recommend a general strategy of creating projects for EPsy 8252. (This will also work for other types of projects.) For each project, we will have, at a minimum, the following directories:\nThe screenshot below shows a project with these directories.\nIt is typical to show directory and file organization via a “directory tree”. The directory tree for this project is shown below.\nHere the directory assignment-01 is the primary folder associated with our project. We refer to this as the root directory of our project.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-02-project-organization.html#projects",
    "href": "01-02-project-organization.html#projects",
    "title": "1  Project Organization",
    "section": "",
    "text": "What directories you will have;\nWhich files will be located in which directories;\nThe naming rules for files and directories;\n\n\n\nassets\ndata\nfigs\nscripts\n\n\n\n\n├── assets\n├── data\n├── figs\n└── scripts\n\nYour Turn\nSet up a directory called assignment-01. Within this directory, create the four directories: assets, data, figs, and scripts. The directory tree will look like this:\nassignment-01\n    ├── assets\n    ├── data\n    ├── figs\n    └── scripts\n\n\n\n\n1.1.1 Naming Conventions\nThe naming conventions for the directories and files in our project are as follows:\n\nFile names should be short but descriptive (less than 25 characters)\nAll lowercase letters\nAvoid special characters and spaces in a file name\n\nUse hyphens instead of spaces to separate words (e.g., assignment-01)\n\nAny names that include the date will use the ISO 8601 date format (YYYYMMDD)\nAny names that include a number will include at least two digits (e.g., assignment-01 rather than assignment-1)\n\nAgain, while there is no one best naming convention, it is important that you have one, and that you are consistent throughout the project. That being said, as you develop naming conventions for your projects, all the conventions should be documented! This documentation helps onboard collaborators to your project.\n\nFYI\nThere are several guides available to help you establish naming conventions including here and here.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-02-project-organization.html#r-projects",
    "href": "01-02-project-organization.html#r-projects",
    "title": "1  Project Organization",
    "section": "1.2 R Projects",
    "text": "1.2 R Projects\nCreating an R project is a way to inform RStudio about which folder is your project’s root directory. This also sets R’s working directory to your project’s root directory making it easier to access data and other files in your QMD documents. To create an R Project associated with the assignmnet-01 root directory,\n\nClick the project icon in RStudio (it might say Project: (None) or something like that) and select New Project... (see screenshot below)\n\n\n\n\n\n\n\n\n\n\nIn the project wizard, select:\n\nExisting Directory,\nClick Browse and select the assignment-01 root directory you created earlier\nClick Open\nClick Create Project\n\nThe R Project will then be created and you should now see this project (called assignment-01.Rproj) in the assignment-01 directory.1 The project icon in RStudio should also have changed to assignment-01.\n└── assignment-01\n    ├── assets\n    ├── assignment-01.Rproj\n    ├── data\n    ├── figs\n    └── scripts\nOnce an R project has been created, it can be opened in RStudio by double-clicking on the .Rproj file.\n\nYour Turn\nIf you haven’t already, create an R project associated with the assignment-01 root directory.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-02-project-organization.html#working-in-your-project",
    "href": "01-02-project-organization.html#working-in-your-project",
    "title": "1  Project Organization",
    "section": "1.3 Working in Your Project",
    "text": "1.3 Working in Your Project\nIn RStudio, the Files pane of your assignmnet-01 R Project (see below) should show the contents of your root directory. You can use this pane to navigate to the different directories, or open files in the project.\n\n\n\n\n\n\n\n\n\nWe can add files and directories by creating them using an application on our computer or downloading them and saving them to the appropriate project directory.\n\nYour Turn\nDownload the pew.csv file to your computer. Make sure that the file suffix is .csv. (Sometimes Safari users will find that the suffix .txt gets appended to the end of the filename—pew.csv.txt. If that is the case delete the .txt part that was appended.) Place pew.csv into the data directory of your project. The new tree is:\nassignment-01\n    ├── assets\n    ├── assignment-01.Rproj\n    ├── data\n    │   └── pew.csv\n    ├── figs\n    └── scripts\n\nIn RStudio, in the Files pane, if you click on the data directory, you should see pew.csv.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-02-project-organization.html#creating-a-readme-file",
    "href": "01-02-project-organization.html#creating-a-readme-file",
    "title": "1  Project Organization",
    "section": "1.4 Creating a README File",
    "text": "1.4 Creating a README File\nWe can also create files from inside RStudio . We are going to create a README file. A README is a plain text file that introduces and explains a project. It contains information that is commonly required to understand what the project is about. Every project should have a README file.\n\nYour Turn\nFrom the RStudio menu, select: File &gt; New File... &gt; Text File. This should open a blank text file in the RStudio editor. Copy the following text into your blank file:\n# assignment-01\n\nThis directory contains all of the files necessary to complete Assignment 1.\nClick the Save icon (or select File &gt; Save) and save this file as README; all uppercase. (README files are the one exception to our use of uppercase letters in our naming convention.) README files typically have no file extension—README is correct as opposed to README.txt.\n\nOur directory tree now look like this:\nassignment-01\n    ├── README\n    ├── assets\n    ├── assignment-01.Rproj\n    ├── data\n    │   └── pew.csv\n    ├── figs\n    └── scripts\nNotice that despite the README file starting with the letter “r”, it is placed first in the root directory prior to our other directories and files. This is by design. In Unix, any file in all uppercase letters is shown first in the directory tree. (It does not show first in the RStudio Files pane, nor in the file view when you open the root directory on your computer.)\n\nProtip The {usethis} package includes functionality to create many useful files for projects, including README files.\n\n\n\n1.4.1 Adding Content to README\nSince README files are plain text files, they cannot include formatting like bold or italic. However, they do typically include Markdown syntax (which is itself plain text). The plain text nature of these files keeps them small in size and accessible to anyone with any type of computer.\n\nFYI\nThere are several online guides for what to include in a README file, including here and here. There is also a pretty good template for a README for data science oriented projects here.\n\nSince the README file is informational, you can include any type of information in this file that is useful to the project. For example, you could add your naming conventions to this file.\n# assignment-01\n\nThis directory contains all of the files necessary to complete Assignment 1.\n\n\n# Naming Conventions\n\n- File names should be short but descriptive (less than 25 characters)\n- All lowercase letters\n- Avoid special characters and spaces in a file name\n  + Use hyphens instead of spaces to separate words (e.g., `assignment-01`)\n- Any names that include the date will use the ISO 8601 date format (YYYYMMDD)\n- Any names that include a number will include at least two digits (e.g., `assignment-01` rather than `assignment-1`)\n\nWhile you should only have one README file per directory, you can have different README files in other directories. For example, you could create a README file in the data directory that includes the codebook information for your data files.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-02-project-organization.html#references",
    "href": "01-02-project-organization.html#references",
    "title": "1  Project Organization",
    "section": "1.5 References",
    "text": "1.5 References",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-02-project-organization.html#footnotes",
    "href": "01-02-project-organization.html#footnotes",
    "title": "1  Project Organization",
    "section": "",
    "text": "R projects have the same name as the root directory you associated it with and have the file suffix .Rproj. Our directory tree is now:↩︎",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Organization</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html",
    "href": "01-03-introduction-to-quarto.html",
    "title": "2  Introduction to Quarto",
    "section": "",
    "text": "2.1 Creating a New Quarto Document\nIn this set of notes, you will begin your Quarto journey.\nTo create a new Quarto document, go to the directory where you want to put the file and click New File under the “Files” tab, and select Quarto Document. In the popup box, give your new document a name. Quarto documents have the extension .qmd.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#yaml-the-stuff-between-the-sets-of-three-dashes",
    "href": "01-03-introduction-to-quarto.html#yaml-the-stuff-between-the-sets-of-three-dashes",
    "title": "2  Introduction to Quarto",
    "section": "2.2 YAML: The Stuff Between the Sets of Three Dashes",
    "text": "2.2 YAML: The Stuff Between the Sets of Three Dashes\nYAML (pronounced Yam-el) is an acronym for “Yet Another Markup Language”. It constitutes the metadata for your Quarto document. Not only is it informational, but it also helps format your document. The metadata in your newly created Quarto document looks something like this:\n\n---\ntitle: \"hello-world\"\nformat: html\n---\n\nThese are called key-value pairs. The “keys” are “title” and “format”. The value associated with title is “hello-world” (by default this will be whatever you named the document), and the value associated with format is “html”.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#rendering-the-document",
    "href": "01-03-introduction-to-quarto.html#rendering-the-document",
    "title": "2  Introduction to Quarto",
    "section": "2.3 Rendering the Document",
    "text": "2.3 Rendering the Document\nClick the Render button above your Quarto document. This will render the quarto document (i.e., the QMD file) into a human-consumable format—in this case an HTML file that can be opened in a web browser. The rendered document should also open in your “Viewer” tab in RStudio. If you go back and look at yuor project directory, you should also see a new HTML file has been created. (In my case, this was “hello-world.html”.) This can be opened in a web-browser; try it by double-clicking the HTML file.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#update-the-yaml",
    "href": "01-03-introduction-to-quarto.html#update-the-yaml",
    "title": "2  Introduction to Quarto",
    "section": "2.4 Update the YAML",
    "text": "2.4 Update the YAML\nOur document is pretty boring—it only has a title that is the same as our file name. Let’s edit the YAML to see how we can add to our document’s metadata. First you can update the title value to whatever you want the title of your document to be:\n\n---\ntitle: \"My First Quarto Document\"\nformat: html\n---\n\nWe can also add additional metadata. Here we add a subtitle, author(s), and date.\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\nOnce you have edited and added in the new YAML keys and values, re-click the Render button.\n\nNotice\nAll the YAML keys and special values (e.g. html) are all lower case! Special values do not need quotes. All other strings need quotes.\n\nThe keys “subtitle”, “author”, and “date” are all keys that Quarto recognizes. Some of these keys are more global (e.g., title, author) and work across different formats. Others are unique to the type of document format. For example, some keys work with HTML formatted documents and others work with PDF formatted documents. You can learn about some of the YAML that works with HTML formatted documents here: https://quarto.org/docs/output-formats/html-basics.html\n\n\n2.4.1 Autopopulating the Date\nWe can auto-populate the date: key in our document’s YAML by including “today” rather than a specifc date. The date will update to the current date when you render the document.\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#adding-content",
    "href": "01-03-introduction-to-quarto.html#adding-content",
    "title": "2  Introduction to Quarto",
    "section": "2.5 Adding Content",
    "text": "2.5 Adding Content\nThe primary part of a Quarto document is not the YAML, but the actual body of the document. Things like headings, text, analysis, and other content are part of the body of the Quarto document. This is all added BELOW the sets of three dashes.\nLet’s add some text to our document.\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\nHere is some text for my document. Notice that it is below the YAML.\n\nHere is a new paragraph. A line space indicates that we start a new paragraph.\n\n\nWORKFLOW\nAs you add content to your QMD file you should save it often. To see how the new content looks, you need to render the document. Rendering the document also typically saves the added content to the QMD file. So your workflow is: Add content –&gt; Render.\n\n\n\n2.5.1 Headings\nIn an HTML document there are six levels of headings. You can think about headings as denoting sections and subsections (and sub-sub-sections, etc.) in a document.\nHeadings need to be on their own line and the star of the line is one or more hashtags. A level-1 heading uses one hashtag, a level-2 heading uses two hashtags. A level-3 heading uses three hastags. Etc. You also need a blank line above and below the heading line.\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Hello\n\nThis is a level-1 heading. Note the blank line above and below the heading.\n\n## World\n\nThis is a level-2 heading. Note the heading starts with two hashtags.\n\n\n\n\n2.5.2 Bold and Italics\nText can be bolded or italicized by adding asterisks around the text. A single set of asterisks is placed around text that you want to italicize and a double set placed around text you want to bold. (Note: You can also use underscores to italicize or bold things in a similar manner.)\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Bold and Italics Text\n\nHere is *italicized text*. And here is **bold text**.\n\nWe can also use undercores to create _italicized text_ or __bold text__.\n\nCombinations of asterisks and underscore create __*bold, italicized text*__.\n\n\n\n\n2.5.3 Other Fun Content\nYou can also add content such as unordered (i.e., bulleted) lists, ordered (i.e., numbered) lists, hyperlinks, images, footnotes and blockquotes. To see how to do check out the Markdown Basics page.\n\n\n\n2.5.4 Source versus Visual Editor\nAbove the QMD document is a set of buttons named “Source” and “Visual”. These buttons change the editong mode of the document. By default the “Source” mode is used. Clicking the “Visual” button will switch to the visual editing mode. The visual editing mode is more like Word or Google Docs—there is a toolbar to select things like bold/italics, etc.\nYou can use whatever editor you want, but In my experience, novices like to use the visual editor because it is closer to other editing programs they have used before. Experts tend to use the source editor. Not only is it faster for them, but as your documents become more complex, the visual editor has many shortcomings.\nFor that reason, I encourage you to become proficient with the source editor. To further encourage this all of the instruction I give will be in the source editor.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#back-to-the-yaml-indentation-for-options",
    "href": "01-03-introduction-to-quarto.html#back-to-the-yaml-indentation-for-options",
    "title": "2  Introduction to Quarto",
    "section": "2.6 Back to the YAML: Indentation for Options",
    "text": "2.6 Back to the YAML: Indentation for Options\nNow that you have added a few headings and some text to youe QMD file, let’s include some additional YAML. The new YAML will add a table-of-contents to your rendered document. Note that the format: html key-value pair is now separated—html is on a new line and indented.\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-location: left\n---\n\nAs you try out different YAML it is important to pay attention to the indentation. Keys that are indented have to be 2 or 4 spaces (generally 1 tab). The indentation is a way of denoting options for different keys. For example, consider the following YAML:\nNow the value “html” is indented (2 spaces) from the “format” key. This is because we have now added additional keys that set options in our HTML document. Namely adding a table of contents (“toc: true”) and a location for that table of contents (“toc-location: left”). These keys are indented an additional two spaces to indicate they are options for the “html” key.\n\nIMPORTANT\nThe spacing in the YAML really matters a lot. If you don’t have the correct spacing, the document will not render.\n\nYou can think of the indentation as indicating the hierarchical structure of elements withing the YAML. In our example the key “html” is a subcomponent of the “format” key. And, the “toc” and “toc-location” keys are subcomponents of the “html” key.\nHere is a more complex example. Don’t worry what the new keys do, but instead think about the heirarchy. Which keys are subcomponents of which other keys?\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-location: left\n    html-math-method:\n      method: mathjax\n  pdf:\n    mainfont: \"Open Sans\"\n---",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#code-chunks-integrating-r-syntax-into-your-qmd-document",
    "href": "01-03-introduction-to-quarto.html#code-chunks-integrating-r-syntax-into-your-qmd-document",
    "title": "2  Introduction to Quarto",
    "section": "2.7 Code Chunks: Integrating R Syntax into Your QMD Document",
    "text": "2.7 Code Chunks: Integrating R Syntax into Your QMD Document\nThe real bang-for-your-buck of wrting in a Quarto document is that you can integrate your R syntax directly into your text document. To do this we need to add a code chunk. To add a code chunk click the green “+C” above your quarto document and select “R”. This will add a blank R code chunk to your QMD document.\nAny R code you add inside the code chunk will be executed when you render the document.\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Code Chunk\n\n```{r}\n2 + 3 * 3\n```\n\nWhen you render the document, both the syntax and output from the syntax are printed in the rendered HTML document.\n&lt; br /&gt;\n\n2.7.1 Code Chunk Options\nWe can include options for code chunks that customize what will be printed into the HTML document. For example, in a paper, you often want to include the figure, but may not want to print all of the syntax that was used to create that plot in the final paper. Code chunk options are placed at the beginning of the code chunk and begin with #|. Similar to YAML, they are key-value pairs. For example, the code chunk below has two options: #| label: and #| echo.\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Code Chunk\n\n```{r}\n#| label: order-of-operations\n#| echo: true\n2 + 3 * 3\n```\n\nThe #| label: option should be included with every code chunk. It is a way of naming the code chunk. Here, the name of this code chunk is order-of-operations. Other code chunks can have any label you want. They should be descriptive of what code is being run in the chunk. For example, label: correlations or label: fit-model-1. Note that labels can’t include space or special characters. They also have to be unique, so a label name cannot be used again.\nThe second option, #| echo: indicates whether the syntax should be printed in the HTML document. If the value of this option is true, then the syntax is printed along with the output of the syntax. If the value of this option is false, then only the output of the syntax is printed to the HTML document.\nYou can see more code chunk options in the Quarto documentation page, Executable Options.\n\nPROTIP\nAt the beginning of a Quarto document (immediately after the YAML) include a code chunk that loads all the libraries you need and also imports any datasets for an analysis. In this code chunk I often use the following code options:\n\n#| label: setup\n#| echo: false\n#| message: false\n#| warning: false\n\nThe last two options supress any messages or warnings that are elicited when you load your libraries. The setup label is special value. The code chunk labelled setup will be run when you try to run other chunks. This is important since other code often relies on you having loaded libraries and imported the data!\n\nHere is an example of where I would include the first setup code chunk.\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Code Chunk\n\n```{r}\n#| label: setup\n#| echo: false\n#| message: false\n#| warning: false\n\n# Load libraries\nlibrary(tidyverse)\n\n# Import data\npew = read_csv(\"data/pew.csv\")\n```\n\n\nSUPER IMPORTANT\nIn EPsy 8251, when we create objects, they are stored in the R environment and we can operate on those objects. For example, in this document we will read in the pew.csv data and assign it to the object pew. We can then use functions to compute values and operate on the data.\nTrying to operate on an object that we created in the R environment, but do not create in our Quarto document will lead to an error. This is because the R environment and your Quarto document are completely independent from one another.\nIf you want to operate on an object you have to create the object in a code chunk in the Quarto document.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-03-introduction-to-quarto.html#equations",
    "href": "01-03-introduction-to-quarto.html#equations",
    "title": "2  Introduction to Quarto",
    "section": "2.8 Equations",
    "text": "2.8 Equations\n\nIMPORTANT\nEquation syntax is not R code. Therefore equation syntax should not be placed inside of a code chunk!\n\nThere are two different manners in which equations/mathematics is included in a document.\n\nDisplay equations are typeset on a separate line from the body text and are centered on the page.\nInline equations are typeset directly within the body text.\n\nFor example, here is a display equation:\n\\[\ny_i = \\beta_0 + \\beta_1(x_i) + \\epsilon_i\n\\]\nThe syntax I used to create this display equation is:\n$$\ny_i = \\beta_0 + \\beta_1(x_i) + \\epsilon_i\n$$\nHere is the same equation as an inline equation: \\(y_i = \\beta_0 + \\beta_1(x_i) + \\epsilon_i\\). Notice that in an inline equation, the equation is embedded directly in the text. To create the inline equation we embed the mathematical expression in single dollar signs ($y_i = \\beta_0 + \\beta_1(x_i) + \\epsilon_i$) rather than double dollar signs.\n\nLEARN MORE\nIf you need a reminder about how to create these, check out the “Equations” section of the Markdown Basics tutorial.\n\nThe syntax we use to create the mathematical expressions is from LaTeX. Here are a couple reference you can use:\n\nList of LaTeX mathematical symbols\nLaTeX Mathematical Symbols\n\nTo write fitted equations, you need a hat over the outcome. To do that we can use \\hat{} or \\widehat{}. I use widehat over words (e.g., \\(\\widehat{\\text{Knowledge}_i}\\)) and hat over single letters (e.g., \\(\\hat{y}_i\\)). In both, whatever you want to add the hat over is placed in between the curly braces. For example, here is how you might write the equation for the sample fitted line:\n$$\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1(x_i)\n$$\n\n\n2.8.1 Better Typesetting of Equations\nWhen we typeset equations, there are a couple things we should do:\n\nUse variable names that make sense to our reader, rather than their name in our dataset/R. For example “News Exposure” is a better name than “news”.\nVariable names should also be typeset using normal text rather than italic (the default in mathematical expressions).\n\nWe can fix both issues by including the variable names in a text environment (\\text{}). So to get the name “News Knowledge” we need to use the following in our equation:\n$$\n\\text{News Knowledge}\n$$\nOther useful environments include: \\mathit{} (italics), \\mathbf{} (bold), \\mathtt{} (typewriter text), and \\mathcal{} (caligraphy; this is useful for the “N” we use to indicate a normal distribution). If you want a space when using these, you include a tilde (~) to denote a space.\n\nHyphens need special syntax since a hyphen would be interpreted as a minus sign. In typesetting, the minus sign is longer than the hyphen symbol.\n\nIf you want to include a hyphen, we need to include it in \\mbox{}. For example, to add a hyphen in our variable name to get “Education-Level”, we use:\n$$\n\\text{Education\\mbox{-}Level}\n$$",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html",
    "href": "01-04-more-quarto.html",
    "title": "3  More Quarto",
    "section": "",
    "text": "3.1 Code Chunks for Figures\nThis document contains some additional instruction for EPsy 8252. Note that I might add to this as students hit me with questions over the course of the semester.\nCode chunks can be used to do anything you can do in R. For example we can fit regression models, create plots, compute summary measures, or mutate dummy variables into our data. For figures, there are a bunch of chunk options that are useful.\nTo label code chunks that include figures, we will append fig- to the label name. For example, we can use #| label: fig-density-knowledge to label a code chunk where we are creating a density plot of the news knowledge variable.\nWe can add a caption using the #| fig-cap: chunk option. It is also a good idea to also include the #| fig-alt: chunk option with figures. This provides alternative text for screenreaders (which is required under the new federal accessability laws). Below is an example of this:\nNotice that by labeling this chunk with fig-, the figure is numbered in the document. This allows the figure to also be cross-referenced. You can do this by using the “at” symbol (@) along with the figure’s label name. For example, to cross-reference to the density plot we created, we could use something like:",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html#code-chunks-for-figures",
    "href": "01-04-more-quarto.html#code-chunks-for-figures",
    "title": "3  More Quarto",
    "section": "",
    "text": "---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Code Chunk\n\n```{r}\n#| label: fig-density-knowledge\n#| fig-cap: \"Here is your figure caption.\"\n#| fig-alt: \"Here is alternative text for screenreaders.\"\nggplot(data = pew, aes(x = knowledge)) +\n   geom_density()\n```\n\n\n---\ntitle: \"My First Quarto Document\"\nsubtitle: \"Hello World\"\nauthor: \"Andy and Iggy\"\ndate: today\nformat: html\n---\n\n# Code Chunk\n\n```{r}\n#| label: fig-density-knowledge\n#| fig-cap: \"Here is your figure caption.\"\n#| fig-alt: \"Here is alternative text for screenreaders.\"\nggplot(data = pew, aes(x = knowledge)) +\n   geom_density()\n```\n\n\nThe distribution of news knowledge is unimodal (see @fig-density-knowledge).\n\n\n3.1.1 Figure Size\nThere are a couple other chunk options for figures that help set the aspect ratio and size of the figure in your document. The aspect ratio is the ratio of a figure’s width to its height. We set this using the #| fig-width: and #| fig-height: chunk options. (The default value for both is 6; thus the figure appears square.) Below we change these so that the figure appears wider than it is tall.\n\n```{r}\n#| label: fig-density-knowledge-2\n#| fig-cap: \"Density plot showing the distribution of news knowledge.\"\n#| fig-alt: \"Density plot showing a a unimodal distribution of news knowledge.\"\n#| fig-width: 9\n#| fig-height: 5\nggplot(data = pew, aes(x = knowledge)) +\n   geom_density()\n```\n\n\n\n\n\n\n\nFigure 3.1: Density plot showing the distribution of news knowledge.\n\n\n\n\n\n\nFYI\nIt is important to change the aspect ratio when you have legends or when you are using {patchwork} to stack or put multiple figures next to each other. Many times you need to find a good aspect ratio though trial-and-error of trying different values.\n\nThe actual size of the figure in the document is independent of the aspect ratio and can be set using the #| out-width: or #| out-height: chunk option. These options take a character string of the direct size (in HTML documents this is typically in pixels) or of the percentage of the output width/height. Here we keep the same aspect ratio, but make the figure smaller by setting the figure width to 40% of the document’s width.\n\n```{r}\n#| label: fig-density-knowledge-3\n#| fig-cap: \"Density plot showing the distribution of news knowledge.\"\n#| fig-alt: \"Density plot showing a a unimodal distribution of news knowledge.\"\n#| out-width: \"40%\"\nggplot(data = pew, aes(x = news, y = knowledge)) +\n   geom_point()\n```\n\n\n\n\n\n\n\nFigure 3.2: Density plot showing the distribution of news knowledge.\n\n\n\n\n\n\nPROTIP\nIf you set the aspect ratio of the figure, you need only set #| out-width: or #| out-height:. You don’t need to set both as the other will be determined by the aspect ratio.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html#inline-code-chunks-for-better-reproducibility",
    "href": "01-04-more-quarto.html#inline-code-chunks-for-better-reproducibility",
    "title": "3  More Quarto",
    "section": "3.2 Inline Code Chunks for Better Reproducibility",
    "text": "3.2 Inline Code Chunks for Better Reproducibility\nIn writing papers where there are results from data analyses being reported in the text, inline code chunks are boss! For example, consider writing the following sentence in an analysis of the pew.csv data.\n\nThe effect of news exposure on news knowledge is 0.327 (SE = 0.09).\n\nRather than computing these values and then transposing the values into the sentence, we can use inline code chunks to directly compute and write the values in the sentence. Recall, earlier we assigned the tidy() output into an object called lm1coef. The results from tidy() are a tibble (or data frame).\n\n# Print output\nlm1coef\n\n\n  \n\n\n\nA tibble/data frame is a combination of rows and columns. We can access different parts of the tibble by indexing the row and column that we want. For example, to access the slope standard error, which is in the 2nd row and 3rd column, we could use:\n\n# Access slope SE\nlm1coef[2, 3]\n\n\n  \n\n\n\nUnfortunately, this also comes out as a labelled tibble. If we just want the SE value we double index this. Thus to get the value of the slope SE, we use:\n\n# Access slope SE number only\nlm1coef[[2, 3]]\n\n[1] 0.08994213\n\n\nHere is how we could extract the slope value and the SE of the slope value to write our earlier sentence:\n\nThe effect of news exposure on news knowledge is `r lm1coef[[2, 2]]` (SE = `r lm1coef[[2, 3]]`).\n\nThis produces the following sentence in the rendered document:\n\nThe effect of news exposure on news knowledge is 0.3271158 (SE = 0.0899421).\n\nNote: You don’t need to assign the output to an object to index it. For example, the SE could also be extracted using: `r tidy(lm.1)[[2, 2]]`\n\n\n3.2.1 Rounding\nThere are several ways to set the rounding to two decimal places. One is to embed the computation in the round() function. For example, in the first inline chunk we could use: round(lm1coef[[2, 3]], 2).\nYou can also do this in a separate code chunk and then call the values in the inline computation (as suggested in the Quarto Computations Tutorial).",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html#multiline-equations",
    "href": "01-04-more-quarto.html#multiline-equations",
    "title": "3  More Quarto",
    "section": "3.3 Multiline Equations",
    "text": "3.3 Multiline Equations\nThere are a couple of ways to create multiline equations in a Quarto document. I tend to use the split environment from LaTeX to do this. To use this:\n\nInclude \\begin{split} and \\end{split} in the display equation.\nAt the end of each line (where you want a linebreak) include a double backslash, \\\\.\nIn each line of the multiline equation, also include the ampersand sign (&) at the point we want the multiple lines to align vertically.\n\nHere is an example:\n$$\n\\begin{split}\n\\hat{y}_i &= 3 + 4(1) \\\\\n&= 7\n\\end{split}\n$$\nAnd the resulting display equation is:\n\\[\n\\begin{split}\n\\hat{y}_i &= 3 + 4(1) \\\\\n&= 7\n\\end{split}\n\\]\nNote that the ampersand appeared immediately before the equal sign in each line of the equation, so that is where the two lines are aligned vertically (the equal signs are on top of each other). You can add as many lines to this as you want.\n\nFYI\nMultiline equations are quite useful to show your work and when you have really long single equations that need to be broken up so they fit on the page (e.g., a regression equation with many predictors).\n\n\n\n3.3.1 Spacing Out the Lines\nYou can add space between the lines of your multiline equation by including a unit of measurement between square brackets after the double backslashes.\n$$\n\\begin{split}\n\\hat{y}_i &= 3 + 4(1) \\\\[1em]\n&= 7\n\\end{split}\n$$\nHere the resulting display equation is:\n\\[\n\\begin{split}\n\\hat{y}_i &= 3 + 4(1) \\\\[1em]\n&= 7\n\\end{split}\n\\]\nHere we have added line space of 1 em. An em is a unit for measuring the width of printed work, equal to the height of the type size being used (typically the width of the letter “m”). Other common printing units include the en (the width of the letter “n”), and the ex (the width of the letter “x”).\n\n\n\n\n\n\n\n3.3.2 Typesetting 101: Hyphens, Minus Signs, and Dashes\nIn printed work the width of a hyphen, a minus sign, and dashes are all different. Quarto will correctly typeset these, but you have to indicate what you want. Below are how we indicate these in Quarto.\n\nHyphen: A hyphen is used to join two or more words together. To typeset a hyphen in Quarto, use a single hyphen; -.\nEn Dash: An en dash is used to mark ranges and to indicate the meaning “to” in phrases like “Minneapolis–St. Paul”. An en dash is slightly longer than a hyphen and is typeset in Quarto using two hyphens; --.\nEm Dash: An em dash is used to separate extra information or mark a break in a sentence. An em dash is slightly longer than an en dash and is typeset in Quarto using three hyphens; ---.\nMinus Sign: A minus sign is used in mathematical expressions (e.g., in subtraction or to indicate a negative number). To typeset a minus sign, use the inline equation with a single hyphen; $-$. When a minus sign is typeset, not only is the length of it different than a hyphen, but it also includes the correct spacing around it for mathematical typesetting.\n\nHere is a sentence incorporating each of these so you can see the differences:\nDuring the time from January–February, my three dogs enjoy playing in the snow—apparently snow is fun for canines—with their sweaters on since it is \\(-30\\)-degrees Fahrenheit outside.\nThe input for this sentence was:\nDuring the time from January--February, my three dogs enjoy playing in the snow---apparently snow is fun for canines---with their sweaters on since it is $-30$-degrees Fahrenheit outside.\nNote that you can also typeset these in Word:\n\nHyphen: On both a Mac and PC, press “hyphen”.\nEn Dash: On a Mac, press “option+hyphen key”. On a PC, press “ctrl+hyphen”.\nEm Dash: On a Mac, press “option+shift+hyphen key”. On a PC, press “alt+ctrl+hyphen”.\nMinus Sign: For a minus sign, you should use Equation Editor.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html#citations",
    "href": "01-04-more-quarto.html#citations",
    "title": "3  More Quarto",
    "section": "3.4 Citations",
    "text": "3.4 Citations\nTo add citations, we need to:\n\nCreate a BibTeX (.bib) file that holds the metadata for our references.\nSave the BibTeX file to our project directory (typically in the assets directory)\nInclude bibliography: \"location/bibliography-name.bib\" in the YAML of our QMD document\n\n\n\n3.4.1 Creating .bib Files Using Zotero\nMost reference managers (e.g., Papers, Zotero, Mendelay) can produce BibTex files. Here I will illustrate the process using Zotero.\n\nCreate a New Collection.\nDrag the references you want in your BibTeX database into this collection. For today, drag the Carmichael (1954) and Ross et al. (2020) references into this collection.\nRight-click the collection and select Export Collection.\nIn the pop-up window, change the format of the exported collection to BibTex.\nClick OK.\n\nName the BibTex file (here I named it my-bibliography.bib) and save it in the assets folder.\n\n\n\n\n\n\n\n\n\nWhen you call the BibTex file in the bibliography: key of the YAML in your QMD document, you will need to give the location of the BibTex (relative to the QMD document) and the name you just gave it. For example if you are calling a bibliography in the quarto document assignment-01.qmd that has the following directory/folder structure:\nassignment-01\n    ├── README\n    ├── assets\n       └── my-bibliography.bib\n    ├── assignment-01.qmd\n    ├── assignment-01.Rproj\n    ├── data\n       └── fertility.csv\n    ├── figs\n    └── scripts\n\n\n\n3.4.2 Including the BIB File in your YAML\nYou would then include the following in your Quarto document’s YAML:\n---\ntitle: \"Assignment 1\"\nsubtitle: \"Introduction to Quarto\"\nauthor: \"Your Group's Names\"\ndate: \"January XX, 2023\"\nformat: html\neditor: visual\nbibliography: \"assets/my-bibliography.bib\"\n---\n\n\n\n3.4.3 BibTex Files\nBibTeX files are essentially databases that store bibliographic information in a plain-text (style-independent) file. The database includes a set of references and their metadata. Here is the raw text inside an example BibTex file that includes two articles written by Carmichael (1954) and Ross et al. (2020).\n@article{carmichael_1954,\n    title = {Laziness and the Scholarly Life},\n    volume = {78},\n    number = {4},\n    journal = {The Scientific Monthly},\n    author = {Carmichael, Leonard},\n    year = {1954},\n    pages = {208--213}\n}\n\n@article{ross_2020,\n    title = {Racial disparities in police use of deadly force against unarmed individuals persist after appropriately benchmarking shooting data on violent crime rates},\n    issn = {1948-5506, 1948-5514},\n    url = {http://journals.sagepub.com/doi/10.1177/1948550620916071},\n    doi = {10.1177/1948550620916071},\n    language = {en},\n    urldate = {2020-06-24},\n    journal = {Social Psychological and Personality Science},\n    author = {Ross, Cody T. and Winterhalder, Bruce and McElreath, Richard},\n    month = jun,\n    year = {2020},\n    pages = {194855062091607}\n}\nThe citation identifiers in this example are carmichael_1954 and ross_2020. These identifiers are the first bit of text after the initial curly brace in the reference. This is how we will refer to the citations in our QMD document. (Typically these are auto-generated from our reference manager.) To determine the citiation identifiers, you can open your BibTeX file in RStudio. (Do this by using Open File... within RStudio; double-clicking on the BibTeX file will likely open it in a different application.)\n\n\n\n3.4.4 Including Citations in Your Quarto Document\nCitations go inside square brackets and are separated by semicolons. Each citation must have a key, composed of @citation_identifier from the database (no spaces between them). For example to cite the Carmichael article:\nHere is some text and a citation [@carmichael_1954].\nThis will create a citation where you included it in the text and also add the reference at the end of the document. If you want a section header for your references, include a level-1 heading called “References” at the end of your document.\nThe rendered document now includes a citation where you added this is the QMD document. The associated reference is also included at the end of the document.\n\n\n\n\n\n\n\n\n\nCitations may also include. additional text before and after the citation. In this example we have prefixed the citation with the word “see” and added a page number after the citation. The citation identifier and the text following the citation identifier are separated by a comma.\nHere is some text and a citation [see @carmichael_1954, p. 208].\nThe output of this is:\n\nHere is some text and a citation (see Carmichael, 1954, p. 208).\n\nYou can also include multiple citations. To do this we include the different citation identifiers separated by a semicolon.\nHere are multiple citations [@ross_2020; @carmichael_1954].\nThe output of this is:\n\nHere are multiple citations (Carmichael, 1954; Ross et al., 2020).\n\nWe can also change the format of the citation. For example, here we use a format common to starting a sentence with a citation. To do that we omit the square brackets.\n@carmichael_1954 suggests something is true.\nThe output of this is:\n\nCarmichael (1954) suggests something is true.\n\n\nYou can learn more on the Citations and Footnotes Help Page in the Quarto documentation.\n\n\n\n\n3.4.5 Use APA Formatted Citations and References\nBy default, citations and references are formatted using the Chicago style. To use another style, you will need to:\n\nDownload the appropriate citation style language (CSL) file. (Find many at https://zotero.org/styles)\nPlace the CSL file in the assets folder of your project’s directory.\nSpecify the name of the CSL style file in the csl: key in the QMD file’s YAML.\n\nFor example, say you have downloaded and saved the apa.csl file and placed it in the assets directory, giving the following directory/folder structure:\nassignment-01\n    ├── README\n    ├── assets\n       ├── apa.csl\n       └── my-bibliography.bib\n    ├── assignment-01.qmd\n    ├── assignment-01.Rproj\n    ├── data\n       └── fertility.csv\n    ├── figs\n    └── scripts\nYou would include the following in your YAML.\nIf, for example, you had named the BIB file my-bibliography.bib and had put this file in the assets directory, your YAML would be:\n---\ntitle: \"Assignment 1\"\nsubtitle: \"Introduction to Quarto\"\nauthor: \"Your Group's Names\"\ndate: \"January XX, 2023\"\nformat: html\neditor: visual\nbibliography: \"assets/my-bibliography.bib\"\ncsl: \"assets/apa-single-spaced.csl\"\n---\nUse of the APA CSL file not only formats the references according to APA format, but it also fixed the order of the citations in the text itself! (Even though we included the Ross citation identifier prior to the Carmichael citation identifier in the multiple references example, the APA CSL file put them in the text alphabetically!)\n\n\n\n3.4.6 Citation Extras\nOne R package that is extremely useful, especially if you are using the visual editor in RStudio for writing QMD documents, is {citr}. This package provides functionality and an RStudio Add-In to search a BibTeX-file to create and insert formatted Markdown citations into a QMD document.\n\n\n\n\n\n\n\n\n\nThe {citr} package is only available via GitHub, so you will need to install it using {remotes} or {devtools}. (See the Computational Toolkit book for a reminder on how to do this.) The directions for installing {citr} are also on the citr webpage along with instructions for using the package.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html#learn-more",
    "href": "01-04-more-quarto.html#learn-more",
    "title": "3  More Quarto",
    "section": "4.1 Learn More",
    "text": "4.1 Learn More\nYihui Xie, the creater of the RMarkdown package (which does a lot of the work in converting your QMD file to an HTML/PDF file, gave a nice trechnical talk to the American Statistical Association’s Section on Statistical Computing that explain what actually happens behind the scenes when you click the Knit or Render button in RStudio to render a RMD or QMD document.\n\nAn anatomy of R Markdown with minimal applications",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-04-more-quarto.html#references",
    "href": "01-04-more-quarto.html#references",
    "title": "3  More Quarto",
    "section": "4.2 References",
    "text": "4.2 References\n\n\n\n\n\n\nCarmichael, L. (1954). Laziness and the scholarly life. The Scientific Monthly, 78(4), 208–213.\n\n\nRoss, C. T., Winterhalder, B., & McElreath, R. (2020). Racial disparities in police use of deadly force against unarmed individuals persist after appropriately benchmarking shooting data on violent crime rates. Social Psychological and Personality Science, 194855062091607. https://doi.org/10.1177/1948550620916071",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More Quarto</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html",
    "href": "01-05-creating-tables.html",
    "title": "4  Creating Tables with gt",
    "section": "",
    "text": "5 Anatomy of a Table\nAny table is essentially a rectangular layout (rows and columns) of information. Below I show two common tables of statistical output taken from the Regression Review notes. I have also added guide-lines to show the rectangular display of information within each of these tables.\nTo illustrate how to create a table in the QMD document, we will attempt to recreate Table 1 from the Regression Review notes. We can use the fact that data frames are also rectangular displays of information to create a table in the QMD document. The data.frame() function is used to enter the cell information from each column. This is, of course, done in a code chunk.\n# Input cell information\ntab_01 = data.frame(\n  variable = c(\"Environmental policy strength\", \"Corruption\", \"Wealth\", \"Toxic waste severity\", \n               \"Democratic control\", \"Interparty competition\", \"Public environmentalism\"),\n  m = c(17.6, .32, 28.15, 3.53, .63, 39.03, 2.49),\n  sd = c(8.23, .22, 36.38, 1.14, .26, 11.40, .10),\n  min = c(4, 0, 12.78, 0, 0, 9.26, 2.31),\n  max = c(37, .98, 278.01, 5.76, 1, 56.58, 2.7)\n)\n\n# Show output\ntab_01",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#column-labels",
    "href": "01-05-creating-tables.html#column-labels",
    "title": "4  Creating Tables with gt",
    "section": "6.1 Column Labels",
    "text": "6.1 Column Labels\nColumn labels can be changed from the names of the columns used in the data frame. To change them we will use the cols_label() function. This function takes as many arguments as there are columns, each mapping a label to the original column name. Below, we change our column names to match those in Table 1.\n\n# Change Column labels\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = \"Variable\",\n    m = \"M\",\n    sd = \"SD\",\n    min = \"Min.\",\n    max = \"Max.\"\n  )\n\n\n\n\n\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\nWe can also use the md() function to include Markdown syntax to further format our labels. For example, to make the column labels italics we use the following.\n\n# Change Column labels to italics\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) \n\n\n\n  \n  \n\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\nThe names we just gave to the variables are only labels. As we refer to the columns in additional functions, we need to use their original names from the data frame.",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#column-alignment",
    "href": "01-05-creating-tables.html#column-alignment",
    "title": "4  Creating Tables with gt",
    "section": "6.2 Column Alignment",
    "text": "6.2 Column Alignment\nTo change the column alignment, we use the cols_align() function. We provide this with two arguments. The columns= argument takes a vector of column names using the c() function, and the align= argument takes a character string of \"left\", \"right\", or \"center\". Following typical formatting rules, we left align text columns and center numerical columns.\n\n# Change Column labels to italics\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |&gt;\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  )\n\n\n\n  \n  \n\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\nLearn more in the documentation: https://gt.rstudio.com/reference/cols_align.html",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#adding-a-title-and-subtitle-using-quarto",
    "href": "01-05-creating-tables.html#adding-a-title-and-subtitle-using-quarto",
    "title": "4  Creating Tables with gt",
    "section": "6.3 Adding a Title and Subtitle (Using Quarto)",
    "text": "6.3 Adding a Title and Subtitle (Using Quarto)\nIf you are creating a table within a QMD document, we will use the code chunk options to add a table number, title, and subtitle. To create these use the following code chunks:\n\nThe label: field is used to label the table and give it a table number. The key here is that the lable name must start withtbl-. In the example below, the label name I gave the table is tbl-summary-measures.\nThe tbl-cap: field is used to provide a table caption, similar to how fig-cap: is used to give a figure caption.\nThe tbl-subcap: field is used to provide a table subcaption. Not all tables need a subcaption!\nThe tbl-cap-location: field can be use to change the table caption location from top to bottom, or even have it placed in the margin.\n\nHere we show the R code chunk and chunk options to produce the table numbering and caption. (I do not use a subcaption on this table.) The resulting table, along with its numbering (which is autopopulated by Quarto), and caption is shown below.\n\n```{r}\n#| label: tbl-summary-measures\n#| tbl-cap: \"Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\"\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |&gt;\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  ) |&gt;\n  opt_align_table_header(\"left\")\n```\n\n\n\nTable 6.1: Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\n\n\n\n\n\n\n\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\n\n\n\nThe advantage to including a label: and naming it with a tbl- prefix is that it makes your tables cross-referenceable in the document. In the document’s text you can link to that table using the @ and giving the table name. For example, see Table 6.1. To do this I wrote the following in my Quarto document:\nFor example, see @tbl-summary-measures.\nSee here for more detail about cross-referencing tables, substables, figures, equations, and more!\n\n6.3.1 Changing the Formatting of the Table Numbering\nWe can change how Quarto formats the table labeling in the YAML part of the document. The default is: “Table X: Table Caption”. Here we make “Table X.” bold with a period after it, where “X” is an arabic number.\n\n---\ntitle: \"My First Quarto Document\"\nformat: html\ncrossref:\n  tbl-title: \"**Table**\"\n  tbl-labels: arabic\n  title-delim: \".\"\n---",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#adding-a-title-and-subtitle-not-using-quarto",
    "href": "01-05-creating-tables.html#adding-a-title-and-subtitle-not-using-quarto",
    "title": "4  Creating Tables with gt",
    "section": "6.4 Adding a Title and Subtitle (Not Using Quarto)",
    "text": "6.4 Adding a Title and Subtitle (Not Using Quarto)\nThe tab_header() function can be used to add a title or subtitle to your table. Here we again use the md() function to allow us to use Markdown syntax directly in the title. I also use the opt_align_table_header() function to left align the title and subtitle per APA.\n\n# Add title and subtitle\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |&gt;\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  ) |&gt;\n    tab_header(\n    title = md(\"**Table 1.** Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\"),\n  ) |&gt;\n  opt_align_table_header(\"left\")\n\n\n\n  \n  \n\n\n\nTable 1. Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70\n\n\n\n\n\n\n\nSince the title and subtitle appear on separate lines, you can take advantage of that to use the title to provide the table number and the subtitle provides the table caption if you are trying to format in APA style.\n\n\nLearn more in the documentation: https://gt.rstudio.com/reference/tab_header.html",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#fine-tuning-the-table",
    "href": "01-05-creating-tables.html#fine-tuning-the-table",
    "title": "4  Creating Tables with gt",
    "section": "6.5 Fine-Tuning the Table",
    "text": "6.5 Fine-Tuning the Table\nThe table is very close to matching the original Table 1. But, there are still a couple of things (if you are an Enneagram One) that we need to attend to. For example, we could remove the horizontal lines in the table. These lines are called “borders” and we can modify them in the tab_style() function. This is a general function that allows us to customize many parts of the table (akin to theme() in ggplot()).\nTo do this we use the style= argument and call the cell_borders() function within tab_style(). Here we remove all borders (top, bottom, left, and right) by using sides=\"all\" and setting style=NULL. The tab_style() function also requires the argument locations=. We give this argument the function cell_body() which we provide the column and row numbers that we want to remove the borders from. Since we want to keep the horizontal border associated with the first and last rows, we omit those row numbers from the rows= argument.\n\n# Add title and subtitle\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |&gt;\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  ) |&gt;\n    tab_header(\n    title = md(\"**Table 1.** Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\"),\n  ) |&gt;\n  opt_align_table_header(\"left\")  |&gt;\n  tab_style(\n    style = cell_borders(\n      sides = \"all\", \n      style = NULL\n      ),\n    locations = cells_body(\n      columns = 1:5,\n      rows = 2:6\n    )\n  )\n\n\n\n  \n  \n\n\n\nTable 1. Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\n\n\nVariable\nM\nSD\nMin.\nMax.\n\n\n\n\nEnvironmental policy strength\n17.60\n8.23\n4.00\n37.00\n\n\nCorruption\n0.32\n0.22\n0.00\n0.98\n\n\nWealth\n28.15\n36.38\n12.78\n278.01\n\n\nToxic waste severity\n3.53\n1.14\n0.00\n5.76\n\n\nDemocratic control\n0.63\n0.26\n0.00\n1.00\n\n\nInterparty competition\n39.03\n11.40\n9.26\n56.58\n\n\nPublic environmentalism\n2.49\n0.10\n2.31\n2.70",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#formatting-decimal-places",
    "href": "01-05-creating-tables.html#formatting-decimal-places",
    "title": "4  Creating Tables with gt",
    "section": "7.1 Formatting Decimal Places",
    "text": "7.1 Formatting Decimal Places\nIn these tables it is typical to format the number of decimal places to 2 for coefficients, standard errors, and t-values and to three decimal places for p-values. To do this, we will use the fmt_number() function from {gt}.\n\ntidy(lm.1) |&gt;\n  gt() |&gt;\n  cols_label(\n    term = md(\"*Variable*\"),\n    estimate = md(\"*B*\"),\n    std.error = md(\"*SE*\"),\n    statistic = md(\"*t*\"),\n    p.value = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(term),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(estimate, std.error, statistic, p.value),\n    align = \"center\"\n  ) |&gt;\n    fmt_number(\n    columns = c(estimate, std.error, statistic),\n    decimals = 2\n  ) |&gt;\n    fmt_number(\n    columns = p.value,\n    decimals = 3\n  )\n\n\n\nTable 7.2: Unstandardized regression coefficients for a model to predict variation in environmental program strength.\n\n\n\n\n\n\n\n\n\nVariable\nB\nSE\nt\np\n\n\n\n\n(Intercept)\n20.49\n3.37\n6.07\n0.000\n\n\ncorrupt\n−10.02\n5.26\n−1.91\n0.063\n\n\ntoxicwaste\n0.08\n0.82\n0.09\n0.927\n\n\n\n\n\n\n\n\n\n\n\nLearn more in the documentation: https://gt.rstudio.com/reference/fmt_number.html\n\nThe p-value for the intercept is now “0.000” after rounding to three decimal places. It is conventional in this case to report it as “&lt;0.001” rather than “0.000”. To do this we will use the sub_values() function from {gt} to search the p.value column for any value less than .001 and replace it with the text “&lt;.001”.\n\ntidy(lm.1) |&gt;\n  gt() |&gt;\n  cols_label(\n    term = md(\"*Variable*\"),\n    estimate = md(\"*B*\"),\n    std.error = md(\"*SE*\"),\n    statistic = md(\"*t*\"),\n    p.value = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(term),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(estimate, std.error, statistic, p.value),\n    align = \"center\"\n  ) |&gt;\n    fmt_number(\n    columns = c(estimate, std.error, statistic),\n    decimals = 2\n  ) |&gt;\n    fmt_number(\n    columns = p.value,\n    decimals = 3\n  ) |&gt;\n  sub_values(columns = p.value, fn = function(x) x &lt; .001, replacement = \"&lt;.001\")\n\n\n\nTable 7.3: Unstandardized regression coefficients for a model to predict variation in environmental program strength.\n\n\n\n\n\n\n\n\n\nVariable\nB\nSE\nt\np\n\n\n\n\n(Intercept)\n20.49\n3.37\n6.07\n&lt;.001\n\n\ncorrupt\n−10.02\n5.26\n−1.91\n0.063\n\n\ntoxicwaste\n0.08\n0.82\n0.09\n0.927\n\n\n\n\n\n\n\n\n\n\nWe can also use the sub_values() function to change the text in the term column to make our vasriable names more human-friendly. To do this we again specify the column name, and then indicate the values we want to replace along with the replacement text.\n\ntidy(lm.1) |&gt;\n  gt() |&gt;\n  cols_label(\n    term = md(\"*Variable*\"),\n    estimate = md(\"*B*\"),\n    std.error = md(\"*SE*\"),\n    statistic = md(\"*t*\"),\n    p.value = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(term),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(estimate, std.error, statistic, p.value),\n    align = \"center\"\n  ) |&gt;\n    fmt_number(\n    columns = c(estimate, std.error, statistic),\n    decimals = 2\n  ) |&gt;\n    fmt_number(\n    columns = p.value,\n    decimals = 3\n  ) |&gt;\n  sub_values(columns = p.value, fn = function(x) x &lt; .001, replacement = \"&lt;.001\") |&gt;\n  sub_values(columns = term, values = \"(Intercept)\", replacement = \"Constant\") |&gt;\n  sub_values(columns = term, values = \"corrupt\", replacement = \"Political Corruption\") |&gt;\n  sub_values(columns = term, values = \"toxicwaste\", replacement = \"Toxic Waste\")\n\n\n\nTable 7.4: Unstandardized regression coefficients for a model to predict variation in environmental program strength.\n\n\n\n\n\n\n\n\n\nVariable\nB\nSE\nt\np\n\n\n\n\nConstant\n20.49\n3.37\n6.07\n&lt;.001\n\n\nPolitical Corruption\n−10.02\n5.26\n−1.91\n0.063\n\n\nToxic Waste\n0.08\n0.82\n0.09\n0.927\n\n\n\n\n\n\n\n\n\n\n\nLearn more in the documentation: https://gt.rstudio.com/reference/sub_values.html",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "01-05-creating-tables.html#adding-a-footnote",
    "href": "01-05-creating-tables.html#adding-a-footnote",
    "title": "4  Creating Tables with gt",
    "section": "7.2 Adding a Footnote",
    "text": "7.2 Adding a Footnote\nWe can add a footnote to our table using the tab_footnote() function. This function takes the argument footnote= which gives the text for the footnote. It also requires the locations= argument to indicate where the actual footnote symbol should be in the table. This is specified using one of the many cells_*() helper functions (e.g., cells_body(), cells_column_labels()).\nBelow we use the cells_body() function to indicate the location of the actual footnote. This function takes the row and column where the footnote should go. Here we want to place the footnote next to the text “Political Corruption” which is in the second row of the table and the first column.\n\ntidy(lm.1) |&gt;\n  gt() |&gt;\n  cols_label(\n    term = md(\"*Variable*\"),\n    estimate = md(\"*B*\"),\n    std.error = md(\"*SE*\"),\n    statistic = md(\"*t*\"),\n    p.value = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(term),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(estimate, std.error, statistic, p.value),\n    align = \"center\"\n  ) |&gt;\n    fmt_number(\n    columns = c(estimate, std.error, statistic),\n    decimals = 2\n  ) |&gt;\n    fmt_number(\n    columns = p.value,\n    decimals = 3\n  ) |&gt;\n  sub_values(columns = p.value, fn = function(x) x &lt; .001, replacement = \"&lt;.001\") |&gt;\n  sub_values(columns = term, values = \"(Intercept)\", replacement = \"Constant\") |&gt;\n  sub_values(columns = term, values = \"corrupt\", replacement = \"Political Corruption\") |&gt;\n  sub_values(columns = term, values = \"toxicwaste\", replacement = \"Toxic Waste\") |&gt;\n  tab_footnote(\n    footnote = \"Political corruption is a dummy-coded variable (0 = no corruption; 1 = corruption).\",\n    locations = cells_body(rows = 2, columns = 1)\n  )\n\n\n\nTable 7.5: Unstandardized regression coefficients for a model to predict variation in environmental program strength.\n\n\n\n\n\n\n\n\n\nVariable\nB\nSE\nt\np\n\n\n\n\nConstant\n20.49\n3.37\n6.07\n&lt;.001\n\n\nPolitical Corruption1\n−10.02\n5.26\n−1.91\n0.063\n\n\nToxic Waste\n0.08\n0.82\n0.09\n0.927\n\n\n\n1 Political corruption is a dummy-coded variable (0 = no corruption; 1 = corruption).\n\n\n\n\n\n\n\n\n\n\n\n\nLearn more about different footnote options in the documentation: https://gt.rstudio.com/reference/tab_footnote.html",
    "crumbs": [
      "Introduction to Statistical Reproducibility",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating Tables with gt</span>"
    ]
  },
  {
    "objectID": "02-00-likelihood.html",
    "href": "02-00-likelihood.html",
    "title": "Likelihood",
    "section": "",
    "text": "Many of the advanced statistical methods use estimation and testing methods based on likelihood. In this section we introduce likelihood for testing and estimating model parameters. To understand likelihood, you need to have some knowledge of probability density and logarithms. For this reason, we begin this section by introducing these two foundational ideas.",
    "crumbs": [
      "Likelihood"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html",
    "href": "02-01-mathematical-foundations-probability-density.html",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "",
    "text": "5.1 Terminology and Basic Ideas of Probability Distributions\nUnderstanding ideas about probability distributions (especially continuous probability distributions) is essential for understanding likelihood.\nA probability distribution is simply a mapping of every outcome in the sample space to a probability. (It is conventional to use x to denote the outcomes.) This mapping can take many forms such a list, a plot, or an equation. All are reasonable to define the probability distribution, although some of these are more typical for discrete variables (e.g., list, plot), while others are more typical for continuous distributions (e.g., plot, equation). Here are a couple different probability distributions:\nFigure 5.1: An example of a discrete and continuous probability distribution.\nIn general, the probability of an outcome (p(x)) is defined as the number of times an outcome appears in the sample space divided by the total number of outcomes. So, for example, in the probability experiment of flipping a fair coin twice, the sample space is:\n\\[\nS = \\{HH,~HT,~TH,~TT\\}\n\\]\nThe probability of a head and a tail (assuming order doesn’t matter) is:\n\\[\np(HT~\\mathrm{or}~TH) = \\frac{2}{4} = 0.5\n\\] This falls apart when we move to continuous distributions. For example, consider finding the probability of a zero occurring in the standard normal distribution given in Figure 6.1. The sample space includes every outcome possible and is:\n\\[\nS = \\{-\\infty, ~ +\\infty\\}\n\\] so the probability of zero (which occurs once in that sample space) is:\n\\[\np(0) = \\frac{1}{\\infty} \\approx 0\n\\]\nLikewise the probability of 1 or 2 is also:\n\\[\n\\begin{split}\np(1) = \\frac{1}{\\infty} \\approx 0 \\\\\np(2) = \\frac{1}{\\infty} \\approx 0\n\\end{split}\n\\]\nIn fact the probability of every possible outcome is 0. The problem is that if we add all the probabilities together, we have to get 1 (Law of Probability), and in this case we don’t; the sum of \\(0 + 0 + \\ldots + 0 = 0\\). Because of this, in continuous distributions, we typically compute the probability of a range of values. For example, in our standard normal distribution:\n\\[\n\\begin{split}\np(x \\leq 0) = 0.5 \\\\\np(-1 \\leq x \\leq 1) = 0.68 \\\\\np(-2 \\leq x \\leq 2) = 0.95\n\\end{split}\n\\]",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#terminology-and-basic-ideas-of-probability-distributions",
    "href": "02-01-mathematical-foundations-probability-density.html#terminology-and-basic-ideas-of-probability-distributions",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "",
    "text": "5.1.1 Probability Density\nIn a continuous distribution we also need to account be able to talk about the fact that some outcomes are more likely than other outcomes. For example, in our standard normal distribution outcomes near zero are more probable than outcomes near 1, which are more probable than outcomes near 2, etc. Since we can’t use probability to do this (remember the probability of each outcome is the same, namely 0), we use something called probability density. This is akin to a relative probability, so outcomes with a higher probability density are more likely than outcomes with a lower probability density.\nThe mapping of all the outcomes to their probability densities is called a probability density function (PDF). Thus the equation or “bell-shaped” curve describing the standard normal distribution in Figure 6.1 is technically a PDF Here are some laws governing PDFs:\n\nProbability densities are always positive.\nThe probability of an outcome x between a and b equals the integral (area under the curve) between a and b of the probability density function. That is:\n\n\\[\np(a \\leq x \\leq b) = \\int_a^b p(x) dx\n\\]\n\nThe area under the curve from negative infinity to positive infinity is 1. That is:\n\n\\[\np(-\\infty \\leq x \\leq +\\infty) = \\int_{-\\infty}^{+\\infty} p(x) = 1\n\\]\nNext we will look at the PDF for a normal distribution.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#pdf-for-a-normal-gaussian-distribution",
    "href": "02-01-mathematical-foundations-probability-density.html#pdf-for-a-normal-gaussian-distribution",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.2 PDF for a Normal (Gaussian) Distribution",
    "text": "5.2 PDF for a Normal (Gaussian) Distribution\nThe probability density function (PDF) of a normal distribution is mathematically defined as:\n\\[\np(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]\n\\]\nfor \\(-\\infty \\leq x \\leq \\infty\\).\nTo calculate the probability density, we need three pieces of information: (1) the outcome (x-value) for which we want to determine the probability density, (2) the mean (\\(\\mu\\)) of the normal distribution x is a member of, and (3) the standard deviation (\\(\\sigma\\)) of the normal distribution x is a member of. Then, we can compute the probability density (\\(p(x)\\)) for a particular \\(x\\) value by using the equation.\nAs an example, consider a normal distribution with a mean of 50, and a standard deviation of 10. The probability density for \\(x=65\\) can be found using,\n\\[\n\\begin{split}\np(65) &= \\frac{1}{10\\sqrt{2\\pi}}\\exp\\left[-\\frac{(65-50)^2}{2\\times10^2}\\right] \\\\[1ex]\n&= 0.01295176\n\\end{split}\n\\]\nUsing R, we can carry out the computation,\n\n# Compute the probability density of x=65 in N(50,10)\n(1 / (10 * sqrt(2 * pi))) * exp(-(225) / 200)\n\n[1] 0.01295176\n\n\nThere is also a more direct way to compute this using the dnorm() function. This function computes the density of x from a normal distribution with a specified mean and sd.\n\n# Compute the probability density of x=65 in N(50,10)\ndnorm(x = 65, mean = 50, sd = 10)\n\n[1] 0.01295176\n\n\nSymbolically, we might write\n\\[\nP\\bigg(x=65 \\mid \\mathcal{N}(50,10)\\bigg) = 0.01295176\n\\]\nwhich is read:\n\nThe probability density of \\(x=65\\) GIVEN the normal distribution having a mean of 50 and standard deviation of 10 is equal to 0.013.\n\nNote that the probability density for a value is not only a function of x, but also depends on the mean and standard deviation of the normal distribution. For example, the probability density of \\(x=65\\) in the normal distribution having a mean of 30 and standard deviation of 20 is a different value than the probability density we found earlier.\n\n# Compute the probability density of x=65 in N(30,20)\ndnorm(x = 65, mean = 30, sd = 20)\n\n[1] 0.004313866\n\n\nHere,\n\\[\nP\\bigg(x=65 \\mid \\mathcal{N}(30,20)\\bigg) = 0.004313866\n\\]\nIn general, when we think about the normal distribution, we are thinking about the mapping of each x-value from \\(-\\infty\\) to \\(+\\infty\\) to its associated probability density. Rather than list each of these mappings out, we can create a plot of these mappings. This plot gives us the familiar “bell shape”. Theoretically this plot is the graphical depiction of the PDF.\n\n\nCode\n# Load library\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\n# Create dataset\nfig_02 = data.frame(\n  X = seq(from = -40, to = 90, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y1 = dnorm(x = X, mean = 50, sd = 10),\n    Y2 = dnorm(x = X, mean = 30, sd = 20)\n    )\n\n# Create plot\nggplot(data = fig_02, aes(x = X, y = Y1)) +\n  geom_line(color = \"#0085af\", linetype = \"solid\") +\n  geom_line(aes(y = Y2), linetype = \"dashed\", color = \"#c62f4b\") +\n  xlab(\"X\") +\n  ylab(\"Probability density\") +\n  theme_light() +\n  geom_point(x = 65, y = 0.01295176,  size = 3, color = \"#0085af\") +\n  geom_point(x = 65, y = 0.004313866, size = 3, color = \"#c62f4b\")\n\n\n\n\n\n\n\n\nFigure 5.2: Plot of the probability density function (PDF) for a \\(\\mathcal{N}(50,10)\\) distribution (blue, solid line) and for a \\(\\mathcal{N}(30,20)\\) distribution (red, dashed line). The probability density value for \\(x=65\\) is also displayed on both PDFs.\n\n\n\n\n\nOf course, the PDF is different for normal distributions with different means (\\(\\mu\\)) or standard deviations (\\(\\sigma\\)). This implies that there is not one normal distribution, but rather an infinite number of normal distributions, each with a different mean or standard deviation. (We refer to the normal distribution as a “family” of distributions.)\nTo completely define the PDF we need to specify the mean and standard deviation we are using to compute the probability densities. Specifying these values is referred to as parameterizing the distribution1.\n\n\n5.2.1 Other Useful R Functions for Working with Normal Probability Distributions\nWe use dnorm() when we want to compute the probability density associated with a particular x-value in a given normal distribution. There are three other functions that are quite useful for working with the normal probability distribution:\n\npnorm() : To compute the probability (area under the PDF)\nqnorm() : To compute the \\(x\\) value given a particular probability\nrnorm() : To draw a random observation from the distribution\n\nEach of these function also requires the arguments mean= and sd=. Below we will examine how to use each of these additional functions.\n\n\n\n5.2.2 pnorm(): Computing Cumulative Probability Density\nThe function pnorm() computes the area under the PDF curve from \\(-\\infty\\) to some x-value. (Sometimes this is referred to as the cumulative probability density of x.) It is important to note that the PDF is defined such that the entire area under the curve is equal to 1. Because of this, we can also think about using area under the curve as an analog to probability in a continuous distribution.\nFor example, we might ask about the probability of observing an x-value that is less than or equal to 65 given it is from a \\(\\mathcal{N}(50,10)\\) distribution. Symbolically, we want to find:\n\\[\nP\\bigg(x \\leq 65 \\mid \\mathcal{N}(50,10)\\bigg)\n\\]\nThis is akin to finding the proportion of the area under the \\(\\mathcal{N}(50,10)\\) PDF that is to the left of 65. The figure below shows a graphical depiction of the cumulative probability density for \\(x=65\\).\n\n\nCode\n# Create dataset\nfig_03 = data.frame(\n  X = seq(from = 10, to = 90, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = dnorm(x = X, mean = 50, sd = 10)\n    )\n\n# Filter out X&lt;=65\nshaded = fig_03 %&gt;%\n  filter(X &lt;= 65)\n\n# Create plot\nggplot(data = fig_03, aes(x = X, y = Y)) +\n  geom_ribbon(data = shaded, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"X\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\n\n\n\nFigure 5.3: Plot of the probability density function (PDF) for a N(50,10) distribution. The area that is shaded grey (relative to the total area under the PDF) represents the cumulative probability density for x=65.\n\n\n\n\n\nWe can compute the cumulative probability density using the pnorm() function. The “p” stand for “probability”.\n\n# Find P(x&lt;=65 | N(50,10) )\npnorm(q = 65, mean = 50, sd = 10)\n\n[1] 0.9331928\n\n\nWe can interpret this as:\n\nThe probability of observing an x-value that is less than or equal to 65 (if it is drawn from a normal distribution with a mean of 50 and standard deviation of 10) is 0.933.\n\nIn mathematics, the area under a curve is called an integral. The grey-shaded area in the previous figure can also be expressed as an integral of the probability density function:\n\\[\n\\int_{-\\infty}^{65} p(x) dx\n\\]\nwhere \\(p(x)\\) is the PDF for the normal distribution.\nThe most common application for finding the cumulative density is to compute a p-value. The p-value is just the area under the distribution (curve) that is AT LEAST as extreme as some observed value. For example, assume we computed a test statistic of \\(z=2.5\\), and were evaluating whether this was different from 0 (two-tailed test). Graphically, we want to determine the proportion of the area under the PDF that is shaded grey in the figure below.\n\n\nCode\n# Create data\nfig_04 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = dnorm(x = X, mean = 0, sd = 1)\n    )\n\n# Filter data for shading\nshade_01 = fig_04 %&gt;%\n  filter(X &gt;= 2.5)\n\nshade_02 = fig_04 %&gt;%\n  filter(X &lt;= -2.5)\n\n# Create plot\nggplot(data = fig_04, aes(x = X, y = Y)) +\n  geom_ribbon(data = shade_01, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_ribbon(data = shade_02, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"z\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\n\n\n\nFigure 5.4: Plot of the probability density function (PDF) for the standard normal distribution (\\(M=0\\), \\(SD=1\\)). The cumulative density representing the p-value for a two-tailed test evaluating whether \\(\\mu=0\\) using an observed mean of 2.5 is also displayed.\n\n\n\n\n\nIf the distribution of the test statistic is normally distributed, we can use pnorm() to compute the p-value. If we assume the test statistic, z, has been scaled to use standardized units, the standard deviation we use in pnorm() will be sd=1. The mean is based on the value being tested in the null hypothesis. In most null hypotheses, we are testing a difference from 0 (e.g., \\(H_0: \\mu=0\\), \\(H_0: \\beta=0\\)), so we would use mean=0 in the pnorm() function.\nRemember, pnorm() computes the proportion of the area under the curve TO THE LEFT of a particular value. Here we will compute the area to the left of \\(-2.5\\) and then double it to produce the actual p-value. (We can double it because the normal distribution is symmetric so the area to the left of \\(-2.5\\) is the same as the area to the right of \\(+2.5\\).)\n\n# Compute the p-value based on z=2.5\n2 * pnorm(q = -2.5, mean = 0, sd = 1)\n\n[1] 0.01241933\n\n\nWe interpret this p-value as:\n\nThe probability of observing a statistic at least as extreme as 2.5, assuming the null hypothesis is true, is 0.012. This is evidence against the null hypothesis since the data are inconsistent with the assumed hypothesis.\n\n\n\n\n5.2.3 qnorm(): Computing Quantiles\nThe qnorm() function is essentially the inverse of the pnorm() function. The pnorm() function computes the cumulative probability GIVEN a particular quantile (x-value). The qnorm() function computes the quantile GIVEN a cumulative probability. For example, in the \\(\\mathcal{N}(50, 10)\\) distribution, half of the area under the PDF is below the x-value (quantile) of 50.\nTo use the qnorm() function to give the x-value (quantile) that defines the lower 0.5 of the area under the \\(\\mathcal{N}(50, 10)\\) PDF, the syntax would be:\n\n# Find the quantile that has a cumulative density of 0.5 in the N(50, 10) distribution\nqnorm(p = 0.5, mean = 50, sd = 10)\n\n[1] 50\n\n\n\n\n\n5.2.4 rnorm(): Generating Random Observations\nThe rnorm() function can be used to generate random observations drawn from a specified normal distribution. Aside from the mean= and sd= arguments, we also need to specify the number of observations to generate by including the argument n=. For example, to generate 15 observations drawn from a \\(\\mathcal{N}(50,10)\\) distribution we would use the following syntax:\n\n# Generate 15 observations from N(50,10)\nset.seed(100)\nrnorm(n = 15, mean = 50, sd = 10)\n\n [1] 44.97808 51.31531 49.21083 58.86785 51.16971 53.18630 44.18209 57.14533\n [9] 41.74741 46.40138 50.89886 50.96274 47.98366 57.39840 51.23380\n\n\nThe set.seed() function sets the state of the random number generator used in R so that the results are reproducible. If you don’t use set.seed() you will get a different set of observations each time you run rnorm(). Here we set the starting seed to 100, but you can set this to any integer you want.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#students-t-distribution",
    "href": "02-01-mathematical-foundations-probability-density.html#students-t-distribution",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.3 Student’s t-Distribution",
    "text": "5.3 Student’s t-Distribution\nThe PDF of Student’s t-distribution looks similar to the PDF for a standard normal distribution. In the figure below, Student’s t-distribution is depicted with a solid, black line and the standard normal distribution (\\(M=0\\), \\(SD=1\\)) is depicted with a dotted, red line.\n\n\nCode\n# Create data\nfig_05 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n) %&gt;%\n  mutate(\n    Y_t = dt(x = X, df = 5),\n    Y_norm = dnorm(x = X, mean = 0, sd = 1)\n    ) \n\n# Create plot\nggplot(data = fig_05, aes(x = X, y = Y_t)) +\n  geom_line() +\n  geom_line(aes(y = Y_norm), color = \"red\", linetype = \"dotted\") +\n  xlab(\"t\") +\n  ylab(\"Probability density\") +\n  theme_light() +\n  geom_vline(xintercept = 0)\n\n\n\n\n\n\n\n\nFigure 5.5: Plot of the probability density function (PDF) for both the standard normal distribution (dotted, red line) and Student’s \\(t(5)\\) distribution (solid, black line).\n\n\n\n\n\n\nBoth the standard normal distribution and Student’s t-distribution are symmetric distributions.\nBoth the standard normal distribution and Student’s t-distribution have a mean (expected value) of 0.\nThe standard deviation for Student’s t-distribution is, however, larger than the standard deviation for the standard normal distribution (\\(SD&gt;1\\)). You can see this in the distribution because the tails in Student’s t-distribution are fatter (more error) than the standard normal distribution.\n\nIn practice, we often use Student’s t-distribution rather than the standard normal distribution in our evaluations of sample data. This is because the increased error (i.e., standard deviation) associated with Student’s t-distribution better models the additional uncertainty associated with having incomplete information (i.e., a sample rather than the entire population).\nStudent’s t-distribution also constitutes a family of distributions; there is not a single t-distribution. The specific shape (and thus probability density) is defined by a parameter called the degrees of freedom (df). The plot below shows the standard normal distribution (purple) and four t-distributions with varying df-values. The means and standard deviations for each of these distributions is also provided in a table.\n\n\nCode\n# Create data for t(3)\ndf_03 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = dt(x = X, df = 3),\n    df = \"03\"\n    ) \n\n# Create data for t(5)\ndf_05 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = dt(x = X, df = 5),\n    df = \"05\"\n    )\n\n# Create data for t(10)\ndf_10 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n) %&gt;%\n   mutate(\n    Y = dt(x = X, df = 10),\n    df = \"10\"\n    )\n\n# Create data for t(25)\ndf_25 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = dt(x = X, df = 25),\n    df = \"25\"\n    )\n\n# Create data for standard normal\nz = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = dnorm(x = X, mean = 0, sd = 1),\n    df = \"Standard normal\"\n    )\n\n# Combine all datasets into one\nfig_06 = rbind(df_03, df_05, df_10, df_25, z)\n\n# Create plot\nggplot(data = fig_06, aes(x = X, y = Y, color = df, linetype = df)) +\n  geom_line() +\n  xlab(\"t\") +\n  ylab(\"Probability density\") +\n  theme_light() +\n  geom_vline(xintercept = 0) +\n  ggsci::scale_color_d3() +\n  scale_linetype_manual(values = 5:1)\n\n\n\n\n\n\n\n\nFigure 5.6: Plot of several t-Distributions with differing degrees of freedom.\n\n\n\n\n\n\n\nCode\n# Load library\nlibrary(gt)\n\n# Create data from table\ntab_01 = data.frame(\n  df = c(\"3\", \"5\", \"10\", \"25\", \"z\"),\n  M = 0.00,\n  SD = c(2, 1.5, 1.22, 1.08, 1.00)\n)\n\n# Create table\ntab_01 |&gt;\n  gt() |&gt;\n  cols_label(\n    df = md(\"*df*\"),\n    M = md(\"*M*\"),\n    SD = md(\"*SD*\")\n  ) |&gt;\n  cols_align(\n    columns = c(df, M, SD),\n    align = \"center\"\n  )\n\n\n\n\nTable 5.1: Means and standard deviations for four t-Distributions and the standard normal distribution.\n\n\n\n\n\n\n\n\n\ndf\nM\nSD\n\n\n\n\n3\n0\n2.00\n\n\n5\n0\n1.50\n\n\n10\n0\n1.22\n\n\n25\n0\n1.08\n\n\nz\n0\n1.00\n\n\n\n\n\n\n\n\n\n\nIf we compare the means and standard deviations for these distributions, we find that the mean for all the t-distributions is 0, same as the standard normal distribution. All t-distributions are unimodal and symmetric around zero. The standard deviation for every t-distribution is higher than the standard deviation for the standard normal distribution. Mathematically, the variance for the t-distribution is:\n\\[\n\\sigma^2(t) = \\frac{\\nu}{\\nu-2}\n\\]\nwhere \\(\\nu\\) (the Greek letter nu) is the degrees of freedom. (Note that \\(\\nu \\geq 2\\).) Examining this formula, we find that Student t-distributions with higher df values have less variation. When \\(\\nu=+\\infty\\), the variance approaches 1, which is the same as the standard normal distribution.\nThere are four primary functions for working with Student’s t-distribution:\n\ndt() : To compute the probability density (point on the curve)\npt() : To compute the cumulative density (area under the PDF)\nqt() : To compute the quantile value given a particular probability\nrt() : To draw a random observation from the distribution\n\nEach of these requires the argument df=.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#the-f-distribution",
    "href": "02-01-mathematical-foundations-probability-density.html#the-f-distribution",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.4 The F-distribution",
    "text": "5.4 The F-distribution\nThe F-distribution, like the t-distribution, constitutes a family of distributions. They are positively skewed and generally have a lower-limit of 0. To parameterize an F-distribution we need two parameters, namely \\(\\nu_1\\) and \\(\\nu_2\\). These are both degrees of freedom. The exact shape of the F-distribution s governed by the two degrees of freedom parameters. The figure below shows several F-distributions with different degrees of freedom.\n\n\nCode\n# Create data for F(1,10)\nf1 = data.frame(\n  X = seq(from = 0, to = 10, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = df(x = X, df1 = 1, df2 = 10),\n    df = \"F(1, 10)\"\n    ) \n\n# Create data for F(5,25)\nf2 = data.frame(\n  X = seq(from = 0, to = 10, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = df(x = X, df1 = 5, df2 = 25),\n    df = \"F(5, 25)\"\n    )\n\n# Create data for F(15,30)\nf3 = data.frame(\n  X = seq(from = 0, to = 10, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = df(x = X, df1 = 15, df2 = 30),\n    df = \"F(15, 30)\"\n    )\n\n# Create data for F(30,60)\nf4 = data.frame(\n  X = seq(from = 0, to = 10, by = 0.01)\n) %&gt;%\n  mutate(\n    Y = df(x = X, df1 = 30, df2 = 60),\n    df = \"F(30, 60)\"\n    )\n\n# Combine all datasets into one\nfig_07 = rbind(f1, f2, f3, f4)\n\n# Create plot\nggplot(data = fig_07, aes(x = X, y = Y, color = df, linetype = df)) +\n  geom_line() +\n  xlab(\"F\") +\n  ylab(\"Probability density\") +\n  theme_light() +\n  ggsci::scale_color_d3(name = \"\") +\n  scale_linetype_manual(name = \"\", values = 1:4)\n\n\n\n\n\n\n\n\nFigure 5.7: Plot of several F-Distributions with differing degrees of freedom.\n\n\n\n\n\nThe expected value (mean) and standard deviation of the F-distribution is:\n\\[\n\\begin{split}\nE(F) &= \\frac{\\nu_2}{\\nu_2 - 2} \\\\[1ex]\n\\sigma^2(F) &= \\frac{2\\nu_2^2(\\nu_1 + \\nu_2 - 2)}{\\nu_1(\\nu_2-2)^2(\\nu_2-4)}\n\\end{split}\n\\]\nwhere \\(\\nu_2 &gt; 2\\) for the mean and \\(\\nu_2 &gt; 4\\) for the variance.\nFrom these formulas we can see that as \\(\\nu_2 \\rightarrow +\\infty\\) the mean of the F-distribution approaches 1. We can also see that the variation in the F-distribution is a function of both parameters and the variance decreases as either parameter gets larger.\nThe means and standard deviations for our four example F-distributions are given in the table below.\n\n\nCode\n# Create data from table\ntab_02 = data.frame(\n  df1 = c(1, 15, 30, 5),\n  df2 = c(10, 30, 60, 25)\n  ) %&gt;%\n  mutate(       \n    M = df2 / (df2 - 2),\n    SD = sqrt((2*df2^2*(df1 + df2 - 2)) / (df1*(df2-2)^2*(df2-4)))\n  )\n\n# Create table\ntab_02 |&gt;\n  gt() |&gt;\n  cols_label(\n    df1 = md(\"*df1*\"),\n    df2 = md(\"*df2*\"),\n    M = md(\"*M*\"),\n    SD = md(\"*SD*\")\n  ) |&gt;\n  cols_align(\n    columns = c(df1, df2, M, SD),\n    align = \"center\"\n  ) |&gt;\n  fmt_number(\n    columns = c(M, SD),\n    decimals = 2\n  )\n\n\n\n\nTable 5.2: Means and standard deviations for four F-distributions.\n\n\n\n\n\n\n\n\n\ndf1\ndf2\nM\nSD\n\n\n\n\n1\n10\n1.25\n2.17\n\n\n15\n30\n1.07\n0.50\n\n\n30\n60\n1.03\n0.33\n\n\n5\n25\n1.09\n0.79\n\n\n\n\n\n\n\n\n\n\nBecause there is no negative side of the distribution, when we use the F-distribution to compute a p-value, we only compute the cumulative density GREATER THAN OR EQUAL TO the value of the F-statistic. For example, the figure below shows the \\(F(5,25)\\)-distribution and the shaded area corresponds to the p-value for an observed F-statistic of 2.\n\n\nCode\n# Create data for F(5,25)\nfig_08 = data.frame(\n  X = seq(from = 0, to = 10, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = df(x = X, df1 = 5, df2 = 25)\n    )\n\n# Filter data for shading\nshade_01 = fig_08 %&gt;%\n  filter(X &gt;= 2)\n\n# Create plot\nggplot(data = fig_08, aes(x = X, y = Y)) +\n  geom_ribbon(data = shade_01, ymin = 0, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"F\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\n\n\n\nFigure 5.8: Plot of the probability density function (PDF) for the \\(F(5,25)\\)-distribution. The cumulative density representing the p-value associated with an F-statistic of 2 is shaded in grey.\n\n\n\n\n\nHere we can use the pf() function to compute the p-value. Remember, pf() computes the proportion of the area under the curve TO THE LEFT of a particular value. Here we will need to compute the area to the RIGHT of \\(+2\\).\n\n# Compute the p-value based on F(5,25)=2\n1 - pf(q = 2, df1 = 5, df2 = 25)\n\n[1] 0.1134803\n\n\nThe probability of observing an F-statistic at least as extreme as 2, assuming the null hypothesis is true, is 0.113. This is not evidence against the null hypothesis since the data are consistent with the assumed hypothesis.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#creating-a-pdf-and-adding-shading-in-a-ggplot",
    "href": "02-01-mathematical-foundations-probability-density.html#creating-a-pdf-and-adding-shading-in-a-ggplot",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.5 Creating a PDF and Adding Shading in a ggplot",
    "text": "5.5 Creating a PDF and Adding Shading in a ggplot\nOne method to create the PDF for a distribution using ggplot() is to create a dataset that includes a sequence of X-values for which you want to show the PDF and compute the probability density for each of those values. Then you can use geom_line() to connect those probability densities.\nFor example, say we want to create the PDF of the \\(F(15, 100)\\)-distribution. Here I will define this for F-values from 0 to 10. (These are the x-values in my plot.) Then I need to compute the probability densities for each of those values using pf().\n\n# Create F-value and compute probability densities\nfig_09 = data.frame(\n  X = seq(from = 0, to = 10, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = df(x = X, df1 = 5, df2 = 25)\n    )\n\n# View data\nhead(fig_09)\n\n\n  \n\n\n\nThen we can plot the Y versus the X values and connect them using geom_line().\n\nggplot(data = fig_09, aes(x = X, y = Y)) +\n  geom_line() +\n  xlab(\"F\") +\n  ylab(\"Probability density\") +\n  theme_light() \n\n\n\n\n\n\n\nFigure 5.9: Plot of the probability density function (PDF) for the \\(F(5,25)\\)-distribution.\n\n\n\n\n\nTo add shading under the curve we need to create a new dataset that only includes the X and Y values in the shaded region. For example to shade the area under the PDF where \\(F &gt; 2\\), we need to create a new dataset where the X values are greater than 2. Below I do this using filter() and store the data in an object called shade_01.\n\n# Filter data included in the shaded region\nshade_09 = fig_09 %&gt;%\n  filter(X &gt;= 2)\n\n# View data\nhead(shade_09)\n\n\n  \n\n\n\nWe re-draw the PDF and then use geom_ribbon() to add shading. This layer requires us to define the area we want shaded. Here we want to shade from \\(Y=0\\) to \\(Y=\\) the probability density for each of the X values in the shading data. To carry this out we need to define x=, ymin= and ymax=.\nSince the X values are in a column called X and the probability densities are in a column called Y in the shaded dataa frame, we can call x=X and ymax=Y in the aes() of geom_ribbon(). The ymin= value of 0 is not a column in the data frame, so it is specified OUTSIDE the aes() function. We can then also set characteristics like color of the shading (color=) and transparency level (alpha=). Finally, to ensure that geom_ribbon() is shading only the region we want, we set data=shade_01.\n\n# Create plot\nggplot(data = fig_09, aes(x = X, y = Y))  +\n  geom_ribbon(data = shade_09, ymin = 0, aes(x = X, ymax = Y), \n              color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"F\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\n\n\nFigure 5.10: Plot of the probability density function (PDF) for the \\(F(5,25)\\)-distribution. The cumulative density representing the p-value associated with an F-statistic of 2 is shaded in grey.\n\n\n\n\n\n\nYou can click the Code button to see the underlying syntax for many of the figures created in this document.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#probability-distributions-in-regression",
    "href": "02-01-mathematical-foundations-probability-density.html#probability-distributions-in-regression",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.6 Probability Distributions in Regression",
    "text": "5.6 Probability Distributions in Regression\nTo illustrate how probability distributions are used in practice, we will will use the data in the file riverview.csv (see the data codebook for more information about these data) and fit a regression model that uses education level and seniority to predict variation in employee income. Some (most?) of this content should also be review from EPsy 8251.\n\n# Load libraries\nlibrary(broom)\nlibrary(tidyverse)\n\n# Import data\ncity = read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/riverview.csv\")\n\nRows: 32 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): gender, party\ndbl (3): education, income, seniority\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View data\ncity\n\n\n  \n\n\n\nTo begin, we will fit a multiple regression model that uses level of education and seniority to predict variation in employee’s incomes. The model is:\n\\[\n\\begin{split}\n\\mathrm{Income}_i &= \\beta_0 + \\beta_1(\\mathrm{Education}_i) + \\beta_2(\\mathrm{Seniority}_i) + \\epsilon_i \\\\\n& \\mathrm{where}\\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_{\\epsilon})\n\\end{split}\n\\]\nWe have four unknowns in this model that need to be estimated: \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\sigma^2_{\\epsilon}\\). This last unknown is the error (or residual) variance. As a side note, when you report results from a fitted regression model, you should report the estimated residual variance (or residual standard error) along with the coefficient estimates.\nAside from the estimates for the coefficients and RSE, we are also generally interested in the estimates of uncertainty for the coefficients (i.e., the standard errors). These uncertainty estimates also allow us to carry out hypothesis tests on the effects included in the model.\n\n# Fit regression model\nlm.1 = lm(income ~ 1 + education + seniority, data = city)\n\nIn practice, all of the estimates, SEs, and inferential output are available using functionality in R. For example, the model-level output, including \\(R^2\\), the F-statistic, the model and residual df, and the residual standard error are all outputted from the glance() function from the {broom} package. We can also partition the variation using the anova() function.\n\n# Model-level output\nglance(lm.1)\n\n\n  \n\n\n# Partition the variation\nanova(lm.1)\n\n\n  \n\n\n\nSimilarly the tidy() function from the {broom} package outputs coefficient-level output, including the coefficients, standard errors, t-values, and associated p-values.\n\n# Coefficient-level output\ntidy(lm.1, conf.int = TRUE)\n\n\n  \n\n\n\nOur goal here is to understand how the probability distributions play a role in determining some of these values.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#model-level-inference-the-f-distribution",
    "href": "02-01-mathematical-foundations-probability-density.html#model-level-inference-the-f-distribution",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.7 Model-Level Inference: The F-Distribution",
    "text": "5.7 Model-Level Inference: The F-Distribution\nAt the model-level, we are interested in whether or not the model (as a whole) explains variation in the outcome. Our estimate of how much variation the model explains is based on partitioning the total variation of the outcome (\\(\\mathrm{SS}_{\\mathrm{Total}}\\)) into that which is explained by the model (\\(\\mathrm{SS}_{\\mathrm{Model}}\\)) and that which is not explained by the model (\\(\\mathrm{SS}_{\\mathrm{Residual}}\\)). From the anova() output:\n\\[\n\\begin{split}\n\\mathrm{SS}_{\\mathrm{Model}} &= 4147.3 + 722.9 = 4870.2\\\\[1ex]\n\\mathrm{SS}_{\\mathrm{Residual}} &= 1695.3\\\\[1ex]\n\\mathrm{SS}_{\\mathrm{Total}} &= 4870.2 + 1695.3 = 6565.5\n\\end{split}\n\\]\nThen we compute the a statistic called \\(R^2\\) by computing the ratio of the explained variation to the total variation.\n\\[\nR^2 = \\frac{4870.2}{6565.5} = 0.742\n\\]\nThe model (differences in education and seniority levels) explains 74.2% of the variation in employee’s incomes in the sample. We might also want to test whether this is more variation than we expect because of sampling error. To do this we want to test the hypothesis that:\n\\[\nH_0: \\rho^2 = 0\n\\]\nTo evaluate this we convert the sample \\(R^2\\) value into a test statistic using,\n\\[\nF = \\frac{R^2}{1 - R^2} \\times \\frac{\\mathit{df}_{\\mathrm{Error}}}{\\mathit{df}_{\\mathrm{Model}}}\n\\]\nThe degrees of freedom (df) is also partitioned in the anova() output:\n\\[\n\\begin{split}\n\\mathrm{df}_{\\mathrm{Model}} &= 1 + 1 = 2\\\\[1ex]\n\\mathrm{df}_{\\mathrm{Residual}} &= 29\\\\[1ex]\n\\mathrm{df}_{\\mathrm{Total}} &= 2 + 29 = 31\n\\end{split}\n\\]\nConverting our \\(R^2\\) value of 0.742 value to an F-statistic:\n\\[\n\\begin{split}\nF &= \\frac{0.742}{1-0.742} \\times \\frac{29}{2} \\\\\n&= 41.7\n\\end{split}\n\\]\nWe write this standardization of \\(R^2\\) as \\(F(2,29)=41.7\\).\n\n\n5.7.1 Computing F from the ANOVA Partitioning\nWe can also compute the model-level F-statistic directly using the partitioning of variation from the ANOVA table.\n\n# Partition the variation\nanova(lm.1)\n\n\n  \n\n\n\nThe F-statistic is a ratio of the mean square for the model and the mean square for the error. To compute a mean square we use the general formula:\n\\[\n\\mathrm{MS} = \\frac{\\mathrm{SS}}{\\mathrm{df}}\n\\]\nThe model includes both the education and seniority predictor, so we combine the SS and df. The MS model is:\n\\[\n\\begin{split}\n\\mathrm{MS}_{\\mathrm{Model}} &= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{4147.3 + 722.9}{1 + 1} \\\\[1ex]\n&= \\frac{4870.2}{2} \\\\[1ex]\n&= 2435.1\n\\end{split}\n\\]\nThe MS error is:\n\\[\n\\begin{split}\n\\mathrm{MS}_{\\mathrm{Error}} &= \\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\frac{1695.3 }{29} \\\\[1ex]\n&= 58.5\n\\end{split}\n\\]\nThen, we compute the F-statistic by computing the ratio of these two mean squares.\n\\[\n\\begin{split}\nF &= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\frac{2435.1}{58.5} \\\\[1ex]\n&= 41.6\n\\end{split}\n\\]\nSince a mean square represents the average amount of variation (per degree of freedom), we can see that F is a ratio between the average amount of variation explained by the model and the average amount of variation unexplained by the model. In our example, this ratio is 41.6; on average the model explains 41.6 times the variation that is unexplained.\nNote that this is an identical computation (although reframed) as the initial computation for F. We can use mathematics to show this equivalence:\n\\[\n\\begin{split}\nF &= \\frac{R^2}{1-R^2} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Total}}}}{\\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Total}}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\mathrm{MS}_{\\mathrm{Model}} \\times \\frac{1}{\\mathrm{MS}_{\\mathrm{Error}}}\\\\[1ex]\n&= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}}\n\\end{split}\n\\]\n\n\n\n5.7.2 Testing the Model-Level Null Hypothesis\nWe evaluate our test statistic (F in this case) in the appropriate test distribution, in this case an F-distribution with 2 and 29 degrees of freedom. The figure below, shows the \\(F(2,29)\\)-distribution as a solid, black line. The p-value is the area under the curve that is at least as extreme as the observed F-value of 41.7.\n\n\nCode\n# Create data\nfig_11 = data.frame(\n  X = seq(from = 0, to = 50, by = 0.01)\n  ) %&gt;%\n  mutate(\n    Y = df(x = X, df1 = 2, df2 = 29)\n    )\n\n# Filter shaded area\nshade = fig_11 %&gt;%\n  filter(X &gt;= 41.7)\n\n# Create plot\nggplot(data = fig_11, aes(x = X, y = Y)) +\n  geom_line() +\n  theme_bw() +\n  geom_ribbon(data = shade, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4)\n\n\n\n\n\n\n\n\nFigure 5.11: Plot of the probability density function (PDF) for the \\(F(2,~29)\\)-distribution. The cumulative density representing the p-value for a test evaluating whether \\(\\rho^2=0\\) using an observed F-statistic of 41.7 is also displayed.\n\n\n\n\n\nThe computation using the cumulative density function, pf(), to obtain the p-value is:\n\n# p-value for F(2,29)=41.7\n1 - pf(41.7, df1 = 2, df2 = 29)\n\n[1] 2.942114e-09\n\n\nBecause we want the upper-tail, rather than taking the difference from 1, we can also use the lower.tail=FALSE argument in pf().\n\n# p-value for F(2,29)=41.7\npf(41.7, df1 = 2, df2 = 29, lower.tail = FALSE)\n\n[1] 2.942114e-09\n\n\n\n\n\n5.7.3 Mean Squares are Variance Estimates\nMean squares are also estimates of the variance. Consider the computational formula for the sample variance,\n\\[\n\\hat{\\sigma}^2 = \\frac{\\sum(Y - \\bar{Y})^2}{n-1}\n\\]\nThis is the total sum of squares divided by the total df. The variance of the outcome variable is interpreted as the average amount of variation in the outcome variable (in the squared metric). Thus, it is also referred to as the mean square total.\nWhen we compute an F-statistic, we are finding the ratio of two different variance estimates—one based on the model (explained variance) and one based on the error (unexplained variance). Under the null hypothesis that \\(\\rho^2 = 0\\), we are assuming that all the variance is unexplained. In that case, our F-statistic would be close to zero. When the model explains a significant amount of variation, the numerator gets larger relative to the denominator and the F-value is larger.\nThe mean squared error (from the anova() output) plays a special role in regression analysis. It is the variance estimate for the conditional distributions of the residuals in our visual depiction of the distributional assumptions of the residuals underlying linear regression.\n\n\n\n\n\n\n\n\nFigure 5.12: Visual Depiction of the Distributional Assumptions of the Residuals Underlying Linear Regression\n\n\n\n\n\nRecall that we made implicit assumptions about the conditional distributions of the residuals, namely that they were identically and normally distributed with a mean of zero and some variance. Based on the estimate of the mean squared error, the variance of each of these distributions is 58.5.\nWhile the variance is a mathematical convenience, the standard deviation is often a better descriptor of the variation in a distribution since it is measured in the original metric. The standard deviation fro the residuals (error) is 7.6. Because the residuals are statistics (summaries computed from sample data), their standard deviation is referred to as a “standard error”.\n\nThe residual standard error (RSE) is sometimes referred to as the Root Mean Squared Error (RMSE).\n\n\n# Compute RMSE\nsqrt(58.5)\n\n[1] 7.648529\n\n\nWhy is this value important? It gives the expected variation in the conditional residual distributions, which is a measure of the average amount of error. For example, since all of the conditional distributions of the residuals are assumed to be normally distributed, we would expect that 95% of the residuals would fall between \\(\\pm2\\) standard errors from 0; or, in this case, between \\(-15.3\\) and \\(+15.3\\). Observations with residuals that are more extreme may be regression outliers.\nMore importantly, it is a value that we need to estimate in order to specify the model.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#coefficent-level-inference-the-t-distribution",
    "href": "02-01-mathematical-foundations-probability-density.html#coefficent-level-inference-the-t-distribution",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.8 Coefficent-Level Inference: The t-Distribution",
    "text": "5.8 Coefficent-Level Inference: The t-Distribution\nRecall that the coefficients and SEs for the coefficients are computed directly from the raw data based on the OLS estimation. These can then be uses to construct a test statistic (e.g., t) to carry out a hypothesis test or compute endpoints for a confidence interval. To see how this is done, we will consider the partial effect of education level (after controlling for differences in seniority) in our fitted model.\n\\[\n\\begin{split}\n\\hat\\beta_{\\mathrm{Education}}&=2.25 \\\\[1ex]\n\\mathrm{SE}(\\hat\\beta_{\\mathrm{Education}}) &=0.335\n\\end{split}\n\\]\nWe might want to test whether the partial effect of education level on income, after accounting for differences in seniority level, we observed in the data is more than we would expect because of sampling error. To answer this we need to evaluate the following hypothesis:\n\\[\nH_0: \\beta_{\\mathrm{Education}} = 0\n\\]\nWe begin by converting our estimated regression coefficient to a t-statistic using:\n\\[\nt_k = \\frac{\\hat\\beta_k}{\\mathrm{SE}(\\hat\\beta_k)}\n\\]\nIn our example,\n\\[\n\\begin{split}\nt_{\\mathrm{Education}} &= \\frac{2.25}{0.335} \\\\[1ex]\n&= 6.72\n\\end{split}\n\\]\nRemember this tells us the education coefficient of 2252 is 6.72 standard errors above 0. Since we are estimating the SE using sample data, our test statistic is likely t-distributed2. Which value should we use for df? Well, for that, statistical theory tells us that we should use the error df value from the model. In our example, this would be:\n\\[\nt(29) = 6.72\n\\]\nUsing the t-distribution with 29 df, we can compute the p-value associated with the two-tailed test that \\(\\beta_{\\mathrm{Education}=0}\\):\n\n# p-value for the two-tailed test of no effect of education\n2 * pt(q = -6.72, df = 29)\n\n[1] 2.257125e-07\n\n# alternatively\n2 * pt(q = 6.72, df = 29, lower.tail = FALSE)\n\n[1] 2.257125e-07\n\n\nThe p-value is 0.0000002. The data are inconsistent with the hypothesis that there is no partial effect of education level on income (after accounting for differences in seniority level).\nWe could also carry out these tests for the partial effect of seniority level and, if it is of interest, for the intercept. For both of those tests, we would use the same t-distribution, but our test statistic would be computed based on the coefficient estimates and standard errors for those terms, respectively.\n\n\n5.8.1 Confidence/Compatibility Intervals for the Coefficients\nThe confidence interval for the kth regression coefficient is computed as:\n\\[\n\\mathrm{CI} = \\hat\\beta_k \\pm t^{*}(\\mathrm{SE}_{\\hat\\beta_k})\n\\]\nwhere \\(t^*\\) is the quantile of the t-distribution that defines the confidence level for the interval. (This t-distribution, again, has degrees-of-freedom equal to the error df in the model.) The confidence level is related to the alpha level (type I error rate) used in inference. Namely,\n\\[\n\\mathrm{Confidence~Level} = 1 - \\alpha\n\\]\nSo, if you use \\(\\alpha=.05\\), then the confidence level would be \\(.95\\), and we would call this a 95% confidence interval. The alpha value also helps determine the quantile we use in the CI formula,\n\\[\nt^* = (1-\\frac{\\alpha}{2}) ~ \\mathrm{quantile}\n\\] For the example using \\(\\alpha=.05\\), a 95% confidence interval, the \\(t^*\\) value would be associated with the quantile of 0.975. We would denote this as:\n\\[\nt^{*}_{.975}\n\\]\nSay we wanted to find the 95% confidence interval for the education coefficient. We know that the estimated coefficient for education is 2.25, and the standard error for this estimate is 0.335. We also know that based on the model fitted, the residual df is 29. We need to find the 0.975th quantile in the t-distribution with 29 df.\n\n# Find 0.975th quantile\nqt(p = 0.975, df = 29)\n\n[1] 2.04523\n\n\nNow we can use all of this information to compute the confidence interval:\n\\[\n\\begin{split}\n95\\%~CI  &= 2.25 \\pm 2.04523(0.335) \\\\[1ex]\n&= \\big[1.56,~2.94\\big]\n\\end{split}\n\\]",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#learn-more",
    "href": "02-01-mathematical-foundations-probability-density.html#learn-more",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.9 Learn More",
    "text": "5.9 Learn More\nFor more information, you can also see the section Some Continuous Distributions in CHapter 5 of the Fox (2021) textbook.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#references",
    "href": "02-01-mathematical-foundations-probability-density.html#references",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "5.10 References",
    "text": "5.10 References\n\n\n\n\n\n\nFox, J. (2021). A mathematical primer for social statistics (2nd ed.). Sage.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-01-mathematical-foundations-probability-density.html#footnotes",
    "href": "02-01-mathematical-foundations-probability-density.html#footnotes",
    "title": "5  Mathematical Foundations: Probability Density",
    "section": "",
    "text": "Remember, the mean and standard deviations in the population are called “parameters”.↩︎\nWhether this is actually t-distributed depends on whether the model assumptions are met.↩︎",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mathematical Foundations: Probability Density</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html",
    "href": "02-03-likelihood-framework-for-evidence.html",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "",
    "text": "6.1 Preparation\nIn this set of notes, you will learn about the law of likelihood, and the use of likelihood ratios as statistical evidence for model selection. To do so, we will use the pew.csv dataset (see the data codebook) to fit a set of models that explain variation in American’s political knowledge.\n# Load libraries\nlibrary(broom)\nlibrary(educate)\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(tidyr)\n\n# Import data\npew = read_csv(file = \"https://raw.githubusercontent.com/zief0002/fluffy-ants/main/data/pew.csv\")\n\n# View data\npew",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#research-questions-and-modeling-strategy",
    "href": "02-03-likelihood-framework-for-evidence.html#research-questions-and-modeling-strategy",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.2 Research Questions and Modeling Strategy",
    "text": "6.2 Research Questions and Modeling Strategy\nRecall that we had three research questions for these data:\n\nIs there an effect of news exposure on political knowledge?\nIs there an effect of news exposure on political knowledge after controlling for demographic and political covariates?\nDoes education level moderate the effect of news exposure on political knowledge after controlling for demographic and political covariates?\n\nAny analysis should begin with looking at plots and computing summary statistics of the sample data. (We already did this in a previous set of notes.) After the data exploration, we can begin to think about fitting one or more models to the data. It is good science to consider the modeling strategy you will be using before you begin fitting models. There are many modeling strategies that educational scientists use in practice (e.g., forward-selection, backward-elimination) and there is no one “right” method. As you consider a modeling strategy, think about how this strategy helps provide a narrative structure for answering your research question; sometimes this leads to one strategy being more productive than others.\nIn the previous set of notes we began by fitting a model that only included the main-effect of news exposure. Evaluating the effect of news exposure in this model helped us answer RQ 1. We then fitted a model that included news exposure along with the set of demographic and political covariates. Evaluating the effect of news exposure in this model helped us answer RQ 2. Finally, we fitted a model that included an interaction effect of news exposure and education in along with the demographic and political covariates. Evaluating the interaction effect allowed us to answer RQ 3. These three models are:\n\\[\n\\begin{split}\n\\mathbf{Model~1:~} \\quad \\mathrm{News~Knowledge}_i = &\\beta_0 + \\beta_1(\\mathrm{News~Exposure}_i) + \\epsilon_i \\\\[2ex]\n\\mathbf{Model~2:~} \\quad \\mathrm{News~Knowledge}_i = &\\beta_0 + \\beta_1(\\mathrm{Age}_i) + \\beta_2(\\mathrm{Education}_i) + \\beta_3(\\mathrm{Male}_i) + \\beta_4(\\mathrm{Engagement}_i) + \\beta_5(\\mathrm{Ideology}_i)+ \\beta_6(\\mathrm{Democrat}_i) + \\\\ &\\beta_7(\\mathrm{Republican}_i) + \\beta_8(\\mathrm{News~Exposure}_i) + \\epsilon_i \\\\[2ex]\n\\mathbf{Model~3:~} \\quad \\mathrm{News~Knowledge}_i = &\\beta_0 + \\beta_1(\\mathrm{Age}_i) + \\beta_2(\\mathrm{Education}_i) + \\beta_3(\\mathrm{Male}_i) + \\beta_4(\\mathrm{Engagement}_i) + \\beta_5(\\mathrm{Ideology}_i)+ \\beta_6(\\mathrm{Democrat}_i) + \\\\ &\\beta_7(\\mathrm{Republican}_i) + \\beta_8(\\mathrm{News~Exposure}_i) + \\beta_9(\\mathrm{News~Exposure}_i)(\\mathrm{Education}_i) + \\epsilon_i \\\\[2ex]\n\\end{split}\n\\]\nwhere \\(\\epsilon_i \\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\) for each of the models.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#classical-framework-of-evidence",
    "href": "02-03-likelihood-framework-for-evidence.html#classical-framework-of-evidence",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.3 Classical Framework of Evidence",
    "text": "6.3 Classical Framework of Evidence\nWhen we have looked at statistical evidence to this point, it has been from a hypothesis testing point of view. The primary piece of evidence we use in this paradigm is the p-value. For example, if we fit Model 1 and examine the evidence for the effect of news exposure on news knoledge, we find:\n\n# Fit Model 1\nlm.1 = lm(knowledge ~ 1 + news, data = pew)\n\n# Coefficient-level output\ntidy(lm.1)\n\n\n  \n\n\n\nThe p-value associated with the effect of news exposure (\\(p&lt;.001\\)), which suggests that the size of this effect is more than we would expect because of chance if the population effect was 0.\nInterpreting this p-values, we would say that the probability of seeing the empirical evidence we observed (or evidence that is more extreme) if the null hypothesis that there is no effect of news exposure on news knowledge is true, is 0.000442. This implies that our observed data are inconsistent with the hypothesized model that there is no effect of news exposure. In an applied setting, we might use such evidence to decide that the level of news exposure does indeed predict variation in American’s news knowledge.\nDespite being the predominant evidential paradigm used in the education and social sciences, hypothesis testing has many criticisms (e.g., Johansson, 2011; Weakliem, 2016). Among some of the stronger criticisms,\n\nThe p-value only measures evidence against the hypothesized model; not the evidence FOR a particular model.\nThe model we specify in the null hypothesis is often substantively untenable (how often is the effect 0? Generally as applied scientists the reason we include predictors is because we believe there is an effect.)\nThe p-value is based on data we haven’t observed (it is based on the observed data AND evidence that is more extreme).\n\nIf we write the p-value as a probability statement, it would be:\n\\[\np\\mbox{-}\\mathrm{value} = P(\\mathrm{Data~or~more~extreme~unobserved~data} \\mid \\mathrm{Model})\n\\]\nWhile hypothesis tests have filled a need in the educational and social science to have some standard for evaluating statistical evidence, it is unclear whether this is the approach we should be using. As statistician David Lindley so aptly states, “[significance tests] are widely used, yet are logically indefensible” (comment in Johnstone, 1986, p. 502). Psychologist Jacob Cohen was more pointed, saying “[hypothesis testing] has not only failed to support the advance of psychology as a science but also has seriously impeded it” (Cohen, 1994, p. 997).\n\n“The main purpose of a significance test is to inhibit the natural enthusiasm of the investigator” (Mosteller & Bush, 1954, pp. 331–332).",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#likelihood-paradigm-to-statistical-evidence",
    "href": "02-03-likelihood-framework-for-evidence.html#likelihood-paradigm-to-statistical-evidence",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.4 Likelihood Paradigm to Statistical Evidence",
    "text": "6.4 Likelihood Paradigm to Statistical Evidence\nIn applied science, we ideally would like to collect some evidence (data) and use that to say something about how likely a particular model (or hypothesis) is based on that evidence. Symbolically we want to know,\n\\[\nP(\\mathrm{Model} \\mid \\mathrm{Observed~data})\n\\]\nThis probability is known as the likelihood and is very different than the probability given by the p-value. In the likelihood paradigm, the likelihood is the key piece of statistical evidence used to evaluate models. For example if you were comparing Model A and Model B, you could compute the likelihood for each model and compare them. Whichever model has the higher likelihood has more empirical support. This is, in a nutshell what the Law of Likelihood states. What is even more attractive is that another axiom, the Likelihood Principle, tells us that if the goal is to compare the empirical support of competing models, all of the information in the data that can be used to do so, is contained in the ratio of the model likelihoods. That is, we can’t learn more about which model is more supported unless we collect additional data.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#joint-probability-density-a-roadstop-to-computing-likelihood",
    "href": "02-03-likelihood-framework-for-evidence.html#joint-probability-density-a-roadstop-to-computing-likelihood",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.5 Joint Probability Density: A Roadstop to Computing Likelihood",
    "text": "6.5 Joint Probability Density: A Roadstop to Computing Likelihood\nIn the preparation reading, you learned about the probability density of an observation \\(x_i\\). Now we will extend this idea to the probability density of a set of observations, say \\(x_1\\), \\(x_2\\), AND \\(x_k\\). The probability density of a set of observations is referred to as the joint probability density, or simply joint density.\nIf we can make an assumption about INDEPENDENCE, then the joint probability density would be the product of the individual densities:\n\\[\np(x_1, x_2, x_3, \\ldots, x_k) = p(x_1) \\times p(x_2) \\times p(x_3) \\times \\ldots \\times p(x_k)\n\\]\nSay we had three independent observations, \\(x =\\{60, 65, 67\\}\\), from a \\(\\sim\\mathcal{N}(50,10)\\) distribution. The joint density would be:\n\n# Compute joint density\ndnorm(x = 60, mean = 50, sd = 10) * dnorm(x = 65, mean = 50, sd = 10) * dnorm(x = 67, mean = 50, sd = 10)\n\n[1] 0.000002947448\n\n\nWe could also shortcut this computation,\n\n# Compute joint density\nprod(dnorm(x = c(60, 65, 67), mean = 50, sd = 10))\n\n[1] 0.000002947448\n\n\nThis value is the joint probability density. The joint probability density indicates the probability of observing the data (\\(x =\\{60, 65, 67\\}\\)) GIVEN (1) they are drawn from a normal distribution and (2) the normal distribution has a mean of 50 and a standard deviation of 10. In other words, the joint probability density is the probability of the data given a model and parameters of the model.\nSymbolically,\n\\[\n\\mathrm{Joint~Density} = P(\\mathrm{Data} \\mid \\mathrm{Model~and~Parameters})\n\\]",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#computing-likelihood",
    "href": "02-03-likelihood-framework-for-evidence.html#computing-likelihood",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.6 Computing Likelihood",
    "text": "6.6 Computing Likelihood\nLikelihood is the probability of a particular set of parameters GIVEN (1) the data, and (2) the data are generated from a particular model (e.g., normal distribution). Symbolically,\n\\[\n\\mathrm{Likelihood} = P(\\mathrm{Parameters} \\mid \\mathrm{Model~and~Data})\n\\]\nSymbolically we denote likelihood with a scripted letter “L” (\\(\\mathcal{L}\\)). For example, we might ask the question, given the observed data \\(x = \\{30, 20, 24, 27\\}\\) come from a normal distribution, what is the likelihood (probability) that the mean is 20 and the standard deviation is 4? We might denote this as,\n\\[\n\\mathcal{L}(\\mu = 20, \\sigma = 4 \\mid x)\n\\]\n\nFYI\nAlthough we need to specify the model this is typically not included in the symbolic notation; instead it is often a part of the assumptions.\n\n\n\n6.6.1 An Example of Computing and Evaluating Likelihood\nThe likelihood allows us to answer probability questions about a set of parameters. For example, what is the likelihood (probability) that the data (\\(x = \\{30, 20, 24, 27\\}\\)) were generated from a normal distribution with a mean of 20 and standard deviation of 4? To compute the likelihood we compute the joint probability density of the data under that particular set of parameters.\n\nprod(dnorm(x = c(30, 20, 24, 27), mean = 20, sd = 4))\n\n[1] 0.0000005702554\n\n\nWhat is the likelihood (probability) that the same set of data (\\(x = \\{30, 20, 24, 27\\}\\)) were generated from a normal distribution with a mean of 25 and standard deviation of 4?\n\nprod(dnorm(x = c(30, 20, 24, 27), mean = 25, sd = 4))\n\n[1] 0.00001774012\n\n\nGiven the data and the model, there is more empirical support that the parameters are \\(\\mathcal{N}(25,4^2)\\) rather than \\(\\mathcal{N}(20, 4^2)\\), because the likelihood is higher for the former set of parameters. We can compute a ratio of the two likelihoods to quantify the amount of additional support for the \\(\\mathcal{N}(25,4^2)\\).\n\\[\n\\begin{split}\n\\mathrm{Likelihood~Ratio} &= \\frac{0.00001774012}{0.0000005702554} \\\\[1ex]\n&= 31.11\n\\end{split}\n\\]\nThe empirical support for the \\(\\mathcal{N}(25,4^2)\\) parameterization is 31 times that of the \\(\\mathcal{N}(20, 4^2)\\) parameterization! In a practical setting, this would lead us to adopt a mean of 25 over a mean of 20.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#some-notes-and-caveats",
    "href": "02-03-likelihood-framework-for-evidence.html#some-notes-and-caveats",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.7 Some Notes and Caveats",
    "text": "6.7 Some Notes and Caveats\nIt is important to note that although we use the joint probability under a set of parameters to compute the likelihood of those parameters, theoretically joint density and likelihood are very different. Likelihood takes the data and model as given and computes the probability of a set of parameters. Whereas joint density assumes that the model and parameters are given and gives us the probability of the data.\n\nFYI\nLikelihood refers to the probability of the parameters and joint probability density refers to the probability of the data.\n\nOnce we collect the data, the probability of observing that set of data is 1; it is no longer unknown. The likelihood method treats our data as known and offers us a way of making probabilistic statements about the unknown parameters. This is more aligned with our scientific process than making some assumption about the parameter (e.g., \\(\\beta_1=0\\)) and then trying to determine the probability of the data under that assumption. Moreover, likelihood does not use unobserved data (e.g., data more extreme than what we observed) in the computation.\nIt is also important to acknowledge what likelihood and the likelihood ratio don’t tell us. First, they only tell us the probability of a set of parameters for the data we have. Future collections of data might change the amount of support or which set of parameters is supported. Since changing the data, changes the likelihood, this also means we cannot make cross study comparisons of the likelihood (unless the studies used the exact same data). Secondly, the model assumed is important. If a different model is assumed, the likelihood will be different, and again could change the amount of support or which set of parameters is supported.\nThe likelihood ratio (LR), while useful for comparing the relative support between parameterizations, does not tell you that a particular parameterization is correct. For example, the LR of 31.11 tells us that there is more empirical support for the \\(\\mathcal{N}(25,4^2)\\) parameterization than \\(\\mathcal{N}(20, 4^2)\\). But, there might be even more support for a parameterization we haven’t considered.\nThese shortcomings are not unique to the likelihood paradigm The also exist in the classical hypothesis testing paradigm for statistical evidence. All in all, the added advantages to the likelihood paradigm make it more useful to applied work than hypothesis testing.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#likelihood-in-regression-back-to-our-example",
    "href": "02-03-likelihood-framework-for-evidence.html#likelihood-in-regression-back-to-our-example",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.8 Likelihood in Regression: Back to Our Example",
    "text": "6.8 Likelihood in Regression: Back to Our Example\nWhen fitting a regression model, we make certain assumptions about the relationship between a set of predictors and the outcome. For example, in Model 1 from our earlier example, we assume that the relationship between news exposure and news knowledge can be described by the following model:\n\\[\n\\begin{split}\n\\mathrm{News~Knowledge}_i = &\\beta_0 + \\beta_1(\\mathrm{News~Exposure}_i) + \\epsilon_i \\\\[1ex]\n&\\mathrm{where~}\\epsilon_i \\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\sigma^2_{\\epsilon})\n\\end{split}\n\\]\nHere we use OLS to estimate the regression coefficients. Then we can use those, along with the observed data to obtain the residuals and the estimate for the residual standard error. The residuals are the GIVEN data and the set up distributional assumptions for the model (e.g., normal, mean of 0, constant variance) allow us to compute the likelihood for the entire set of parameters in this model (\\(\\beta_0\\), \\(\\beta_1\\), \\(\\sigma^2_{\\epsilon}\\)).\nBelow is a set of syntax to compute the likelihood, based on fitting lm.1. We use the resid() function to compute the residuals. (It is the same as grabbing the column called .resid from the augment() output.) We also use the estimated value of the residual standard error (\\(\\hat{\\sigma}_{\\epsilon} = 20.3\\)) from the glance() output.\n\n# Get RSE for use in likelihood\nglance(lm.1)\n\n\n  \n\n\n# Compute likelihood for lm.1\nprod(dnorm(x = resid(lm.1), mean = 0, sd = 20.3))\n\n[1] 1.382925e-192\n\n\nThis value by itself is somewhat meaningless. It is only worthwhile when we compare it to the likelihood from another model. For example, let’s compute the likelihood for an intercept-only model (Model 0) and compare this to the likelihood for lm.1.\n\n# Fit Model 0\nlm.0 = lm(knowledge ~ 1, data = pew)\n\n# Get RSE for use in likelihood\nglance(lm.0)\n\n\n  \n\n\n# Compute likelihood for lm.2\nprod(dnorm(x = resid(lm.0), mean = 0, sd = 21.5))\n\n[1] 2.489266e-195\n\n\nThe likelihood value for lm.1 is higher than the likelihood value for lm.0. Computing the likelihood ratio:\n\n1.382925e-192 / 2.489266e-195\n\n[1] 555.5553\n\n\nThis suggests that given the data, Model 1 is 555.6 times more likely than Model 0. In practice, we would adopt Model 1 over Model 0 because it is more likely given the empirical evidence (data) we have.\n\n6.8.1 Mathematics of Likelihood\nBeing able to express the likelihood mathematically is important for quantitative methodologists as it allows us to manipulate and study the likelihood function and its properties. It also gives us insight into how the individual components of the likelihood affect its value.\nRemember, we can express the likelihood of the regression residuals mathematically as:\n\\[\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) = p(\\epsilon_1) \\times p(\\epsilon_2) \\times \\ldots \\times p(\\epsilon_n)\n\\]\nwhere the probability density of each residual (assuming normality) is:\n\\[\np(\\epsilon_i) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{(\\epsilon_i-\\mu)^2}{2\\sigma^2}\\right]\n\\]\nIn addition to normality, which gives us the equation to compute the PDF for each residual, the regression assumptions also specify that each conditional error distribution has a mean of 0 and some variance (that is the same for all conditional error distributions). We can call it \\(\\sigma^2_{\\epsilon}\\). Substituting these values into the density function, we get,\n\\[\n\\begin{split}\np(\\epsilon_i) &= \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{(\\epsilon_i-0)^2}{2\\sigma^2_{\\epsilon}}\\right] \\\\[1em]\n&= \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{(\\epsilon_i)^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\end{split}\n\\]\nNow we use this expression for each of the \\(p(\\epsilon_i)\\) values in the likelihood computation.\n\\[\n\\begin{split}\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) &= p(\\epsilon_1) \\times p(\\epsilon_2) \\times \\ldots \\times p(\\epsilon_n) \\\\[1em]\n&= \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{\\epsilon_1\n^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\\\\n&~~~~~~\\ldots \\times \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\end{split}\n\\]\nWe can simplify this:\n\\[\n\\begin{split}\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) &=\\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\exp\\left[-\\frac{\\epsilon_1^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\ldots \\\\\n&~~~~~~ \\times \\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\end{split}\n\\] We can also simplify this by using the product notation:\n\\[\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) =\\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\prod_{i=1}^n \\exp\\left[-\\frac{\\epsilon_i^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\] We can also write the residuals (\\(\\epsilon_i\\)) as a function of the regression parameters we are trying to find the likelihood for.\n\\[\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) =\\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\prod_{i=1}^n \\exp\\left[-\\frac{\\big[Y_i - \\beta_0 - \\beta_1(X_i)\\big]^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\]\nwhere \\(\\sigma^2_{\\epsilon} = \\frac{\\sum \\epsilon_i^2}{n}\\). Because the numerator of \\(\\sigma^2_{\\epsilon}\\) can be written as \\(\\sum_i^n\\big(Y_i - \\beta_0 - \\beta_1(X_i)\\big)^2\\), we see that the likelihood is a function of \\(n\\), and the regression coefficients, \\(\\beta_0\\) and \\(\\beta_1\\). Moreover, \\(n\\) is based on the data (which is given) and is thus is a constant. Mathematically, this implies that the only variables (values that can vary) in the likelihood function are the regression coefficients.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#log-likelihood",
    "href": "02-03-likelihood-framework-for-evidence.html#log-likelihood",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.9 Log-Likelihood",
    "text": "6.9 Log-Likelihood\nThe likelihood values are quite small since we are multiplying several probability densities (values between 0 and 1) together. Since it is hard to work with these smaller values, in practice, we often compute and work with the natural logarithm of the likelihood. So in our example, Model 0 (\\(\\mathcal{L}_0 = 2.489266 \\times 10^{-195}\\)) has a log-likelihood of:\n\n# Log-likelihood for Model 0\nlog(2.489266e-195)\n\n[1] -448.0921\n\n\nSimilarly, we can compute the log-likelihood for Model 1 as:\n\n# Log-likelihood for Model 1\nlog(1.382925e-192)\n\n[1] -441.7721\n\n\nWe typically denote log-likelihood using a scripted lower-case “l” (\\(\\mathcal{l}\\)). Here,\n\\[\n\\begin{split}\n\\mathcal{l}_0 &= -448.0921 \\\\[1ex]\n\\mathcal{l}_1 &= -441.7721 \\\\[1ex]\n\\end{split}\n\\]\nNote that the logarithm of a decimal will be negative, so the log-likelihood will be a negative value. Less negative log-likelihood values correspond to higher likelihood values, which indicate more empirical support. Here Model 1 has a log-likelihood value (\\(-441.8\\)) that is less negative than Model 0’s log-likelihood value (\\(-448.1\\)), which indicates there is more empirical support for Model 1 than Model 0.\n\nFYI\nWhile it is possible using algebra to express the likelihood ratio using log-likelihoods, it does not have the same interpretational value as the LR does. Because of this, the likelihood ratio is not often expresseds using log-likelihoods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.9.1 Mathematics of Log-Likelihood\nWe can express the log-likelihood of the regression residuals mathematically by taking the natural logarithm of the likelihood we computed earlier:\n\\[\n\\begin{split}\n\\ln \\Bigl(\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data})\\Bigr) &= \\ln \\Biggl( \\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\exp\\left[-\\frac{\\epsilon_1^2}{2\\sigma^2_{\\epsilon}}\\right] \\times  \\\\\n&~~~~~~ \\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\ldots \\times \\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right] \\Biggr) \\\\\n\\end{split}\n\\]\nUsing our rules for logarithms and re-arranging gives,\n\\[\n\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = -\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\epsilon_i^2\n\\]\nExamining this equation, we see that the log-likelihood is a function of \\(n\\), \\(\\sigma^2_{\\epsilon}\\) and the sum of squared residuals (SSR)1. We can of course, re-express this using the the regression parameters:\n\\[\n\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = -\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\big[Y_i - \\beta_0 - \\beta_1(X_i)\\big]^2\n\\]\nAnd, again, since \\(\\sigma^2_{\\epsilon}\\) is a function of the regression coefficients and \\(n\\), this means that the only variables in the log-likelihood function are the coefficients.\n\n\n\n6.9.2 Shortcut: The logLik() Function\nThe logLik() function can be used to obtain the log-likelihood directly from a fitted model object. For example, to find the log-likelihood for Model 1, we can use:\n\n# Compute log-likelihood for Model 1\nlogLik(lm.1)\n\n'log Lik.' -441.7579 (df=3)\n\n\nThe df output tells us how many total parameters are being estimated in the model. In our case the number of total parameters in Model 0 is three (\\(\\beta_0\\), \\(\\beta_{\\mathrm{News~Exposure}}\\), and \\(\\sigma^2_{\\epsilon}\\)). What is more important to us currently, is the log-likelihood value; \\(\\mathcal{l}_1=-450.2233\\).\nThis value is slightly different than the log-likelihood we just computed of \\(-441.7721\\). This is not because of rounding in this case. It has to do with how the model is being estimated; the logLik() function assumes the parameters are being estimated using maximum likelihood (ML) rather than ordinary least squares (OLS). You can learn more about ML estimation in the optional set of notes, but for now, we will just use logLik() to compute the log-likelihood.\nHere we compute the log-likelihood for Model 0 using the logLik() function. We also use the output to compute the likelihood for Model 0. To compute the likelihood from the log-likelihood we need to exponentiate (the reverse function of the logarithm) the the log-likelihood value using the exp() function. We also compute the log-likelihood and likelihood value for Model 1.\n\n# Compute log-likelihood for Model 0\nlogLik(lm.0)\n\n'log Lik.' -448.0884 (df=2)\n\n# Compute likelihood for Model 0\nexp(logLik(lm.0)[1])\n\n[1] 2.498531e-195\n\n# Compute log-likelihood for Model 1\nlogLik(lm.1)\n\n'log Lik.' -441.7579 (df=3)\n\n# Compute likelihood for Model 1\nexp(logLik(lm.1)[1])\n\n[1] 1.402758e-192\n\n\nBecause the output from logLik() includes extraneous information (e.g., df), we use indexing (square brackets) to extract only the part of the output we want. In this case, the [1] extracts the log-likelihood value from the logLik() output (ignoring the df part).",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#model-complexity",
    "href": "02-03-likelihood-framework-for-evidence.html#model-complexity",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.10 Model Complexity",
    "text": "6.10 Model Complexity\nOne aspect that we need to consider is that more complex models tend to have higher likelihoods and log-likelihoods. Therefore when we compare likelihoods (or log-likelihoods) we need to consider the complexity in addition to the likelihoods. One way to quantify a model’s complexity is to consider the number of parameters that are being estimated. The more parameters that we need to estimate, the more complex the model. Recall that the number of parameters for a model are given in the df value from the logLik() function’s output.\nIn our example, the df value for Model 0 is two, indicating that this model is estimating two parameters (\\(\\beta_0\\), and \\(\\sigma^2_{\\epsilon}\\)). For Model 1, the df value was three. This indicates that Model 1 is more complex than Model 0.\nAs we consider using the likelihood ratio (LR) or the difference in log-likelihoods for model selection, we also need to consider the model complexity. In our example, the likelihood ratio of 555.6 indicates that Model 1 has approximately 555.6 times the empirical support than Model 0. But, Model 1 is more complex than Model 0, so we would expect that it would be more empirically supported.\nIn this case, with a likelihood ratio of 555.6, it seems like the empirical data certainly support adopting Model 1 over Model 0. despite the added complexity of Model 1. But what if the LR was 10? Would that be enough additional support to warrant adopting Model 1 over Model 0? What about a LR of 5?",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#likelihood-ratio-test-for-nested-models",
    "href": "02-03-likelihood-framework-for-evidence.html#likelihood-ratio-test-for-nested-models",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.11 Likelihood Ratio Test for Nested Models",
    "text": "6.11 Likelihood Ratio Test for Nested Models\nOne key question that arises is, if the likelihood for a more complex model is higher than the likelihood for a simpler model, how large does the likelihood ratio have to be before we adopt the more complex model? In general, there is no perfect answer for this.\nIf the models being compared are nested, then we can carry out a hypothesis test2 to see if the LR is more than we would expect because of chance. Models are nested when the parameters in the simpler model are a subset of the parameters in the more complex model. For example, in our example, the parameters in Model 0 are a subset of the parameters in Model 1:\n\\[\n\\begin{split}\n\\mathbf{Model~1~Parameters:}&\\quad\\{\\beta_0,~\\beta_{\\mathrm{News~Exposure}},~\\sigma^2_{\\epsilon}\\} \\\\[1ex]\n\\mathbf{Model~0~Parameters:}&\\quad\\{\\beta_0,~\\sigma^2_{\\epsilon}\\} \\\\[1ex]\n\\end{split}\n\\]\nThe parameters for Model 0 all appear in the list of parameters for Model 1. Because of this we can say that Model 0 is nested in Model 1.\n\n\n6.11.1 Hypothesis Test of the LRT\nWhen we have nested models we can carry out a hypothesis test to decide between the following competing hypotheses:\n\\[\n\\begin{split}\nH_0:& ~\\theta_0 = \\{\\beta_0,~\\sigma^2_{\\epsilon}\\}\\\\[1ex]\nH_A:& \\theta_1 = \\{\\beta_0,~\\beta_{\\mathrm{News~Exposure}},~\\sigma^2_{\\epsilon}\\}\n\\end{split}\n\\]\nwhere \\(\\theta_0\\) refers to the simpler model and \\(\\theta_1\\) refers to the more complex model. This translates to adopting either the simpler model (fail to reject \\(H_0\\)) or the more complex model (reject \\(H_0\\)). To carry out this test, we translate our likelihood ratio to a test statistic called \\(\\chi^2\\) (pronounced chi-squared):\n\\[\n\\chi^2 = -2 \\ln \\bigg(\\frac{\\mathcal{L}({\\theta_0})}{\\mathcal{L}({\\theta_1})}\\bigg)\n\\]\nThat is we compute \\(-2\\) times the log of the likelihood ratio where the likelihood for the simpler model is in the numerator. (Note this is the inverse of how we have been computing the likelihood ratio!) Equivalently, we can compute this as:\n\\[\n\\chi^2 = -2 \\bigg(\\ln \\bigg[\\mathcal{L}({\\theta_0})\\bigg] - \\ln \\bigg[\\mathcal{L}({\\theta_1})\\bigg]\\bigg)\n\\]\nFor our example, we compute this using the following syntax:\n\n# Compute chi-squared\n-2 * (logLik(lm.0)[1] - logLik(lm.1)[1])\n\n[1] 12.66099",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#deviance-a-measure-of-the-modeldata-error",
    "href": "02-03-likelihood-framework-for-evidence.html#deviance-a-measure-of-the-modeldata-error",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.12 Deviance: A Measure of the Model–Data Error",
    "text": "6.12 Deviance: A Measure of the Model–Data Error\nIf we re-write the formula for the \\(\\chi^2\\)-statistic by distributing the \\(-2\\), we get a better glimpse of what this statistic is measuring.\n\\[\n\\chi^2 = -2 \\ln \\bigg[\\mathcal{L}({\\theta_0})\\bigg] - \\bigg(-2\\ln \\bigg[\\mathcal{L}({\\theta_1})\\bigg]\\bigg)\n\\]\nThe quantity \\(-2\\ln\\big[\\mathcal{L}(\\theta_k)\\big]\\) is referred to as the residual deviance3 of Model K. It measures the amount of misfit between the model and the data. (As such, when evaluating deviance values, lower is better.) For linear models, with the classic assumptions (\\(\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\)), the deviance is a function of the residual sum of squares (RSS):\n\\[\n\\mathrm{Deviance} = n \\ln\\big(2\\pi\\sigma^2_{\\epsilon}\\big) + \\frac{\\mathrm{RSS}}{\\sigma^2_{\\epsilon}}\n\\]\nwhere \\(\\mathrm{RSS}=\\sum\\epsilon_i^2\\) and \\(\\sigma^2_{\\epsilon} = \\frac{\\mathrm{RSS}}{n}\\). This formula illustrates that the residual deviance is a generalization of the residual sum of squares (RSS), and measures the model–data misfit.\n\n6.12.1 Mathematics of Deviance\nWe can express the deviance mathematically by multiplying the log-likelihood by \\(-2\\).\n\\[\n\\begin{split}\n\\mathrm{Deviance} &= -2 \\times\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) \\\\[1ex]\n&= -2 \\bigg(-\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\epsilon_i^2\\bigg) \\\\[1ex]\n&= -n\\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{1}{\\sigma^2_{\\epsilon}}\\sum \\epsilon_i^2 \\\\[1ex]\n&= -n\\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{\\mathrm{RSS}}{\\sigma^2_{\\epsilon}}\n\\end{split}\n\\]\nRewriting this using the parameters from the likelihood:\n\\[\n\\mathrm{Deviance} = -n\\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{\\sum_{i=1}^n \\big[Y_i-\\beta_0-\\beta_1(X_i)\\big]^2}{\\sigma^2_{\\epsilon}}\n\\]\nOnce again, we find that the only variables in the deviance function are the regression coefficients.\n\nIn practice, we will use the logLik() function to compute the deviance.\n\n# Compute the deviance for Model 0\n-2 * logLik(lm.0)[1]\n\n[1] 896.1768\n\n# Compute the deviance for Model 1\n-2 * logLik(lm.1)[1]\n\n[1] 883.5158\n\n\nHere the deviance for Model 1 (883.5) is less than the deviance for Model 0 (896.2). This indicates that the data have better fit to Model 1 than Model 0. How much better is the model–data fit for Model 1?\n\n# Compute difference in deviances\n896.2 - 883.5\n\n[1] 12.7\n\n\nModel 1 improves the fit (reduces the misfit) by 12.7 over Model 0. This is the value of our \\(\\chi^2\\)-statistic. That is, the \\(\\chi^2\\)-statistic is the difference in residual deviance values and measures the amount of improvement in the model–data misfit.\n\n\n6.12.2 Modeling the Variation in the Test Statistic\nIf the null hypothesis is true, the difference in deviances can be modeled using a \\(\\chi^2\\)-distribution. The degrees-of-freedom for this \\(\\chi^2\\)-distribution is based on the difference in the number of parameters between the complex and simpler model. In our case this difference is four (\\(3-2=1\\)):\n\\[\n\\chi^2(1) = 12.66\n\\]\n\n\nCode\n# Create dataset\nfig_01 = data.frame(\n  X = seq(from = 0, to = 20, by = 0.01)\n  ) %&gt;%\n  mutate(\n    Y = dchisq(x = X, df = 1)\n    )\n\n# Filter out X&lt;=65\nshaded = fig_01 %&gt;%\n  filter(X &gt;=12.66)\n\n# Create plot\nggplot(data = fig_01, aes(x = X, y = Y)) +\n  geom_line() +\n  xlab(\"Chi-squared\") +\n  ylab(\"Probability density\") +\n  theme_light() +\n  geom_ribbon(data = shaded, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4)\n\n\n\n\n\n\n\n\nFigure 6.1: Plot of the probability density function (PDF) for a \\(\\chi^2(1)\\) distribution. The grey shaded area represents the p-value based on \\(\\chi^2=12.66\\).\n\n\n\n\n\nTo compute the p-value we use the pchisq() function.\n\n# Compute p-value for X^2(1) = 14.50\n1 - pchisq(q = 12.66, df = 1)\n\n[1] 0.0003735622\n\n# Alternative method\npchisq(q = 12.66, df = 1, lower.tail = FALSE)\n\n[1] 0.0003735622\n\n\nBased on the p-value, we would reject the null hypothesis for the likelihood ratio test, which suggests that we should adopt the more complex model (Model 1). This means that the model that includes the effect of news exposure is more empirically supported than a model that includes no predictors. Note that we are making a holistic evaluation about the model rather than about individual predictors.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#using-the-lrtest-function",
    "href": "02-03-likelihood-framework-for-evidence.html#using-the-lrtest-function",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.13 Using the lrtest() Function",
    "text": "6.13 Using the lrtest() Function\nIn practice, we can also use the lrtest() function from the {lmtest} package to carry out a likelihood ratio test. We provide this function the name of the model object for the simpler model, followed by the name of the model object for the more complex model.\n\n# Load library\nlibrary(lmtest)\n\n# LRT to compare Model 0 and Model 1\nlrtest(lm.0, lm.1)",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#evaluating-the-effect-of-news-exposure-in-research-question-2",
    "href": "02-03-likelihood-framework-for-evidence.html#evaluating-the-effect-of-news-exposure-in-research-question-2",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.14 Evaluating the Effect of News Exposure in Research Question 2",
    "text": "6.14 Evaluating the Effect of News Exposure in Research Question 2\nIn RQ 2, we are evaluating the effect of news exposure on news knowledge after controlling for the set of political and demographic covariates. To do this using a likelihood ratio test, we need to compare a baseline model that includes all of the covariates to a model that includes the covariates AND the effect of news exposure. That is, the only thing that is different between the two models is that the second model includes the effect of news exposure on top of the covariates. That is we will be comparing the following two models:\n\\[\n\\begin{split}\n\\mathbf{Simple~Model:~} \\quad \\mathrm{News~Knowledge}_i = &\\beta_0 + \\beta_1(\\mathrm{Education}_i) + \\beta_2(\\mathrm{Male}_i) + \\beta_3(\\mathrm{Engagement}_i) + \\epsilon_i \\\\[2ex]\n\\mathbf{Complex~Model:~} \\quad \\mathrm{News~Knowledge}_i = &\\beta_0 + \\beta_1(\\mathrm{Education}_i) + \\beta_2(\\mathrm{Male}_i) + \\beta_3(\\mathrm{Engagement}_i) + \\beta_8(\\mathrm{News~Exposure}_i) + \\epsilon_i \\\\[2ex]\n\\end{split}\n\\]\nBy comparing the parameters of the two models, we can see that the simpler model is nested in the more complex model.\n\\[\n\\begin{split}\n\\mathbf{Simple~Model~Parameters:}&\\quad\\{\\beta_0,~\\beta_{\\mathrm{Education}},~\\beta_{\\mathrm{Male}},~\\beta_{\\mathrm{Engagement}},~\\sigma^2_{\\epsilon}\\} \\\\[1ex]\n\\mathbf{Complex~Model~Parameters:}&\\quad\\{\\beta_0, ~\\beta_{\\mathrm{Education}},~\\beta_{\\mathrm{Male}},~\\beta_{\\mathrm{Engagement}},~\\beta_{\\mathrm{News~Exposure}},~\\sigma^2_{\\epsilon}\\} \\\\[1ex]\n\\end{split}\n\\]\nTo carry out the LRT we fit the two models, compute the difference in model deviances, and evaluate that difference in a \\(\\chi^2\\)-distribution with degrees-of-freedom equal to the difference in model complexity (based on the number of parameters being estimated).\n\n# Simple model\nlm.2 = lm(knowledge ~ 1 + education + male + engagement, data = pew)\n\n# Complex model\nlm.3 = lm(knowledge ~ 1 + education + male + engagement + news, data = pew)\n\n# Compute the difference in deviances between Model 1 and Model 2\n-2 * logLik(lm.2)[1] - (-2 * logLik(lm.3)[1])\n\n[1] 8.924345\n\n# Compute the difference in model complexity\n6 - 5\n\n[1] 1\n\n# Compute p-value for X^2(1) = 7.960401\npchisq(q = 8.924345, df = 1, lower.tail = FALSE)\n\n[1] 0.002813943\n\n\nThe p-value is 0.002813943, suggesting that there is an effect of news exposure on news knowledge, after controlling for the set of political and demographic covariates. Again, we could also obtain this same result via using the lrtest() function.\n\n# LRT to compare Model 2 and Model 3\nlrtest(lm.2, lm.3)",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#evaluating-the-interaction-effect-in-research-question-3",
    "href": "02-03-likelihood-framework-for-evidence.html#evaluating-the-interaction-effect-in-research-question-3",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.15 Evaluating the Interaction Effect in Research Question 3",
    "text": "6.15 Evaluating the Interaction Effect in Research Question 3\nTo evaluate the potential interaction effect between news exposure and education on news knowledge, after controlling for demographic and political covariates, we need to fit the interaction model and then compare it to a model that includes all the same predictors EXCEPT the interaction effect.\n\n# Fit interaction model\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\n\nNote that Model 3 (fitted earlier) is the baseline comparison model to evaluate the interaction effect. Here we will use the lrtest() function to compare Models 3 and 4 to evaluate the interaction effect.\n\n# LRT to compare Model 1 and Model 2\nlrtest(lm.3, lm.4)\n\n\n  \n\n\n\nThe p-value is 0.02324, suggesting that there is an interaction effect between news exposure and education level after controlling for differences in the set of political and demographic covariates.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#evaluating-assumptions",
    "href": "02-03-likelihood-framework-for-evidence.html#evaluating-assumptions",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.16 Evaluating Assumptions",
    "text": "6.16 Evaluating Assumptions\nIf we were adopting a “final model”, the empirical evidence would support adopting Model 4. It is always important to evaluate any adopted final models’ assumptions.\n\n# Create residual plots\nresidual_plots(lm.4)\n\n\n\n\n\n\n\nFigure 6.2: Two residual plots for Model 4.\n\n\n\n\n\nBased on the density plot, the assumption of normality looks reasonably met. The scatterplot suggests the assumption that the average residual is 0 is generally met—the loess smoother suggests the average residual is close to 0 at all fitted values. The homoscadasticity assumption also seems reasonable with the range of residuals generally being constant across the different fitted values. Lastly, since the sample is a random sample of Americans, the independence assumption also seems tenable.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#presenting-the-results-from-the-lrts",
    "href": "02-03-likelihood-framework-for-evidence.html#presenting-the-results-from-the-lrts",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.17 Presenting the Results from the LRTs",
    "text": "6.17 Presenting the Results from the LRTs\nBelow we present a table summarizing the results of the likelihood ratio tests.\n\n\nCode\n# Load library\nlibrary(gt)\n\n# Set up data\nd = data.frame(\n  comparison = c(\"Model 0 vs. Model 1\", \"Model 2 vs. Model 3\", \"Model 3 vs. Model 4\"),\n  chi = c(\"$$\\\\chi^2(1) = 12.66$$\", \"$$\\\\chi^2(1) = 8.92$$\", \"$$\\\\chi^2(1) = 5.15$$\"),\n  p = c(\"&lt;.001\", \".003\", \".023\")\n)\n\n# Create table\nd |&gt;\n  gt() |&gt;\n  cols_label(\n    comparison = md(\"*Model Comparison*\"),\n    chi = \"LRT Result\",\n    p = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(comparison),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(chi, p),\n    align = \"center\"\n  ) |&gt; \n  tab_options(\n    table.width = pct(60),\n    quarto.disable_processing = TRUE\n    )\n\n\n\n\nTable 6.1: Results from a set of likelihood ratio tests (LRT) to compare sets of nested candidate models.\n\n\n\n\n\n\n  \n    \n      Model Comparison\n      LRT Result\n      p\n    \n  \n  \n    Model 0 vs. Model 1\n$$\\chi^2(1) = 12.66$$\n&lt;.001\n    Model 2 vs. Model 3\n$$\\chi^2(1) = 8.92$$\n.003\n    Model 3 vs. Model 4\n$$\\chi^2(1) = 5.15$$\n.023",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#reporting-individual-predictors-from-a-model",
    "href": "02-03-likelihood-framework-for-evidence.html#reporting-individual-predictors-from-a-model",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.18 Reporting Individual Predictors From a Model",
    "text": "6.18 Reporting Individual Predictors From a Model\nIt is also good practice to report the coefficient-level and model-level estimates from any adopted models in a regression table. We will report results from Model 4. At the coefficient-level, the coefficients and standard errors can be reported from the tidy() output. However, since we are using a likelihood framework, the p-values from the tidy() output are incorrect! We need to compute and report likelihood-based p-values.\nFor example, to evaluate the effect of education on news knowledge we would essentially want to test the hypothesis that:\n\\[\nH_0: \\beta_{\\mathrm{Education}} = 0\n\\]\nTo do this with a LRT, we need to compare two models:\n\nOne model that includes all of the predictors from Model 4;\nOne model that includes everything from Model 4 except the effect of education.\n\nThe only difference between these two models is the inclusion of the effect of education in the second model. That means any additional empirical support for the second model over the first is completely due to the effect of education. And, because the second model is nested in the first, we can evaluate this via a LRT.\n\n# Fit full model 4\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\n\n# Fit model without education\nlm.4_education = lm(knowledge ~ 1 + male + engagement + news + news:education, data = pew)\n\n# Carry out LRT\nlrtest(lm.4_education, lm.4)\n\n\n  \n\n\n\nThe p-value associated with the test of whether or not there is an effect of education (at least for the main effect) is 0.0001314. We will need to obtain the likelihood-based p-value for all of the other effects in a similar way. We will\n\n# Effect of male\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\nlm.4_male = lm(knowledge ~ 1 + education + engagement + news + news:education, data = pew)\nlrtest(lm.4_male, lm.4) # Carry out LRT\n\n\n  \n\n\n# Effect of engagement\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\nlm.4_engage = lm(knowledge ~ 1 + education + male + news + news:education, data = pew)\nlrtest(lm.4_engage, lm.4) # Carry out LRT\n\n\n  \n\n\n# Effect of new exposure (main-effect)\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\nlm.4_news = lm(knowledge ~ 1 + education + male + engagement + news:education, data = pew)\nlrtest(lm.4_news, lm.4) # Carry out LRT\n\n\n  \n\n\n# Effect of interaction\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\nlm.4_interaction = lm(knowledge ~ 1 + education + male + engagement + news, data = pew)\nlrtest(lm.4_interaction, lm.4) # Carry out LRT\n\n\n  \n\n\n# Intercept\nlm.4 = lm(knowledge ~ 1 + education + male + engagement + news + news:education, data = pew)\nlm.4_intercept = lm(knowledge~ 0 + education + male + engagement + news + news:education, data = pew)\nlrtest(lm.4_intercept, lm.4) # Carry out LRT\n\n\n  \n\n\n\nWe can then replace the p-values from the tidy() output with these likelihood-based p-values. Also, since these p-values are based on chi-squared (not a t-value), we should replace the t-values from the tidy() output with the \\(\\chi^2\\)-values from the LRTs.\n\ntidy(lm.4) |&gt;\n  mutate(\n    statistic = c(11.38, 14.62, 12.80, 17.94, 7.45, 5.15),\n    p.value = c(0.0007417, 0.0001314, 0.0003466, 0.0000228, 0.006351, 0.02324)\n  ) \n\n\n  \n\n\n\nIf we were reporting this for publication, we could round the p-values to three decimal places. We would also want to indicate that these are likelihood-based p-values, and that the LRT is based on 1 df.\n\n\nCode\n# Load library\nlibrary(gt)\nlibrary(gtsummary) # for formatting p-values\n\n# Create table\ntidy(lm.4) |&gt;\n  mutate(\n    term = c(\"Intercept\", \"Education-level\", \"Male\", \"Political engagement\", \"News exposure\", \"Education-level x News exposure\"),\n    statistic = c(11.38, 14.62, 12.80, 17.94, 7.45, 5.15),\n    p.value = style_pvalue(c(0.001, 0.0001314, 0.0003466, 0.0000228, 0.006351, 0.02324), digits = 3)\n  )  |&gt;\n  gt() |&gt;\n  cols_label(\n    term = md(\"*Predictor*\"),\n    estimate = md(\"*B*\"),\n    std.error = md(\"*SE*\"),\n    statistic = \"$$\\\\chi^2$$\",\n    p.value = md(\"*p*\")\n  ) |&gt;\n  cols_align(\n    columns = c(term),\n    align = \"left\"\n  ) |&gt;\n  cols_align(\n    columns = c(estimate, std.error, statistic, p.value),\n    align = \"center\"\n  ) |&gt; \n  fmt_number(\n    columns = c(estimate, std.error),\n    decimals = 2,\n    use_seps = FALSE\n  ) |&gt;\n  tab_options(\n    table.width = pct(70)\n    ) |&gt;\n  tab_footnote(\n    footnote = md(\"Male is a dummy-coded predictor with non-male as the reference group.\"),\n    locations =  cells_body(columns = term, rows = 3)\n  ) \n\n\n\n\nTable 6.2: Coefficients and standard errors for Model 4. The \\(\\chi^2\\) values and p-values are based on likelihood ratio tests (LRT) with 1 degree-of-freedom to evaluate each predictor.\n\n\n\n\n\n\n\n\n\nPredictor\nB\nSE\n$$\\chi^2$$\np\n\n\n\n\nIntercept\n−80.79\n24.00\n11.38\n0.001\n\n\nEducation-level\n6.44\n1.68\n14.62\n&lt;0.001\n\n\nMale1\n11.30\n3.15\n12.80\n&lt;0.001\n\n\nPolitical engagement\n0.39\n0.09\n17.94\n&lt;0.001\n\n\nNews exposure\n1.17\n0.44\n7.45\n0.006\n\n\nEducation-level x News exposure\n−0.07\n0.03\n5.15\n0.023\n\n\n\n1 Male is a dummy-coded predictor with non-male as the reference group.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#references",
    "href": "02-03-likelihood-framework-for-evidence.html#references",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "6.19 References",
    "text": "6.19 References\n\n\n\n\n\n\nCohen, J. (1994). The earth is round (\\(p &lt; .05\\)). American Psychologist, 49(12), 997–1003.\n\n\nJohansson, T. (2011). Hail the impossible: P-values, evidence, and likelihood. Scandinavian Journal of Psychology, 52, 113–125. https://doi.org/10.1111/j.1467-9450.2010.00852.x\n\n\nJohnstone, D. J. (1986). Tests of significance in theory and practice. The Statistician, 35, 491–504.\n\n\nMosteller, F., & Bush, R. B. (1954). Selected quantitative techniques. In G. Lindzey (Ed.), Handbook of social psychology (pp. 289–334). Addison-Wesley.\n\n\nWeakliem, D. L. (2016). Hypothesis testing and model selection in the social sciences. The Guilford Press.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-03-likelihood-framework-for-evidence.html#footnotes",
    "href": "02-03-likelihood-framework-for-evidence.html#footnotes",
    "title": "6  Likelihood: A Framework for Evidence",
    "section": "",
    "text": "Sometimes this is also referred to a the sum of squared errors (SSE).↩︎\nThis is in some sense mixing the paradigms of likelihood-based evidence and classical hypothesis test-based evidence. In a future set of notes we will learn about information criteria which eliminate the need to mix these two paradigms.↩︎\nThe use of the term “residual deviance” is not universal. Some textbooks omit the “residual” part and just refer to it as the “deviance”. Others use the term “model deviance”.↩︎",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Likelihood: A Framework for Evidence</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html",
    "href": "02-04-likelihood-framework-for-estimation.html",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "",
    "text": "7.1 Likelihood\nIn this set of notes, you will learn about the method of maximum likelihood to estimate model parameters.\nRemember that likelihood is the probability of a particular set of parameters GIVEN (1) the data, and (2) the data are from a particular distribution (e.g., normal). Symbolically,\n\\[\n\\mathrm{Likelihood} = P(\\mathrm{Parameters} \\mid \\mathrm{Distribution~and~Data})\n\\]\nAlso recall, that to compute the likelihood we compute the joint probability density of the data under that particular set of parameters. For example, to compute \\(\\mathcal{L}(\\mu = 20, \\sigma = 4 \\mid x = \\{30, 20, 24, 27\\}; \\mathcal{N})\\), we use the following syntax:\n# Compute likelihood mu=20, sigma=4\nprod(dnorm(x = c(30, 20, 24, 27), mean = 20, sd = 4))\n\n[1] 5.702554e-07",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#likelihood-as-a-framework-for-estimation",
    "href": "02-04-likelihood-framework-for-estimation.html#likelihood-as-a-framework-for-estimation",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.2 Likelihood as a Framework for Estimation",
    "text": "7.2 Likelihood as a Framework for Estimation\nLikelihood can be used as the basis for estimating parameters given a model and data. The idea is that the “best” estimates for the parameters are those that produce the highest likelihood given the data and model. Consider this example: Which set of parameters,\\(\\mathcal{N}(20,4)\\) or \\(\\mathcal{N}(25,4)\\), was more likely to generate the data \\(x = \\{30, 20, 24, 27\\}\\)? To answer this, we compute the likelihood for both candidate sets of parameters.\n\n# Compute likelihood mu=20, sigma=4\nprod(dnorm(x = c(30, 20, 24, 27), mean = 20, sd = 4))\n\n[1] 5.702554e-07\n\n# Compute likelihood mu=25, sigma=4\nprod(dnorm(x = c(30, 20, 24, 27), mean = 25, sd = 4))\n\n[1] 1.774012e-05\n\n\nSince the second set of parameters produced a higher likelihood, it is more probable that the data were generated from the \\(\\mathcal{N}(25,4)\\) distribution than from the \\(\\mathcal{N}(20,4)\\) distribution. (Using the likelihood ratio, we can also say how much more likely the data were to be generated from this distribution.)\nSo now we come to the crux of Maximum Likelihood Estimation (MLE). The goal of MLE is to find a set of parameters that MAXIMIZES the likelihood given the data and a distribution. For example, given the observed data \\(x = \\{30, 20, 24, 27\\}\\) were generated from a normal distribution, what are the values for the parameters of this distribution (mean and standard deviation) that produce the HIGHEST (or maximum) value of the likelihood?\nWhichever parameters produce the highest likelihood end up being the parameter estimates. We will illustrate this a couple examples.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#example-1-using-mle-to-estimate-the-mean",
    "href": "02-04-likelihood-framework-for-estimation.html#example-1-using-mle-to-estimate-the-mean",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.3 Example 1: Using MLE to Estimate the Mean",
    "text": "7.3 Example 1: Using MLE to Estimate the Mean\nFor our first example, we will keep it simple by only trying to estimate a single parameter value using MLE. For this example, we will try to estimate the mean value for the four data values, \\(x=\\{30,20,24,27\\}\\). To do this we will assume that the data were generated from a normal distribution and that the standard deviation in this normal distribution was \\(\\sigma=4.272\\).\nWhat we need to do, is compute the likelihood for several different \\(\\mu\\) values, and determine which produces the highest likelihood value. Here we try five different values for \\(\\mu\\).\n\n# Compute L(mu = 10)\nprod(dnorm(c(30, 20, 24, 27), mean = 10, sd = 4.272))\n\n[1] 1.449116e-16\n\n# Compute L(mu = 15)\nprod(dnorm(c(30, 20, 24, 27), mean = 15, sd = 4.272))\n\n[1] 1.695637e-10\n\n# Compute L(mu = 20)\nprod(dnorm(c(30, 20, 24, 27), mean = 20, sd = 4.272))\n\n[1] 8.27684e-07\n\n# Compute L(mu = 25)\nprod(dnorm(c(30, 20, 24, 27), mean = 25, sd = 4.272))\n\n[1] 1.685382e-05\n\n# Compute L(mu = 30)\nprod(dnorm(c(30, 20, 24, 27), mean = 30, sd = 4.272))\n\n[1] 1.431642e-06\n\n\nBased on this it looks like a \\(\\mu\\approx25\\) produces the highest likelihood. We can also plot the likelihood versus the candidate parameter values.\n\n\nCode\n# Create data\nexample_01 = data.frame(\n  mu = c(10, 15, 20, 25, 30)\n  ) |&gt;\n  rowwise() |&gt;\n  mutate(\n    L = prod(dnorm(c(30, 20, 24, 27), mean = mu, sd = 4.272))\n    )\n\n\n# Plot\nggplot(data = example_01, aes(x = mu, y = L)) +\n  geom_line(color = \"darkgrey\", size = 0.5) +\n  geom_point() +\n  xlab(expression(mu)) +\n  ylab(\"Likelihood\") +\n  theme_light()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nFigure 7.1: Plot of the likelihood values for five candidate parameter values.\n\n\n\n\n\nWe could then continue to try values around 30 to hone in on the \\(\\mu\\) value that produces the highest likelihood (e.g., \\(\\mu=\\{24.7, 24.8, 24.9, 25.1, 25.2, 25.3\\}\\)). This methodology essentially boils down to continuing to narrow the search space by determining the likelihood value for more and more precisely defined values of the parameter.\nWe could also carry out this search computationally. Here for example, I set up a data frame that includes candidates for \\(\\mu\\). This search space is looks at all values of \\(\\mu\\) between 10.00 and 30.00. This is called the search space. (Given our search space, we will be able to determine \\(\\mu\\) to within the nearest hundredth.) We are then going to compute the likelihood based on each of those values. We use rowwise() so that the likelihood (in the mutate() layer) is carried out correctly; using the \\(\\mu\\) value in each row. Because rowwise() At the end of the chain, we use ungroup() to return the results to an ungrouped data frame.\n\n# Set up parameter search space\n# Compute likelihood\nexample_01_grid = data.frame(\n  mu = seq(from = 10, to = 30, by = 0.01)\n  ) |&gt;\n  rowwise() |&gt;\n  mutate(\n    L = prod(dnorm(c(30, 20, 24, 27), mean = mu, sd = 4.272))\n    ) |&gt;\n  ungroup()\n\n# View results\nhead(example_01_grid)\n\n\n  \n\n\n\nWe can then plot the likelihood versus the parameter values for these 2,001 parameter candidates. Typically, when there are many values, we do this using a line plot. This is what is referred to as a profile plot.\n\n# Plot the likelihood versus the parameter values\nggplot(data = example_01_grid, aes(x = mu, y = L)) +\n  geom_line() +\n  xlab(expression(mu)) +\n  ylab(\"Likelihood\") +\n  theme_light()\n\n\n\n\n\n\n\nFigure 7.2: Likelihood profile for several parameter values assuming a normal distribution and a standard deviation of 4.272.\n\n\n\n\n\nUsing the profile plot, we can see that the \\(\\mu\\) value that produces the largest value for the likelihood is a bit higher than 25. We can find the exact value in our search space by arranging the rows in our search data frame by their likelihood values, and then using the slice_max() function to find the row with the highest likelihood value. The n=1 argument finds the maximum row. If you wanted the highest two rows, we would use n=2\n\n# Find mu with maximum likelihood\nexample_01_grid |&gt;\n  slice_max(L, n = 1)\n\n\n  \n\n\n\nIn the candidate values for \\(\\mu\\) that we included in the search space, \\(\\mu=25.25\\) produces the highest likelihood. Thus, given the data and that the data were generated from a normal distribution with \\(\\sigma=4.272\\), the most probable value for \\(\\mu\\) is 25.25. This is our maximum likelihood estimate for the mean!\n\nYou can get more precision in the estimate by changing the by= argument in the seq() function when you are initially setting up your search space. For example if you need the estimate to the nearest 1000th, set by=.001.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#example-2-using-mle-to-estimate-the-mean-and-standard-deviation",
    "href": "02-04-likelihood-framework-for-estimation.html#example-2-using-mle-to-estimate-the-mean-and-standard-deviation",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.4 Example 2: Using MLE to Estimate the Mean and Standard Deviation",
    "text": "7.4 Example 2: Using MLE to Estimate the Mean and Standard Deviation\nIn the previous example, we assumed that we knew the value for \\(\\sigma\\). In practice, this often needs to be estimates along with \\(\\mu\\). To do this, you need to set up a search space that includes different combinations of \\(\\mu\\) and \\(\\sigma\\). Here we search \\(\\mu = \\{10.0, 10.1, 10.2,\\ldots, 30.0\\}\\) and \\(\\sigma=\\{0.0, 0.1, 0.2,\\ldots,10.0\\}\\) values from 0.1 to 10.0. (Remember, that \\(\\sigma\\geq0\\)).\nThe crossing() function creates every combination of \\(\\mu\\) and \\(\\sigma\\) that we define in our search space. So, for example, [\\(\\mu=10.0; \\sigma=0.0\\)], ]\\(\\mu=10.0; \\sigma=0.1\\)], [\\(\\mu=10.0; \\sigma=0.2\\)], etc. Since we have included 201 \\(\\mu\\) values and 101 \\(\\sigma\\) values, the search space is \\(201 \\times 101 = 20,301\\) parameter combinations.\n\n# Set up search space\n# Compute the likelihood\nexample_02 = crossing(\n  mu = seq(from = 10, to = 30, by = 0.1),\n  sigma = seq(from = 0, to = 10, by = 0.1)\n  ) |&gt;\n  rowwise() |&gt;\n  mutate(\n    L = prod(dnorm(c(30, 20, 24, 27), mean = mu, sd = sigma))\n    ) |&gt;\n  ungroup()\n\n# Find row with highest likelihood\nexample_02 |&gt;\n  slice_max(L, n = 1)\n\n\n  \n\n\n\nThe parameters that maximize the likelihood (in our search space) are a mean of 25.2 and a standard deviation of 3.7. Again, if you need to be more precise in these estimates, you can increase the precision in the by= argument of the seq() functions.\nIn computer science, this method for finding the MLE is referred to as a grid search. This is because the combinations of parameter values in the search space constitute a grid. In the figure below, the search space for each parameter is listed in the first row/column of the table. Every other cell of the table (the “grid”) constitutes a particular combination of the parameters. We are then computing the likelihood for each combination of parameters and searching for the cell with the highest likelihood.\n\n\n\n\n\n\n\n\nFigure 7.3: Grid showing the combinations of parameter values used in the search space.\n\n\n\n\n\n\nWhen we have two (or more) parameters we need to estimate the time taken to carry out a grid search is increased in a non-linear way. For example, combining 100 values of each parameter does not result in a search space of 200, but a search space of 10,000. So increasing the precision of both parameters to by=.01 increases each the number of candidates from \\(20,301\\) to \\(2001 \\times 1001 = 2,003,001\\). This increase the computational time it takes to solve the problem.\nIf you are relying on grid search, it is often better to operate with less precision initially, and then identify smaller parts of the grid that can be searched with more precision.\n\n\n\n7.4.1 Likelihood Profile for Multiple Parameters\nWe could also plot the profile of the likelihood for our search space, but this time there would be three dimensions: one dimension for \\(\\mu\\) (x-axis), one dimension for \\(\\sigma\\) (y-axis), and one dimension for the likelihood (z-axis). When we plot the likelihood profile across both \\(\\mu\\) and \\(\\sigma\\), the profile looks like an asymmetrical mountain. The highest likelihood value is at the summit of the mountain and corresponds to \\(\\mu=25.2\\) and \\(\\sigma=3.7\\).\n\n\nCode\n# Load library\nlibrary(plot3D)\n\nscatter3D(x = example_02$mu, y = example_02$sigma, z = example_02$L, \n          pch = 18, cex = 2, theta = 45, phi = 20, ticktype = \"detailed\",\n          xlab = expression(mu), ylab = expression(sigma), zlab = \"Likelihood\",\n          colkey = FALSE,\n          colvar = example_02$L,\n          col = ramp.col(col = c(\"#f6eff7\", \"#bdc9e1\", \"#67a9cf\", \"#1c9099\", \"#016c59\"), n = 100, alpha = 1)\n)\n\n\n\n\n\n\n\n\nFigure 7.4: Likelihood profile for the search space of both \\(\\mu\\) and \\(\\sigma\\) assuming a normal distribution.\n\n\n\n\n\nIf we extend our estimation to three or more parameters, we can still use the computational search to find the maximum likelihood estimates (MLEs), but it would be difficult to plot (there would be four or more dimensions). In general, the profile plots are more useful as a pedagogical tool rather than as a way of actually finding the MLEs.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#log-likelihood",
    "href": "02-04-likelihood-framework-for-estimation.html#log-likelihood",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.5 Log-Likelihood",
    "text": "7.5 Log-Likelihood\nThe likelihood values are quite small since we are multiplying several probability densities together. To alleviate this issue, it is typical to compute the natural logarithm of the likelihood and operate on it, rather than on the likelihood itself. For example, in our first example, we would compute the log-likelihood1 and then determine the \\(\\mu\\) value that has the highest log-likelihood value.\n\n# Set up parameter search space\n# Compute likelihood and\nexample_01 = data.frame(\n  mu = seq(from = 10, to = 30, by = 0.01)\n  ) |&gt;\n  rowwise() |&gt;\n  mutate(\n    L = prod(dnorm(c(30, 20, 24, 27), mean = mu, sd = 4.272)),\n    ln_L = log(L)\n    ) |&gt;\n  ungroup()\n\n# Find mu with maximum log-likelihood\nexample_01 |&gt;\n  slice_max(ln_L, n = 1)\n\n\n  \n\n\n\nThe profile of the log-likelihood looks a little different than that of the likelihood. What is important here is that the \\(\\mu\\) value that produces the highest value for the log-likelihood, is the same \\(\\mu\\) value that produces the highest likelihood.\n\n# Plot the log-likelihood versus the parameter values\nggplot(data = example_01, aes(x = mu, y = ln_L)) +\n  geom_line() +\n  xlab(expression(mu)) +\n  ylab(\"Log-likelihood\") +\n  theme_light()\n\n\n\n\n\n\n\nFigure 7.5: Log-likelihood profile for several parameter values assuming a normal distribution and a standard deviation of 4.272.\n\n\n\n\n\nMaximizing the log-likelihood gives the same parameter values as maximizing the likelihood. Remember that , so maximizing the log-likelihood is the same as maximizing the likelihood2.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#example-3-using-mle-to-estimate-regression-parameters",
    "href": "02-04-likelihood-framework-for-estimation.html#example-3-using-mle-to-estimate-regression-parameters",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.6 Example 3: Using MLE to Estimate Regression Parameters",
    "text": "7.6 Example 3: Using MLE to Estimate Regression Parameters\nIn estimating parameters for a regression model, we want to maximize the likelihood (or log-likelihood) for a given set of residuals that come from a normal distribution. We use the residuals since that is what we make distributional assumptions about in the model (e.g., normality, homogeneity of variance, independence). Our goal in regression is to estimate a set of parameters (\\(\\beta_0\\), \\(\\beta_1\\)) that maximize the likelihood of the residuals.\nTo understand this, consider the following a toy example of \\(n=10\\) observations.\n\n\nCode\n# create toy data\nexample_03 = data.frame(\n  x = c(4, 0, 3, 4, 7, 0, 0, 3, 0, 2),\n  y = c(53, 56, 37, 55, 50, 36, 22, 75, 37, 42)\n) \n\n# Create table\nexample_03 |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(x, y),\n    align = \"center\"\n  ) |&gt;\n  cols_label(\n    x = md(\"*x*\"),\n    y = md(\"*y*\")\n  ) |&gt;\n  tab_options(\n   table.width = pct(30) \n  )\n\n\n\n\nTable 7.1: Toy data set that includes predictor (x) and outcome (y) values.\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n4\n53\n\n\n0\n56\n\n\n3\n37\n\n\n4\n55\n\n\n7\n50\n\n\n0\n36\n\n\n0\n22\n\n\n3\n75\n\n\n0\n37\n\n\n2\n42\n\n\n\n\n\n\n\n\n\n\nWe initially enter these observations into two vectors, x and y.\n\n# Enter data into vectors\nx = c(4, 0, 3, 4, 7, 0, 0, 3, 0, 2)\ny = c(53, 56, 37, 55, 50, 36, 22, 75, 37, 42)\n\nNext, we will write a function to compute the log-likelihood of the residuals given a set of coefficient estimates. The bones for how we will create such a function is show below.\nll = function(b_0, b_1){\n\n  *Compute and output the log-likelihood*\n}\nWe use the function() function to write new functions. In our example, this function will be called ll. The arguments to the function() function are the inputs that a user of the function needs to input. Here we are asking users to input the two regression coefficients for a simple linear regression, namely \\(\\hat{\\beta}_0\\) (b_0) and \\(\\hat{\\beta}_1\\) (b_1).\nAll the computation that the function is going to execute is placed in-between the curly braces. For us this means we need to:\n\nCompute the residuals based on the inputs to the function;\nCompute the log-likelihood based on the residuals; and\nOutput the log-likelihood value.\n\nTo compute the residuals, we need to compute the fitted values, and subtract those from the outcome values. This means that we need x and y defined inside our function3.\nOnce we have the residuals, we compute the log-likelihood by incorporating the assumptions of the regression model. Since we assume the residuals are normally distributed, we compute the log-likelihood using the dnorm() function. The regression assumptions also specify that the mean residual value is 0; which implies that we should use the argument mean=0 in the dnorm() function.\nThe assumption about the standard deviation is that the conditional distributions all have the same SD, but it doesn’t specify what that value is. However, the SD of the errors seems like a reasonable value, so we will use that.\nFinally, we can output values from a function using the return() function. Below, we will write a function called ll() that takes two arguments as input, b0= and b1=, and outputs the log-likelihood.\n\n# Function to compute the log-likelihood\nll = function(b_0, b_1){\n\n  # Use the following x and y values\n  x = c(4, 0, 3, 4, 7, 0, 0, 3, 0, 2)\n  y = c(53, 56, 37, 55, 50, 36, 22, 75, 37, 42)\n\n  # Compute the yhat and residuals based on the two input values\n  yhats = b_0 + b_1*x\n  errors = y - yhats\n\n  # Compute the sd of the residuals\n  sigma = sd(errors)\n\n  # Compute the log-likelihood\n  log_lik = sum(dnorm(errors, mean = 0, sd = sigma, log = TRUE))\n\n  # Output the log-likelihood\n  return(log_lik)\n\n}\n\nNow we read in our function by highlighting the whole thing and running it. Once it has been read in, we can use it just like any other function. For example to find the log-likelihood for the parameters \\(\\beta_0=10\\) and \\(\\beta_1=3\\) we use:\n\n# Compute log-likelihood for b_0=10 and b_1=3\nll(b_0 = 10, b_1 = 3)\n\n[1] -64.29224\n\n\nWe can also use our function to compute the log-likelihood in a grid search. Remember, our goal is to estimate the regression coefficients, so we are searching across values of \\(\\beta_0\\) and \\(\\beta_1\\).\n\n# Create data set of search values and log-likelihoods\nexample_03 = crossing(\n  B0 = seq(from = 30, to = 50, by = 0.1),\n  B1 = seq(from = -5, to = 5, by = 0.1)\n) |&gt;\n  rowwise() |&gt;\n  mutate(\n    ln_L = ll(b_0 = B0, b_1 = B1)\n    ) |&gt;\n  ungroup()\n\n# Find parameters that produce highest log-likelihood\nexample_03 |&gt;\n  slice_max(ln_L, n = 1)\n\n\n  \n\n\n\nHere the parameter values that maximize the likelihood are \\(\\beta_0 = 40.1\\) and \\(\\beta_1=2.7\\). We can also compute what the standard deviation for the residual distributions was using the estimated parameter values. Remember, this value is an estimate of the RMSE.\n\n# Compute residuals using MLE estimate\nerrors = y - 40.1 - 2.7*x\n\n# Compute estimate of RMSE\nsd(errors)\n\n[1] 13.18665\n\n\nHere the maximum likelihood estimates for our three parameters are: \\(\\hat{\\beta}_0=40.1\\), \\(\\hat{\\beta}_1=2.7\\), and \\(\\hat{\\sigma}_{\\epsilon}=13.2\\).\n\n\n7.6.1 Complications with Grid Search\nIn practice, there are several issues with the grid search methods we have employed so far. The biggest is that you would not have any idea which values of \\(\\beta_0\\) and \\(\\beta_1\\) to limit the search space to. Essentially you would need to search an infinite number of values unless you could limit the search space in some way. For many common methods (e.g., linear regression) finding the ML estimates is mathematically pretty easy (if we know calculus; see the section Using Calculus to Determine the MLEs). For more complex methods (e.g., mixed-effect models) there is not a mathematical solution. Instead, mathematics is used to help limit the search space and then a grid search is used to hone in on the estimates.\nAlthough not a complication, we made an assumption about the value of the residual standard error, that it was equivalent to sigma(errors). In practice, this value would also need to be estimated, along with the coefficients.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#estimating-regression-parameter-ols-vs.-ml-estimation",
    "href": "02-04-likelihood-framework-for-estimation.html#estimating-regression-parameter-ols-vs.-ml-estimation",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.7 Estimating Regression Parameter: OLS vs. ML Estimation",
    "text": "7.7 Estimating Regression Parameter: OLS vs. ML Estimation\nTo compute ML estimates of the coefficients we will use the mle2() function from the {bbmle} package. To use the mle2() function, we need to provide a user-written function that returns the negative log-likelihood given a set of parameter inputs. Below we adapt the function we wrote earlier to return the negative log-likelihood. Since we are also interested in estimating the residual standard error (RSE), we also include this as an input into the function and use that inputted value in the dnorm() function.\n\n# Function to output the negative log-likelihood\nneg_ll = function(b_0, b_1, rse){\n\n  # Use the following x and y values\n  x = c(4, 0, 3, 4, 7, 0, 0, 3, 0, 2)\n  y = c(53, 56, 37, 55, 50, 36, 22, 75, 37, 42)\n\n  # Compute the yhat and residuals based on the two input values\n  yhats = b_0 + b_1*x\n  errors = y - yhats\n\n  # Compute the negative log-likelihood\n  neg_log_lik = -sum(dnorm(errors, mean = 0, sd = rse, log = TRUE))\n\n  # Output the log-likelihood\n  return(neg_log_lik)\n\n}\n\nNow we can use the mle2() function to estimate the three parameters. This function requires the argument, minuslogl=, which takes the user written function returning the negative log-likelihood. It also requires a list of starting values (initial guesses) for the input parameters in the user-written function.\n\n# Load library\nlibrary(bbmle)\n\nLoading required package: stats4\n\n\n\nAttaching package: 'bbmle'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n# Fit model using ML\nmle.results = mle2(minuslogl = neg_ll, start = list(b_0 = 20.0, b_1 = 5.0, rse = 10))\n\nWarning in dnorm(errors, mean = 0, sd = rse, log = TRUE): NaNs produced\n\n# View results\nsummary(mle.results)\n\nMaximum likelihood estimation\n\nCall:\nmle2(minuslogl = neg_ll, start = list(b_0 = 20, b_1 = 5, rse = 10))\n\nCoefficients:\n    Estimate Std. Error z value     Pr(z)    \nb_0  40.0072     5.6721  7.0533 1.748e-12 ***\nb_1   2.7361     1.7674  1.5481    0.1216    \nrse  12.5097     2.7973  4.4721 7.745e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n-2 log L: 78.90883 \n\n\nWe also obtain the OLS estimates:\n\n# Create data\nx = c(4, 0, 3, 4, 7, 0, 0, 3, 0, 2)\ny = c(53, 56, 37, 55, 50, 36, 22, 75, 37, 42)\n\n# Fit model with OLS\nlm.1 = lm(y ~ 1 + x)\n\n# Get estimates\nglance(lm.1)\n\n\n  \n\n\ntidy(lm.1)\n\n\n  \n\n\n\nBoth sets of parameter estimates are presented in Table 7.2.\n\n\nCode\n# create toy data\nestimates = data.frame(\n  Estimate = c(\"$$\\\\hat{\\\\beta}_0$$\", \"$$\\\\hat{\\\\beta}_1$$\", \"$$\\\\hat{\\\\sigma}^2_{\\\\epsilon}$$\"),\n  ML = c(40.01, 2.74, 12.51),\n  OLS = c(40.01, 2.74, 13.99)\n) \n\n# Create table\nestimates |&gt;\n  gt() |&gt;\n  cols_align(\n    columns = c(Estimate, ML, OLS),\n    align = \"center\"\n  ) |&gt;\n  tab_options(\n   table.width = pct(40) \n  )\n\n\n\n\nTable 7.2: Parameter estimates using maximum likelihood (ML) and ordinary least squares (OLS) estimation.\n\n\n\n\n\n\n\n\n\nEstimate\nML\nOLS\n\n\n\n\n$$\\hat{\\beta}_0$$\n40.01\n40.01\n\n\n$$\\hat{\\beta}_1$$\n2.74\n2.74\n\n\n$$\\hat{\\sigma}^2_{\\epsilon}$$\n12.51\n13.99\n\n\n\n\n\n\n\n\n\n\nComparing the coefficient estimates (\\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)) to those obtained through ordinary least squares, we find they are quite similar. The estimate of the residual standard error (\\(\\sigma_{\\epsilon}\\)), however, differs between the two estimation methods (although they are somewhat close in value).\n\n\n7.7.1 Estimating the Residual Variation: Maximum Likelihood vs. Ordinary Least Squares\nThe estimates of the residual standard error differ because the two estimation methods use different criteria to optimize over; OLS estimation finds the estimates that minimize the sum of squared errors, and ML finds the estimates that maximize the likelihood. Because of the differences, it is important to report how the model was estimated in any publication.\nBoth estimation methods have been well studied, and the resulting residual standard error from these estimation methods can be computed directly once we have the coefficient estimates (which are the same for both methods). Namely, the residual standard error resulting from OLS estimation is:\n\\[\n\\hat\\sigma_{\\epsilon}= \\sqrt{\\frac{\\left(Y_i - \\hat{Y}_i\\right)^2}{n-p-1}}\n\\]\nwhere p is the number of predictors in the model. And the residual standard error resulting from ML estimation is:\n\\[\n\\hat\\sigma_{\\epsilon}=\\sqrt{\\frac{\\left(Y_i - \\hat{Y}_i\\right)^2}{n}},\n\\]\nThe smaller denominator from the OLS estimate produces a higher overall estimate of the residual variation (more uncertainty). When n is large, the differences between the OLS and ML estimates of the residual standard error are minimal and can safely be ignored. When n is small, however, these differences can impact statistical results. For example, since the residual standard error is used to compute the standard error estimates for the coefficients, the choice of ML or OLS will have an effect on the size of the t- and p-values for the coefficients. (In practice, it is rare to see the different estimation methods producing substantively different findings, especially when fitting general linear models.)\nLastly, we note that the value of log-likelihood is the same for both the ML and OLS estimated models. The result from the ML output was:\n\\[\n\\begin{split}\n-2 \\ln(\\mathrm{Likelihood}) &= 78.91 \\\\[1ex]\n\\ln(\\mathrm{Likelihood}) &= -39.45\n\\end{split}\n\\]\nThe log-likelihood for the OLS estimated model is:\n\n# Log-likelihood for OLS model\nlogLik(lm(y ~ 1 + x))\n\n'log Lik.' -39.45442 (df=3)\n\n\nThis is a very useful result. It allows us to use lm() to estimate the coefficients from a model and then use its log-likelihood value in the same way as if we had fitted the model using ML. This will be helpful when we compute measure such as information criteria later in the course.\n\nIn many applications of estimation, it is useful to use a criterion which is modified variant of the likelihood. This variant omits “nuisance parameters” (parameters which are not of direct interest and subsequently not needed in the estimation method) from the computation of the likelihood. This restricted version of the likelihood is then maximized and the estimation method using this modified likelihood is called Restricted Maximum Likelihood (REML).\nWhen REML is used to estimate parameters, the residual standard error turns out to be the same as that computed in the OLS estimation. As such, sometimes this estimate is referred to as the REML estimate of the residual standard error.\n\n\n\n\n7.7.2 Using Calculus to Determine the MLEs\nA more convenient method to determine the ML estimates of the regression parameters is to use mathematics; specifically calculus. Remember, we can express the likelihood of the regression residuals mathematically as:\n\\[\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) = p(\\epsilon_1) \\times p(\\epsilon_2) \\times \\ldots \\times p(\\epsilon_n)\n\\]\nwhere the probability density of each residual (assuming normality) is:\n\\[\np(\\epsilon_i) = \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{(\\epsilon_i-\\mu)^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\]\nIn addition to normality, which gives us the equation to compute the PDF for each residual, the regression assumptions also specify that each conditional error distribution has a mean of 0 and some variance (that is the same for all conditional error distributions). We can call it \\(\\sigma^2_{\\epsilon}\\). Substituting these values into the density function, we get,\n\\[\n\\begin{split}\np(\\epsilon_i) &= \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{(\\epsilon_i-0)^2}{2\\sigma^2_{\\epsilon}}\\right] \\\\[1em]\n&= \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{(\\epsilon_i)^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\end{split}\n\\]\nNow we substitute this expression for each of the \\(p(\\epsilon_i)\\) values in the likelihood computation.\n\\[\n\\begin{split}\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) &= p(\\epsilon_1) \\times p(\\epsilon_2) \\times \\ldots \\times p(\\epsilon_n) \\\\[1em]\n&= \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{\\epsilon_1\n^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\ldots \\times\\\\ &~~~~\\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}}\\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\end{split}\n\\]\nWe can simplify this:\n\\[\n\\begin{split}\n\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data}) = &\\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\exp\\left[-\\frac{\\epsilon_1^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\ldots \\times \\\\\n&\\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right]\n\\end{split}\n\\]\nNow we will take the natural logarithm of both sides of the expression:\n\\[\n\\begin{split}\n\\ln \\Bigl(\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data})\\Bigr) = &\\ln \\Biggl( \\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\exp\\left[-\\frac{\\epsilon_1^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\ldots \\times \\\\\n&\\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right] \\Biggr)\n\\end{split}\n\\]\nUsing our rules for logarithms and re-arranging the terms gives us,\n\\[\n\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = -\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\epsilon_i^2\n\\]\nExamining this equation, we see that the log-likelihood is a function of n, \\(\\sigma^2_{\\epsilon}\\) and the sum of squared residuals (SSR). The observed data define n (the sample size) and the other two components come from the residuals, which are a function of the parameters and the data.\nOnce we have this function, calculus can be used to find the analytic maximum. Typically before we do this, we replace \\(\\epsilon_i\\) with \\(Y_i - \\hat\\beta_0 - \\hat\\beta_1(X_i)\\); writing the residuals as a function of the parameters (which we are solving for) and the data.\n\\[\n\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = -\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2\n\\]\nIn optimization, maximizing the log-likelihood is mathematically equivalent to minimizing the negative log-likelihood. (Note, this is what the mle2() function is doing.) That means we could also optimize over:\n\\[\n-\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = \\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2\n\\]\nThis has the advantage that we are removing the negative signs on the right-hand side of the equation. To find the analytic minimum (or maximum), we compute the partial derivatives with respect to \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), and \\(\\hat\\sigma^2_{\\epsilon}\\), and set these equal to zero and solve for each of the three parameters, respectively. That is:\n\\[\n\\begin{split}\n\\frac{\\partial}{\\partial \\beta_0} \\bigg[\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2\\bigg] &= 0 \\\\[1em]\n\\frac{\\partial}{\\partial \\beta_1} \\bigg[\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2\\bigg] &= 0 \\\\[1em]\n\\frac{\\partial}{\\partial \\sigma^2_{\\epsilon}} \\bigg[\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2\\bigg] &= 0\n\\end{split}\n\\]\nWithin each partial derivative, the parameters that are not being partialled can be treated as constants, which often makes the derivative easier to solve. For example in the first two partial derivatives the residual variance can be treated as a mathematical constant. Since all constant terms can be removed from the derivative, this leads to an interesting result:\n\\[\n\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\bigg[ -\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data})\\bigg] = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\bigg[ \\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2 \\bigg]\n\\]\nThis means that minimizing the negative log-likelihood is equivalent to minimizing the sum of squared residuals! This implies that the coefficients we get for OLS and ML estimation are the same.\nWhen we solve the third partial derivative for the residual standard error, we find that:\n\\[\n\\sigma^2_{\\epsilon} = \\frac{\\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2}{n}\n\\]\nThat is, the residual variance is equal to the sum of squared residuals divided by the sample size. In OLS estimation, the residual variance is the sum of squared residuals divided by the error degrees of freedom for the model. In the simple regression model the residual variance estimated using OLS would be:\n\\[\n\\sigma^2_{\\epsilon} = \\frac{\\sum \\bigg(Y_i - \\beta_0 - \\beta_1(X_i)\\bigg)^2}{n-2}\n\\]\nThis is why the residual standard errors were different when we used OLS and ML to carry out the estimation; the criteria we are optimizing over (sum of squared residuals vs. log-likelihood) impact the value of the residual variance estimate. Again, when n is large, the estimation method does not make a difference (i.e., \\(n \\approx n-2\\)).",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#references",
    "href": "02-04-likelihood-framework-for-estimation.html#references",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "7.8 References",
    "text": "7.8 References",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  },
  {
    "objectID": "02-04-likelihood-framework-for-estimation.html#footnotes",
    "href": "02-04-likelihood-framework-for-estimation.html#footnotes",
    "title": "7  Likelihood: A Framework for Estimation",
    "section": "",
    "text": "We could also compute the log-likelihood directly using sum(dnorm(c(30, 20, 24, 27), mean = mu, sd = 4.272, log = TRUE)).↩︎\nThis is because taking the logarithm of a set of numbers keeps the same ordination of values as the original values.↩︎\nAlternatively, x and y could be included as additional inputs into the function.↩︎",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood: A Framework for Estimation</span>"
    ]
  }
]